{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('/mnt/Vol20tb1/user_workspaces/mmpatil/MEA_Analysis_MAINBRANCH/IPNAnalysis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import helper_functions as helper\n",
    "\n",
    "\n",
    "spike_times = np.load('/mnt/Vol20tb1/user_workspaces/mmpatil/MEA_Analysis_MAINBRANCH/AnalyzedData/CDKL5_R59X_10112024_PS_ks4/CDKL5_R59X_10112024_PS/241025/M08020/Network/000128/well002/spikesorted_spike_times_dict.npy',allow_pickle=True).item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8),sharex=True)\n",
    "# Define the ISI threshold for burst detection (e.g., 0.1 seconds)\n",
    "isi_threshold = 0.1\n",
    "# Detect bursts for each unit\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "# Extracting ISIs as combined arrays\n",
    "all_isis_within_bursts = np.concatenate([stats['isis_within_bursts'] for stats in burst_statistics.values() if stats['isis_within_bursts'].size > 0])\n",
    "all_isis_outside_bursts = np.concatenate([stats['isis_outside_bursts'] for stats in burst_statistics.values() if stats['isis_outside_bursts'].size > 0])\n",
    "all_isis = np.concatenate([stats['isis_all'] for stats in burst_statistics.values() if stats['isis_all'].size > 0])\n",
    "\n",
    "# Calculate combined statistics\n",
    "mean_isi_within_combined = np.mean(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "cov_isi_within_combined = np.cov(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_outside_combined = np.mean(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "cov_isi_outside_combined = np.cov(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_all_combined = np.mean(all_isis) if all_isis.size > 0 else np.nan\n",
    "cov_isi_all_combined = np.cov(all_isis) if all_isis.size > 0 else np.nan\n",
    "\n",
    "# Calculate spike counts for each unit\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "\n",
    "# Sort units by ascending spike counts\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "axs[0]= helper.plot_raster_with_bursts(axs[0],spike_times, bursts,sorted_units=sorted_units, title_suffix=\"(Sorted Raster Order)\")\n",
    "from parameter_free_burst_detector import plot_network_bursts\n",
    "# Call the plot_network_activity function and pass the SpikeTimes dictionary\n",
    "axs[1],network_data=  plot_network_bursts(\n",
    "    axs[1], \n",
    "    spike_times\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8),sharex=True)\n",
    "# Define the ISI threshold for burst detection (e.g., 0.1 seconds)\n",
    "isi_threshold = 0.1\n",
    "# Detect bursts for each unit\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "# Extracting ISIs as combined arrays\n",
    "all_isis_within_bursts = np.concatenate([stats['isis_within_bursts'] for stats in burst_statistics.values() if stats['isis_within_bursts'].size > 0])\n",
    "all_isis_outside_bursts = np.concatenate([stats['isis_outside_bursts'] for stats in burst_statistics.values() if stats['isis_outside_bursts'].size > 0])\n",
    "all_isis = np.concatenate([stats['isis_all'] for stats in burst_statistics.values() if stats['isis_all'].size > 0])\n",
    "\n",
    "# Calculate combined statistics\n",
    "mean_isi_within_combined = np.mean(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "cov_isi_within_combined = np.cov(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_outside_combined = np.mean(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "cov_isi_outside_combined = np.cov(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_all_combined = np.mean(all_isis) if all_isis.size > 0 else np.nan\n",
    "cov_isi_all_combined = np.cov(all_isis) if all_isis.size > 0 else np.nan\n",
    "\n",
    "# Calculate spike counts for each unit\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "\n",
    "# Sort units by ascending spike counts\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "axs[0]= helper.plot_raster_with_bursts(axs[0],spike_times, bursts,sorted_units=sorted_units, title_suffix=\"(Sorted Raster Order)\")\n",
    "from logISINetworkBurst import plot_network_bursts_logISI\n",
    "# Call the plot_network_activity function and pass the SpikeTimes dictionary\n",
    "axs[1],network_data=  plot_network_bursts_logISI(\n",
    "    axs[1], \n",
    "    spike_times,\n",
    "    max_isi_cutoff=0.1,\n",
    "    void_threshold=0.7,\n",
    "    min_spikes_in_burst=5,\n",
    "    min_active_electrodes=0.1,   # 10% units\n",
    "    time_window=0.1,\n",
    "    min_active_threshold=0.1\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times2 =np.load('/mnt/Vol20tb1/user_workspaces/mmpatil/MEA_Analysis_MAINBRANCH/AnalyzedData/CDKL5_R59X_10112024_PS_17Nov/CDKL5_R59X_10112024_PS/241108/M07305/Network/000227/well004//spikesorted_spike_times_dict.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8),sharex=True)\n",
    "# Define the ISI threshold for burst detection (e.g., 0.1 seconds)\n",
    "isi_threshold = 0.1\n",
    "# Detect bursts for each unit\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times2, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "# Extracting ISIs as combined arrays\n",
    "all_isis_within_bursts = np.concatenate([stats['isis_within_bursts'] for stats in burst_statistics.values() if stats['isis_within_bursts'].size > 0])\n",
    "all_isis_outside_bursts = np.concatenate([stats['isis_outside_bursts'] for stats in burst_statistics.values() if stats['isis_outside_bursts'].size > 0])\n",
    "all_isis = np.concatenate([stats['isis_all'] for stats in burst_statistics.values() if stats['isis_all'].size > 0])\n",
    "\n",
    "# Calculate combined statistics\n",
    "mean_isi_within_combined = np.mean(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "cov_isi_within_combined = np.cov(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_outside_combined = np.mean(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "cov_isi_outside_combined = np.cov(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_all_combined = np.mean(all_isis) if all_isis.size > 0 else np.nan\n",
    "cov_isi_all_combined = np.cov(all_isis) if all_isis.size > 0 else np.nan\n",
    "\n",
    "# Calculate spike counts for each unit\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times2.items()}\n",
    "\n",
    "# Sort units by ascending spike counts\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "axs[0]= helper.plot_raster_with_bursts(axs[0],spike_times2, bursts,sorted_units=sorted_units, title_suffix=\"(Sorted Raster Order)\")\n",
    "from parameter_free_burst_detector import plot_network_bursts\n",
    "# Call the plot_network_activity function and pass the SpikeTimes dictionary\n",
    "axs[1],network_data=  plot_network_bursts(\n",
    "    axs[1], \n",
    "    spike_times2,\n",
    "\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network_data[\"info\"]['timescale_ms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from typing import Iterable, List, Tuple, Optional\n",
    "\n",
    "# cell to detect network bursts from the global spike train histogram (log-ISI)\n",
    "\n",
    "def detect_network_bursts_from_global_ISIs(\n",
    "    all_spike_times: Iterable[float],\n",
    "    hist_bins: int = 100,\n",
    "    smooth_sigma: float = 2.0,\n",
    "    max_isi_cutoff: float = 0.1,\n",
    "    void_threshold: float = 0.7,\n",
    "    min_spikes_in_burst: int = 5,\n",
    ") -> Tuple[List[Tuple[float, float, int]], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Detect bursts from the GLOBAL (concatenated) spike train using the Log-ISI\n",
    "    histogram + void-parameter method.\n",
    "\n",
    "    Inputs:\n",
    "    - all_spike_times: iterable of spike times (seconds) across the network\n",
    "    - hist_bins, smooth_sigma: histogram / smoothing params\n",
    "    - max_isi_cutoff: fallback maximum ISI cutoff (s)\n",
    "    - void_threshold: void parameter threshold (0..1)\n",
    "    - min_spikes_in_burst: minimum consecutive spikes to consider a burst\n",
    "\n",
    "    Returns:\n",
    "    - bursts: list of (burst_start, burst_end, n_spikes)\n",
    "    - isi_cutoff: the ISI cutoff (seconds) used for segmentation (or np.nan)\n",
    "    \"\"\"\n",
    "    all_spike_times = np.asarray(all_spike_times)\n",
    "    if all_spike_times.size == 0:\n",
    "        return [], np.nan\n",
    "\n",
    "    # ensure sorted global spike train\n",
    "    spike_times = np.sort(all_spike_times)\n",
    "\n",
    "    # ---- ISIs on GLOBAL network spike train ----\n",
    "    isis = np.diff(spike_times)\n",
    "    if isis.size == 0:\n",
    "        return [], np.nan\n",
    "\n",
    "    # ---- log transform ----\n",
    "    log_isis = np.log10(isis + 1e-12)\n",
    "\n",
    "    # ---- histogram ----\n",
    "    hist_counts, hist_edges = np.histogram(log_isis, bins=hist_bins)\n",
    "    hist_centers = (hist_edges[:-1] + hist_edges[1:]) / 2\n",
    "\n",
    "    # ---- smoothing ----\n",
    "    hist_smooth = gaussian_filter1d(hist_counts.astype(float), sigma=smooth_sigma)\n",
    "\n",
    "    \n",
    "\n",
    "    # ---- peak detection ----\n",
    "    peaks, props = find_peaks(hist_smooth, prominence=np.max(hist_smooth) * 0.1)\n",
    "\n",
    "    #plot the histogram and detected peaks for debugging\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(hist_centers, hist_smooth, label='Smoothed Log-ISI Histogram')\n",
    "    plt.plot(hist_centers[peaks], hist_smooth[peaks], \"x\", label='Detected Peaks')\n",
    "    plt.xlabel('Log10(ISI) [seconds]')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title('Log-ISI Histogram with Detected Peaks')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    if peaks.size == 0:\n",
    "        isi_cutoff = max_isi_cutoff\n",
    "    else:\n",
    "        # intraburst peak = largest peak left of max_isi_cutoff\n",
    "        log_cut = np.log10(max_isi_cutoff)\n",
    "        intraburst_cands = peaks[hist_centers[peaks] <= log_cut]\n",
    "        print(\"Intraburst candidates (indices):\", intraburst_cands)\n",
    "        if intraburst_cands.size == 0:\n",
    "            isi_cutoff = max_isi_cutoff\n",
    "        else:\n",
    "            intraburst_peak = intraburst_cands[np.argmax(hist_smooth[intraburst_cands])]\n",
    "            right_peaks = peaks[peaks > intraburst_peak]\n",
    "            print(\"Right peaks (indices):\", right_peaks)\n",
    "            if right_peaks.size == 0:\n",
    "                isi_cutoff = max_isi_cutoff\n",
    "            else:\n",
    "                # void parameter search\n",
    "                cutoff_idx = None\n",
    "                for rp in right_peaks:\n",
    "                    region = hist_smooth[intraburst_peak: rp + 1]\n",
    "                    if region.size == 0:\n",
    "                        continue\n",
    "                    min_idx = intraburst_peak + int(np.argmin(region))\n",
    "\n",
    "                    h1 = hist_smooth[intraburst_peak]\n",
    "                    h2 = hist_smooth[rp]\n",
    "                    hmin = hist_smooth[min_idx]\n",
    "\n",
    "                    void = (h1 + h2 - 2 * hmin) / (h1 + h2 + 1e-12)\n",
    "\n",
    "                    if void >= void_threshold:\n",
    "                        cutoff_idx = min_idx\n",
    "                        break\n",
    "\n",
    "                if cutoff_idx is None:\n",
    "                    isi_cutoff = max_isi_cutoff\n",
    "                else:\n",
    "                    isi_cutoff = 10 ** hist_centers[cutoff_idx]\n",
    "                    isi_cutoff = min(isi_cutoff, max_isi_cutoff)\n",
    "\n",
    "    # ---- detect network bursts from GLOBAL ISI threshold ----\n",
    "    bursts: List[Tuple[float, float, int]] = []\n",
    "    current = [0]  # indices into spike_times (start with first spike index)\n",
    "\n",
    "    for i, isi in enumerate(isis):\n",
    "        if isi <= isi_cutoff:\n",
    "            current.append(i + 1)\n",
    "        else:\n",
    "            if len(current) >= min_spikes_in_burst:\n",
    "                bursts.append((float(spike_times[current[0]]),\n",
    "                               float(spike_times[current[-1]]),\n",
    "                               len(current)))\n",
    "            current = [i + 1]\n",
    "\n",
    "    # final burst\n",
    "    if len(current) >= min_spikes_in_burst:\n",
    "        bursts.append((float(spike_times[current[0]]),\n",
    "                       float(spike_times[current[-1]]),\n",
    "                       len(current)))\n",
    "\n",
    "    return bursts, isi_cutoff\n",
    "\n",
    "all_spikes = np.sort(np.concatenate(list(spike_times.values())))\n",
    "bursts, isi_cutoff= detect_network_bursts_from_global_ISIs(\n",
    "    all_spikes,\n",
    "    max_isi_cutoff=0.1,\n",
    "    void_threshold=0.7,\n",
    "    min_spikes_in_burst=5,\n",
    ")\n",
    "print(\"Isi cutoff:\", isi_cutoff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8),sharex=True)\n",
    "# Define the ISI threshold for burst detection (e.g., 0.1 seconds)\n",
    "isi_threshold = 0.1\n",
    "# Detect bursts for each unit\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "# Extracting ISIs as combined arrays\n",
    "all_isis_within_bursts = np.concatenate([stats['isis_within_bursts'] for stats in burst_statistics.values() if stats['isis_within_bursts'].size > 0])\n",
    "all_isis_outside_bursts = np.concatenate([stats['isis_outside_bursts'] for stats in burst_statistics.values() if stats['isis_outside_bursts'].size > 0])\n",
    "all_isis = np.concatenate([stats['isis_all'] for stats in burst_statistics.values() if stats['isis_all'].size > 0])\n",
    "\n",
    "# Calculate combined statistics\n",
    "mean_isi_within_combined = np.mean(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "cov_isi_within_combined = np.cov(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_outside_combined = np.mean(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "cov_isi_outside_combined = np.cov(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_all_combined = np.mean(all_isis) if all_isis.size > 0 else np.nan\n",
    "cov_isi_all_combined = np.cov(all_isis) if all_isis.size > 0 else np.nan\n",
    "\n",
    "# Calculate spike counts for each unit\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "\n",
    "# Sort units by ascending spike counts\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "axs[0]= helper.plot_raster_with_bursts(axs[0],spike_times, bursts,sorted_units=sorted_units, title_suffix=\"(Sorted Raster Order)\")\n",
    "from parameter_free_burst_detector import plot_network_bursts_parameter_free\n",
    "# Call the plot_network_activity function and pass the SpikeTimes dictionary\n",
    "axs[1],network_data=  plot_network_bursts_parameter_free(\n",
    "    axs[1], \n",
    "    spike_times,\n",
    "\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_times =np.load('/mnt/Vol20tb1/user_workspaces/mmpatil/MEA_Analysis_MAINBRANCH/AnalyzedData/CDKL5_R59X_10112024_PS/CDKL5_R59X_10112024_PS/241108/M07305/Network/000227/well004//spikesorted_spike_times_dict.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8),sharex=True)\n",
    "# Define the ISI threshold for burst detection (e.g., 0.1 seconds)\n",
    "isi_threshold = 0.1\n",
    "# Detect bursts for each unit\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "# Extracting ISIs as combined arrays\n",
    "all_isis_within_bursts = np.concatenate([stats['isis_within_bursts'] for stats in burst_statistics.values() if stats['isis_within_bursts'].size > 0])\n",
    "all_isis_outside_bursts = np.concatenate([stats['isis_outside_bursts'] for stats in burst_statistics.values() if stats['isis_outside_bursts'].size > 0])\n",
    "all_isis = np.concatenate([stats['isis_all'] for stats in burst_statistics.values() if stats['isis_all'].size > 0])\n",
    "\n",
    "# Calculate combined statistics\n",
    "mean_isi_within_combined = np.mean(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "cov_isi_within_combined = np.cov(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_outside_combined = np.mean(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "cov_isi_outside_combined = np.cov(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_all_combined = np.mean(all_isis) if all_isis.size > 0 else np.nan\n",
    "cov_isi_all_combined = np.cov(all_isis) if all_isis.size > 0 else np.nan\n",
    "\n",
    "# Calculate spike counts for each unit\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "\n",
    "# Sort units by ascending spike counts\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "axs[0]= helper.plot_raster_with_bursts(axs[0],spike_times, bursts,sorted_units=sorted_units, title_suffix=\"(Sorted Raster Order)\")\n",
    "from logISINetworkBurst import plot_network_bursts_logISI\n",
    "# Call the plot_network_activity function and pass the SpikeTimes dictionary\n",
    "axs[1],network_data=  plot_network_bursts_logISI(\n",
    "    axs[1], \n",
    "    spike_times,\n",
    "    max_isi_cutoff=0.1,\n",
    "    void_threshold=0.7,\n",
    "    min_spikes_in_burst=5,\n",
    "    min_active_electrodes=0.1,   # 10% units\n",
    "    time_window=0.1,\n",
    "    min_active_threshold=0.1\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network_data.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8),sharex=True)\n",
    "# Define the ISI threshold for burst detection (e.g., 0.1 seconds)\n",
    "isi_threshold = 0.1\n",
    "# Detect bursts for each unit\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "# Extracting ISIs as combined arrays\n",
    "all_isis_within_bursts = np.concatenate([stats['isis_within_bursts'] for stats in burst_statistics.values() if stats['isis_within_bursts'].size > 0])\n",
    "all_isis_outside_bursts = np.concatenate([stats['isis_outside_bursts'] for stats in burst_statistics.values() if stats['isis_outside_bursts'].size > 0])\n",
    "all_isis = np.concatenate([stats['isis_all'] for stats in burst_statistics.values() if stats['isis_all'].size > 0])\n",
    "\n",
    "# Calculate combined statistics\n",
    "mean_isi_within_combined = np.mean(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "cov_isi_within_combined = np.cov(all_isis_within_bursts) if all_isis_within_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_outside_combined = np.mean(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "cov_isi_outside_combined = np.cov(all_isis_outside_bursts) if all_isis_outside_bursts.size > 0 else np.nan\n",
    "\n",
    "mean_isi_all_combined = np.mean(all_isis) if all_isis.size > 0 else np.nan\n",
    "cov_isi_all_combined = np.cov(all_isis) if all_isis.size > 0 else np.nan\n",
    "\n",
    "# Calculate spike counts for each unit\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "\n",
    "# Sort units by ascending spike counts\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "axs[0]= helper.plot_raster_with_bursts(axs[0],spike_times, bursts,sorted_units=sorted_units, title_suffix=\"(Sorted Raster Order)\")\n",
    "\n",
    "# Call the plot_network_activity function and pass the SpikeTimes dictionary\n",
    "axs[1],network_data= helper.plot_network_activity(axs[1],spike_times, figSize=(8, 4),binSize=0.1, gaussianSigma=0.2,min_peak_distance=10, thresholdBurst=2)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xlim(0, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_network_bursts_population_logISI(SpikeTimes,\n",
    "                                            plot_diagnostics=True,\n",
    "                                            tonic_iqr_factor=1.5,\n",
    "                                            hist_bins=200,\n",
    "                                            smooth_sigma=1.0,\n",
    "                                            gmm_fallback=True,\n",
    "                                            verbose=True):\n",
    "    \"\"\"\n",
    "    Parameter-light detection of network bursts using population log(ISI).\n",
    "    Handles mixed populations (tonic + burst-only units).\n",
    "\n",
    "    Inputs:\n",
    "    - SpikeTimes: dict {unit_id: np.array(spike_times in s)}\n",
    "    - plot_diagnostics: show histograms and burst overlays\n",
    "    - tonic_iqr_factor: multiplier for IQR rule (data-driven; default 1.5)\n",
    "    - hist_bins: bins for log-ISI histogram (moderate default)\n",
    "    - smooth_sigma: smoothing for visualization / peak finding (not a method parameter)\n",
    "    - gmm_fallback: use 2-component GMM on log-ISIs if histogram detection fails\n",
    "    - verbose: print diagnostic messages\n",
    "\n",
    "    Outputs:\n",
    "    - bursts: list of (burst_start, burst_end) inclusive (times in s)\n",
    "    - isi_cutoff: cutoff ISI in seconds (used to segment bursts)\n",
    "    - participation: dict {burst_idx: list of unit_ids that fired in that burst}\n",
    "    - diagnostics: dict with histograms, tonic_units, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------\n",
    "    # 1) Precompute per-unit ISIs and medians\n",
    "    # ----------------------\n",
    "    unit_ids = []\n",
    "    med_isis = []\n",
    "    unit_isis = {}\n",
    "    for uid, spikes in SpikeTimes.items():\n",
    "        spikes = np.array(spikes)\n",
    "        if spikes.size < 2:\n",
    "            unit_isis[uid] = np.array([])\n",
    "            med_isis.append(np.nan)\n",
    "        else:\n",
    "            isis = np.diff(np.sort(spikes))\n",
    "            unit_isis[uid] = isis\n",
    "            med_isis.append(np.median(isis))\n",
    "        unit_ids.append(uid)\n",
    "    med_isis = np.array(med_isis)\n",
    "\n",
    "    # ----------------------\n",
    "    # 2) Identify tonic candidates using IQR outlier rule on median ISI\n",
    "    #    (tonic => small median ISI relative to population)\n",
    "    # ----------------------\n",
    "    # Ignore NaNs when computing IQR\n",
    "    valid = ~np.isnan(med_isis)\n",
    "    med_valid = med_isis[valid]\n",
    "    q1, q3 = np.percentile(med_valid, [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    tonic_thresh = q1 - tonic_iqr_factor * iqr  # significantly below Q1 -> tonic\n",
    "    if np.isnan(tonic_thresh):\n",
    "        tonic_thresh = -np.inf  # guard\n",
    "    tonic_units = [uid for i, uid in enumerate(unit_ids) if valid[i] and med_isis[i] < tonic_thresh]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Detected {len(tonic_units)} tonic candidate units (IQR rule).\")\n",
    "\n",
    "    # ----------------------\n",
    "    # 3) Build population ISI sets\n",
    "    # ----------------------\n",
    "    # Collect all spikes with source unit labels\n",
    "    all_spikes = []\n",
    "    spike_sources = []\n",
    "    for uid, spikes in SpikeTimes.items():\n",
    "        s = np.array(spikes)\n",
    "        all_spikes.append(s)\n",
    "        spike_sources.append(np.full(s.shape, uid, dtype=object))\n",
    "    if len(all_spikes) == 0:\n",
    "        return [], np.nan, {}, {}\n",
    "    all_spikes = np.concatenate(all_spikes)\n",
    "    spike_sources = np.concatenate(spike_sources)\n",
    "    # Sort global spikes\n",
    "    sort_idx = np.argsort(all_spikes)\n",
    "    all_spikes = all_spikes[sort_idx]\n",
    "    spike_sources = spike_sources[sort_idx]\n",
    "\n",
    "    # global population ISIs (all spikes)\n",
    "    if all_spikes.size < 2:\n",
    "        return [], np.nan, {}, {}\n",
    "    pop_isis_all = np.diff(all_spikes)\n",
    "\n",
    "    # population ISIs excluding spikes that come from tonic units:\n",
    "    keep_mask = np.ones_like(all_spikes, dtype=bool)\n",
    "    if len(tonic_units) > 0:\n",
    "        tonic_mask = np.isin(spike_sources, tonic_units)\n",
    "        # Remove tonic spikes from the sequence (so diffs skip them)\n",
    "        keep_mask = ~tonic_mask\n",
    "        spikes_no_tonic = all_spikes[keep_mask]\n",
    "        if spikes_no_tonic.size >= 2:\n",
    "            pop_isis_no_tonic = np.diff(spikes_no_tonic)\n",
    "        else:\n",
    "            pop_isis_no_tonic = np.array([])\n",
    "    else:\n",
    "        pop_isis_no_tonic = pop_isis_all.copy()\n",
    "\n",
    "    # ----------------------\n",
    "    # 4) Compute log10(ISI) histogram (prefer no-tonic set)\n",
    "    # ----------------------\n",
    "    chosen_isis = pop_isis_no_tonic if pop_isis_no_tonic.size >= max(50, int(0.1 * pop_isis_all.size)) else pop_isis_all\n",
    "    if verbose:\n",
    "        print(f\"Using {'no-tonic' if chosen_isis is pop_isis_no_tonic else 'all'} population ISIs for bimodality detection (N={chosen_isis.size}).\")\n",
    "\n",
    "    log_isis = np.log10(chosen_isis + 1e-12)  # avoid zeros\n",
    "\n",
    "    # histogram + smoothing for peak detection (visual aid)\n",
    "    hist_counts, hist_edges = np.histogram(log_isis, bins=hist_bins)\n",
    "    hist_centers = (hist_edges[:-1] + hist_edges[1:]) / 2\n",
    "    hist_smooth = gaussian_filter1d(hist_counts.astype(float), sigma=smooth_sigma)\n",
    "\n",
    "    # find peaks\n",
    "    peaks, _ = find_peaks(hist_smooth, prominence=np.max(hist_smooth) * 0.05)\n",
    "    # if at least two peaks, pick the two largest peaks by height\n",
    "    isi_cutoff = None\n",
    "    used_method = None\n",
    "\n",
    "    if peaks.size >= 2:\n",
    "        # choose two highest peaks\n",
    "        peak_heights = hist_smooth[peaks]\n",
    "        top_idx = np.argsort(peak_heights)[-2:]  # indices into peaks\n",
    "        two_peaks = np.sort(peaks[top_idx])\n",
    "        left_p, right_p = two_peaks[0], two_peaks[1]\n",
    "        # find minimum between them\n",
    "        min_idx = np.argmin(hist_smooth[left_p:right_p+1]) + left_p\n",
    "        isi_cutoff = 10 ** hist_centers[min_idx]\n",
    "        used_method = 'hist_peaks'\n",
    "        if verbose:\n",
    "            print(f\"Histogram peak method found cutoff ISI = {isi_cutoff:.4f} s\")\n",
    "    else:\n",
    "        # fallback to GMM if enabled\n",
    "        if gmm_fallback:\n",
    "            if verbose:\n",
    "                print(\"Histogram unimodal or noisy — using 2-component GMM fallback on log(ISI).\")\n",
    "            # Fit GMM on log ISIs (use all ISIs if chosen set too small)\n",
    "            gmm_data = log_isis.reshape(-1, 1)\n",
    "            # Guard: require enough samples\n",
    "            if gmm_data.shape[0] >= 20:\n",
    "                gm = GaussianMixture(n_components=2, covariance_type='full', random_state=0)\n",
    "                gm.fit(gmm_data)\n",
    "                means = gm.means_.ravel()\n",
    "                # order by mean\n",
    "                order = np.argsort(means)\n",
    "                means = means[order]\n",
    "                covs = np.array([gm.covariances_[i].ravel()[0] for i in order])\n",
    "                weights = gm.weights_[order]\n",
    "                # compute intersection point of the two Gaussians analytically\n",
    "                m1, m2 = means[0], means[1]\n",
    "                s1, s2 = np.sqrt(covs[0]), np.sqrt(covs[1])\n",
    "                # Solve for x where N(x|m1,s1)=N(x|m2,s2) --> quadratic in x\n",
    "                a = 1/(2*s2*s2) - 1/(2*s1*s1)\n",
    "                b = m1/(s1*s1) - m2/(s2*s2)\n",
    "                c = m2*m2/(2*s2*s2) - m1*m1/(2*s1*s1) + np.log((s2*weights[0])/(s1*weights[1]) + 1e-16)\n",
    "                # quadratic: a x^2 + b x + c = 0\n",
    "                xs = np.roots([a, b, c])\n",
    "                xs_real = xs[np.isreal(xs)].real\n",
    "                if xs_real.size > 0:\n",
    "                    # pick intersection between the means\n",
    "                    x_candidates = [x for x in xs_real if m1 < x < m2]\n",
    "                    if len(x_candidates) == 0:\n",
    "                        # pick nearest to midpoint\n",
    "                        x_candidates = [xs_real[np.argmin(np.abs(xs_real - 0.5*(m1+m2)))]]\n",
    "                    x_cut = x_candidates[0]\n",
    "                    isi_cutoff = 10 ** x_cut\n",
    "                    used_method = 'gmm'\n",
    "                    if verbose:\n",
    "                        print(f\"GMM fallback found cutoff ISI = {isi_cutoff:.4f} s\")\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(\"GMM had no real intersection; falling back to median-based cutoff.\")\n",
    "                        isi_cutoff = np.median(chosen_isis)\n",
    "                        used_method = 'median_fallback'\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"Not enough data for GMM; falling back to histogram median cutoff.\")\n",
    "                isi_cutoff = np.median(chosen_isis)\n",
    "                used_method = 'median_fallback'\n",
    "        else:\n",
    "            # final fallback: median\n",
    "            if verbose:\n",
    "                print(\"Using median-based cutoff (no GMM).\")\n",
    "            isi_cutoff = np.median(chosen_isis)\n",
    "            used_method = 'median_fallback'\n",
    "\n",
    "    # ----------------------\n",
    "    # 5) Segment spikes into bursts using isi_cutoff (population-based)\n",
    "    #    This parameter-free segmentation: consecutive spikes with ISI < cutoff are grouped.\n",
    "    # ----------------------\n",
    "    # Work on all_spikes (sorted)\n",
    "    isis_all = np.diff(all_spikes)\n",
    "    # A new burst starts at spike i+1 if isis_all[i] >= isi_cutoff\n",
    "    # We'll build burst windows as [start_time, end_time] inclusive\n",
    "    bursts = []\n",
    "    burst_start_idx = 0\n",
    "    for i, isi in enumerate(isis_all):\n",
    "        if isi >= isi_cutoff:\n",
    "            # end current burst at spike i (index i)\n",
    "            burst_start = all_spikes[burst_start_idx]\n",
    "            burst_end = all_spikes[i]\n",
    "            bursts.append((burst_start, burst_end))\n",
    "            burst_start_idx = i + 1\n",
    "    # last burst\n",
    "    if burst_start_idx < all_spikes.size:\n",
    "        bursts.append((all_spikes[burst_start_idx], all_spikes[-1]))\n",
    "\n",
    "    # Optionally, drop very short bursts (but we prefer parameter-free; skip)\n",
    "    # ----------------------\n",
    "    # 6) Map participation (which units fired inside each burst)\n",
    "    # ----------------------\n",
    "    participation = {}\n",
    "    for bi, (bs, be) in enumerate(bursts):\n",
    "        # boolean mask on all_spikes\n",
    "        mask = (all_spikes >= bs) & (all_spikes <= be)\n",
    "        participants = np.unique(spike_sources[mask]).tolist()\n",
    "        participation[bi] = participants\n",
    "\n",
    "    # diagnostics\n",
    "    diagnostics = {\n",
    "        'tonic_units': tonic_units,\n",
    "        'tonic_thresh': tonic_thresh,\n",
    "        'hist_centers': hist_centers,\n",
    "        'hist_smooth': hist_smooth,\n",
    "        'used_method': used_method,\n",
    "        'chosen_isis_count': chosen_isis.size,\n",
    "        'pop_isis_all_count': pop_isis_all.size,\n",
    "    }\n",
    "\n",
    "    # ----------------------\n",
    "    # 7) Plot diagnostics if requested\n",
    "    # ----------------------\n",
    "    if plot_diagnostics:\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(10, 8), sharex=False)\n",
    "        # a) per-unit median ISI distribution (with tonic threshold)\n",
    "        axes[0].hist(med_valid, bins=50)\n",
    "        axes[0].axvline(tonic_thresh, color='red', linestyle='--', label='tonic threshold (IQR)')\n",
    "        axes[0].set_title('Per-unit median ISI (tonic units left of red line)')\n",
    "        axes[0].legend()\n",
    "\n",
    "        # b) population log(ISI) histogram (smooth) used\n",
    "        axes[1].plot(hist_centers, hist_smooth, '-k', label='smoothed hist(logISI)')\n",
    "        if peaks.size > 0:\n",
    "            axes[1].plot(hist_centers[peaks], hist_smooth[peaks], 'ro', label='peaks')\n",
    "        if isi_cutoff is not None:\n",
    "            axes[1].axvline(np.log10(isi_cutoff), color='blue', linestyle='--', label=f'cutoff={isi_cutoff:.4f}s')\n",
    "            axes[1].set_xlabel('log10(ISI)')\n",
    "\n",
    "        axes[1].set_title('Population log(ISI) smooth histogram')\n",
    "        axes[1].legend()\n",
    "\n",
    "        # c) raster zoom with burst windows overlay (optional small subset)\n",
    "        # For speed, plot only a subset of units for display\n",
    "        ax3 = axes[2]\n",
    "        # Build sparse raster: choose up to 100 units to plot\n",
    "        sample_units = list(SpikeTimes.keys())[:100]\n",
    "        y = 0\n",
    "        for uid in sample_units:\n",
    "            spikes = np.array(SpikeTimes[uid])\n",
    "            ax3.plot(spikes, np.full(spikes.shape, y), '|', markersize=2)\n",
    "            y += 1\n",
    "        for (bs, be) in bursts:\n",
    "            ax3.axvspan(bs, be, color='orange', alpha=0.12)\n",
    "        ax3.set_title('Raster (subset) with detected population bursts highlighted')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return bursts, isi_cutoff, participation, diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bursts, isi_cutoff, participation, diagnostics = detect_network_bursts_population_logISI(spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "spike_times = np.load(\"/mnt/Vol20tb1/user_workspaces/mmpatil/MEA_Analysis_MAINBRANCH/AnalyzedData/CDKL5_R59X_10112024_PS/CDKL5_R59X_10112024_PS/241018/M05506/Network/000037/well000/spikesorted_spike_times_dict.npy\", allow_pickle=True).item()\n",
    "bursts, isi_cutoff, participation, diagnostics = detect_network_bursts_population_logISI(spike_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Burst detection using LogISI method and network burst analysis.\n",
    "References:\n",
    "- Pasquale V, Massobrio P, Bologna L, Chiappalone M, Martinoia S. Self-organization and neuronal avalanches in networks of dissociated cortical neurons. Neuroscience. 2010 Mar 17;153(4):1354-69. doi: 10.1016/j.neuroscience.2008.12.050. Epub 2009 Jan 7. PMID: 19110017.\n",
    "\n",
    "Authors: Mandar M. Patil,LLM Assisted Edits: Yes ChatGPT-4\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def logISI_burst_detection(spike_times, \n",
    "                           max_isi_cutoff=0.1,  # 100 ms default cutoff\n",
    "                           void_threshold=0.7,  # Void parameter threshold\n",
    "                           min_spikes_in_burst=5,\n",
    "                           hist_bins=100,\n",
    "                           smooth_sigma=2.0,\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    Detect bursts using the LogISI method (Pasquale et al. 2010).\n",
    "    \n",
    "    This method analyzes the histogram of log-transformed interspike intervals (ISIs)\n",
    "    to automatically determine the optimal ISI threshold for burst detection.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Calculate all ISIs and transform to log scale\n",
    "    2. Create histogram of log(ISI) values\n",
    "    3. Find peaks in the histogram\n",
    "    4. Identify the \"intraburst peak\" (largest peak with ISI ≤ max_isi_cutoff)\n",
    "    5. Calculate \"void parameter\" between peaks to assess separation\n",
    "    6. Set ISI cutoff where void parameter exceeds threshold\n",
    "    7. Detect bursts as sequences of ≥3 spikes with ISI < cutoff\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spike_times : array-like\n",
    "        Array of spike times in seconds\n",
    "    max_isi_cutoff : float\n",
    "        Maximum allowed ISI cutoff value in seconds (default: 0.1 = 100 ms)\n",
    "    void_threshold : float\n",
    "        Threshold for void parameter (default: 0.7, range: 0-1)\n",
    "        Higher values = more stringent separation requirement\n",
    "    min_spikes_in_burst : int\n",
    "        Minimum number of spikes required to define a burst (default: 3)\n",
    "    hist_bins : int\n",
    "        Number of bins for log(ISI) histogram (default: 100)\n",
    "    smooth_sigma : float\n",
    "        Sigma for Gaussian smoothing of histogram (default: 2.0)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bursts : list of tuples\n",
    "        List of (burst_start_time, burst_end_time, num_spikes) for each detected burst\n",
    "    isi_cutoff : float\n",
    "        The ISI cutoff value determined by the algorithm (in seconds)\n",
    "    burst_info : dict\n",
    "        Dictionary containing detailed burst statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    spike_times = np.array(spike_times)\n",
    "    if len(spike_times) < min_spikes_in_burst:\n",
    "        return [], np.nan, {}\n",
    "    \n",
    "    #sort them\n",
    "    spike_times = np.sort(spike_times)\n",
    "\n",
    "    # Step 1: Calculate interspike intervals\n",
    "    isis = np.diff(spike_times)\n",
    "    \n",
    "    if len(isis) == 0:\n",
    "        return [], np.nan, {}\n",
    "    \n",
    "    # Step 2: Log-transform ISIs (add small value to avoid log(0))\n",
    "    log_isis = np.log10(isis + 1e-10)\n",
    "    \n",
    "    # Step 3: Create histogram of log(ISI) values\n",
    "    hist_counts, hist_edges = np.histogram(log_isis, bins=hist_bins)   # default 100 bins \n",
    "    hist_centers = (hist_edges[:-1] + hist_edges[1:]) / 2\n",
    "    \n",
    "    # Smooth the histogram for better peak detection\n",
    "    hist_counts_smooth = gaussian_filter1d(hist_counts.astype(float), sigma=smooth_sigma)\n",
    "    \n",
    "    # Step 4: Find peaks in the log(ISI) histogram\n",
    "    peaks, properties = find_peaks(hist_counts_smooth, prominence=np.max(hist_counts_smooth) * 0.1)\n",
    "    \n",
    "    if len(peaks) == 0:\n",
    "        # No clear peaks found - use default cutoff\n",
    "        isi_cutoff = max_isi_cutoff\n",
    "    else:\n",
    "        # Find the \"intraburst peak\" - largest peak corresponding to ISI ≤ max_isi_cutoff\n",
    "        log_max_cutoff = np.log10(max_isi_cutoff)\n",
    "        intraburst_candidates = peaks[hist_centers[peaks] <= log_max_cutoff]\n",
    "        \n",
    "        if len(intraburst_candidates) == 0:\n",
    "            # No peak found below max_isi_cutoff - use default\n",
    "            isi_cutoff = max_isi_cutoff\n",
    "        else:\n",
    "            # Select the largest peak (highest histogram count) as intraburst peak\n",
    "            intraburst_peak_idx = intraburst_candidates[np.argmax(hist_counts_smooth[intraburst_candidates])]\n",
    "            \n",
    "            # Step 5: Calculate void parameter between intraburst peak and subsequent peaks\n",
    "            # The void parameter quantifies how well-separated two peaks are\n",
    "            subsequent_peaks = peaks[peaks > intraburst_peak_idx]\n",
    "            \n",
    "            if len(subsequent_peaks) == 0:\n",
    "                # No subsequent peaks - use default cutoff\n",
    "                isi_cutoff = max_isi_cutoff\n",
    "            else:\n",
    "                # Find minima between intraburst peak and each subsequent peak\n",
    "                void_params = []\n",
    "                cutoff_positions = []\n",
    "                \n",
    "                for next_peak in subsequent_peaks:\n",
    "                    # Find minimum between the two peaks\n",
    "                    search_region = hist_counts_smooth[intraburst_peak_idx:next_peak+1]\n",
    "                    if len(search_region) > 0:\n",
    "                        min_idx = np.argmin(search_region) + intraburst_peak_idx\n",
    "                        \n",
    "                        # Calculate void parameter\n",
    "                        # Void = (height_peak1 + height_peak2 - 2*height_min) / (height_peak1 + height_peak2)\n",
    "                        h1 = hist_counts_smooth[intraburst_peak_idx]\n",
    "                        h2 = hist_counts_smooth[next_peak]\n",
    "                        h_min = hist_counts_smooth[min_idx]\n",
    "                        \n",
    "                        if (h1 + h2) > 0:\n",
    "                            void_param = (h1 + h2 - 2*h_min) / (h1 + h2)\n",
    "                            void_params.append(void_param)\n",
    "                            cutoff_positions.append(min_idx)\n",
    "                \n",
    "                # Step 6: Set ISI cutoff at first minimum where void parameter exceeds threshold\n",
    "                if len(void_params) > 0:\n",
    "                    valid_cutoffs = [cutoff_positions[i] for i, vp in enumerate(void_params) \n",
    "                                   if vp >= void_threshold]\n",
    "                    \n",
    "                    if len(valid_cutoffs) > 0:\n",
    "                        cutoff_idx = valid_cutoffs[0]\n",
    "                        isi_cutoff = 10 ** hist_centers[cutoff_idx]\n",
    "                        \n",
    "                        # Ensure cutoff doesn't exceed maximum\n",
    "                        if isi_cutoff > max_isi_cutoff:\n",
    "                            isi_cutoff = max_isi_cutoff\n",
    "                    else:\n",
    "                        # No void parameter exceeded threshold - use default\n",
    "                        isi_cutoff = max_isi_cutoff\n",
    "                else:\n",
    "                    isi_cutoff = max_isi_cutoff\n",
    "    \n",
    "    # Step 7: Detect bursts using the determined ISI cutoff\n",
    "    bursts = []\n",
    "    current_burst_spikes = [0]  # Start with first spike index\n",
    "    \n",
    "    for i in range(len(isis)):\n",
    "        if isis[i] <= isi_cutoff:\n",
    "            # Continue current burst\n",
    "            current_burst_spikes.append(i + 1)\n",
    "        else:\n",
    "            # End current burst and check if valid\n",
    "            if len(current_burst_spikes) >= min_spikes_in_burst:\n",
    "                burst_start = spike_times[current_burst_spikes[0]]\n",
    "                burst_end = spike_times[current_burst_spikes[-1]]\n",
    "                num_spikes = len(current_burst_spikes)\n",
    "                bursts.append((burst_start, burst_end, num_spikes))\n",
    "            \n",
    "            # Start new burst\n",
    "            current_burst_spikes = [i + 1]\n",
    "    \n",
    "    # Check last burst\n",
    "    if len(current_burst_spikes) >= min_spikes_in_burst:\n",
    "        burst_start = spike_times[current_burst_spikes[0]]\n",
    "        burst_end = spike_times[current_burst_spikes[-1]]\n",
    "        num_spikes = len(current_burst_spikes)\n",
    "        bursts.append((burst_start, burst_end, num_spikes))\n",
    "    \n",
    "    # Calculate burst statistics\n",
    "    if len(bursts) > 0:\n",
    "        burst_durations = [b[1] - b[0] for b in bursts]\n",
    "        spikes_per_burst = [b[2] for b in bursts]\n",
    "        \n",
    "        # Calculate inter-burst intervals (offset to onset)\n",
    "        if len(bursts) > 1:\n",
    "            ibis = [bursts[i+1][0] - bursts[i][1] for i in range(len(bursts)-1)]\n",
    "            mean_ibi = np.mean(ibis)\n",
    "            cov_ibi = np.std(ibis) / mean_ibi if mean_ibi > 0 else np.nan\n",
    "        else:\n",
    "            mean_ibi = np.nan\n",
    "            cov_ibi = np.nan\n",
    "        \n",
    "        # Count spikes in bursts\n",
    "        total_burst_spikes = sum(spikes_per_burst)\n",
    "        burst_percentage = (total_burst_spikes / len(spike_times)) * 100\n",
    "        \n",
    "        burst_info = {\n",
    "            'num_bursts': len(bursts),\n",
    "            'mean_burst_duration': np.mean(burst_durations),\n",
    "            'std_burst_duration': np.std(burst_durations),\n",
    "            'cov_burst_duration': np.std(burst_durations) / np.mean(burst_durations) if np.mean(burst_durations) > 0 else np.nan,\n",
    "            'mean_spikes_per_burst': np.mean(spikes_per_burst),\n",
    "            'std_spikes_per_burst': np.std(spikes_per_burst),\n",
    "            'mean_ibi': mean_ibi,\n",
    "            'cov_ibi': cov_ibi,\n",
    "            'burst_percentage': burst_percentage,\n",
    "            'isi_cutoff_used': isi_cutoff\n",
    "        }\n",
    "    else:\n",
    "        burst_info = {\n",
    "            'num_bursts': 0,\n",
    "            'mean_burst_duration': np.nan,\n",
    "            'std_burst_duration': np.nan,\n",
    "            'cov_burst_duration': np.nan,\n",
    "            'mean_spikes_per_burst': np.nan,\n",
    "            'std_spikes_per_burst': np.nan,\n",
    "            'mean_ibi': np.nan,\n",
    "            'cov_ibi': np.nan,\n",
    "            'burst_percentage': 0.0,\n",
    "            'isi_cutoff_used': isi_cutoff\n",
    "        }\n",
    "    \n",
    "    return bursts, isi_cutoff, burst_info\n",
    "\n",
    "\n",
    "def ISI_N_burst_detection(spike_times,\n",
    "                         N=3,  # Minimum number of consecutive spikes\n",
    "                         ISI_threshold=0.1,  # Maximum ISI in seconds\n",
    "                         min_spikes_in_burst=3):\n",
    "    \"\"\"\n",
    "    Detect bursts using the ISI_N method (simple threshold-based).\n",
    "    \n",
    "    A burst is defined as N or more consecutive spikes with ISI < ISI_threshold.\n",
    "    This is a simpler, more deterministic alternative to logISI.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    spike_times : array-like\n",
    "        Array of spike times in seconds\n",
    "    N : int\n",
    "        Minimum number of consecutive spikes required (default: 3)\n",
    "    ISI_threshold : float\n",
    "        Maximum ISI for spikes to be considered in a burst (seconds, default: 0.1)\n",
    "    min_spikes_in_burst : int\n",
    "        Minimum spikes in a burst (usually same as N, default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    bursts : list of tuples\n",
    "        List of (burst_start_time, burst_end_time, num_spikes) for each detected burst\n",
    "    burst_info : dict\n",
    "        Dictionary containing detailed burst statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    spike_times = np.array(spike_times)\n",
    "    if len(spike_times) < min_spikes_in_burst:\n",
    "        return [], {}\n",
    "    \n",
    "    # Calculate ISIs\n",
    "    isis = np.diff(spike_times)\n",
    "    \n",
    "    if len(isis) == 0:\n",
    "        return [], {}\n",
    "    \n",
    "    # Detect bursts\n",
    "    bursts = []\n",
    "    current_burst_spikes = [0]  # Start with first spike\n",
    "    \n",
    "    for i in range(len(isis)):\n",
    "        if isis[i] <= ISI_threshold:\n",
    "            # Continue current burst\n",
    "            current_burst_spikes.append(i + 1)\n",
    "        else:\n",
    "            # End current burst and check if valid\n",
    "            if len(current_burst_spikes) >= min_spikes_in_burst:\n",
    "                burst_start = spike_times[current_burst_spikes[0]]\n",
    "                burst_end = spike_times[current_burst_spikes[-1]]\n",
    "                num_spikes = len(current_burst_spikes)\n",
    "                bursts.append((burst_start, burst_end, num_spikes))\n",
    "            \n",
    "            # Start new burst\n",
    "            current_burst_spikes = [i + 1]\n",
    "    \n",
    "    # Check last burst\n",
    "    if len(current_burst_spikes) >= min_spikes_in_burst:\n",
    "        burst_start = spike_times[current_burst_spikes[0]]\n",
    "        burst_end = spike_times[current_burst_spikes[-1]]\n",
    "        num_spikes = len(current_burst_spikes)\n",
    "        bursts.append((burst_start, burst_end, num_spikes))\n",
    "    \n",
    "    # Calculate statistics\n",
    "    if len(bursts) > 0:\n",
    "        burst_durations = [b[1] - b[0] for b in bursts]\n",
    "        spikes_per_burst = [b[2] for b in bursts]\n",
    "        \n",
    "        if len(bursts) > 1:\n",
    "            ibis = [bursts[i+1][0] - bursts[i][1] for i in range(len(bursts)-1)]\n",
    "            mean_ibi = np.mean(ibis)\n",
    "            cov_ibi = np.std(ibis) / mean_ibi if mean_ibi > 0 else np.nan\n",
    "        else:\n",
    "            mean_ibi = np.nan\n",
    "            cov_ibi = np.nan\n",
    "        \n",
    "        total_burst_spikes = sum(spikes_per_burst)\n",
    "        burst_percentage = (total_burst_spikes / len(spike_times)) * 100\n",
    "        \n",
    "        burst_info = {\n",
    "            'num_bursts': len(bursts),\n",
    "            'mean_burst_duration': np.mean(burst_durations),\n",
    "            'std_burst_duration': np.std(burst_durations),\n",
    "            'cov_burst_duration': np.std(burst_durations) / np.mean(burst_durations) if np.mean(burst_durations) > 0 else np.nan,\n",
    "            'mean_spikes_per_burst': np.mean(spikes_per_burst),\n",
    "            'std_spikes_per_burst': np.std(spikes_per_burst),\n",
    "            'mean_ibi': mean_ibi,\n",
    "            'cov_ibi': cov_ibi,\n",
    "            'burst_percentage': burst_percentage\n",
    "        }\n",
    "    else:\n",
    "        burst_info = {\n",
    "            'num_bursts': 0,\n",
    "            'mean_burst_duration': np.nan,\n",
    "            'std_burst_duration': np.nan,\n",
    "            'cov_burst_duration': np.nan,\n",
    "            'mean_spikes_per_burst': np.nan,\n",
    "            'std_spikes_per_burst': np.nan,\n",
    "            'mean_ibi': np.nan,\n",
    "            'cov_ibi': np.nan,\n",
    "            'burst_percentage': 0.0\n",
    "        }\n",
    "    \n",
    "    return bursts, burst_info\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_network_bursts_logISI(ax, SpikeTimes,\n",
    "                                max_isi_cutoff=0.1,\n",
    "                                void_threshold=0.7,\n",
    "                                min_spikes_in_burst=5,\n",
    "                                min_active_electrodes=0.1,  # Fraction of electrodes that must burst\n",
    "                                time_window=0.1,  # 100 ms window for synchrony detection\n",
    "                                min_active_threshold=0.1): \n",
    "    \"\"\"\n",
    "    Detect NETWORK bursts using LogISI method applied to individual electrodes,\n",
    "    then aggregating across the network.\n",
    "    \n",
    "    Network burst = when a sufficient fraction of electrodes burst within a time window.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Apply LogISI burst detection to each electrode independently\n",
    "    2. Slide a time window through the recording\n",
    "    3. Count how many electrodes are bursting at each time point\n",
    "    4. Network burst = when ≥ threshold electrodes burst simultaneously\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib axis object\n",
    "        Axis to plot on\n",
    "    SpikeTimes : dict\n",
    "        Dictionary where keys are unit/electrode IDs and values are arrays of spike times (in seconds)\n",
    "    max_isi_cutoff : float\n",
    "        Maximum allowed ISI cutoff for LogISI (default: 0.1 = 100 ms)\n",
    "    void_threshold : float\n",
    "        Void parameter threshold for LogISI (default: 0.7)\n",
    "    min_spikes_in_burst : int\n",
    "        Minimum spikes per single-electrode burst (default: 3)\n",
    "    min_active_electrodes : float or int\n",
    "        If float < 1: fraction of active electrodes that must participate\n",
    "        If int >= 1: minimum number of electrodes that must participate\n",
    "        (default: 0.25 = 25% of electrodes)\n",
    "    time_window : float\n",
    "        Time window for detecting synchronous bursting (default: 0.05 = 50 ms)\n",
    "    min_active_threshold : float\n",
    "        Minimum firing rate (Hz) for an electrode to be considered active\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    network_data : dict\n",
    "        Dictionary containing network burst metrics including:\n",
    "        - num_network_bursts: Number of detected network bursts\n",
    "        - network_burst_rate: Network bursts per second\n",
    "        - mean_network_burst_duration: Mean network burst duration (seconds)\n",
    "        - cov_network_burst_duration: Coefficient of variation of duration\n",
    "        - mean_ibi: Mean inter-burst interval (seconds)\n",
    "        - cov_ibi: Coefficient of variation of IBI\n",
    "        - mean_spikes_per_network_burst: Average spikes per network burst\n",
    "        - mean_electrodes_per_network_burst: Average electrode participation\n",
    "        - network_burst_percentage: Percentage of spikes in network bursts\n",
    "        - active_electrodes: Number of active electrodes\n",
    "        - bursting_electrodes: Number of electrodes with detected bursts\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Detect bursts on each electrode using LogISI\n",
    "    electrode_bursts = {}\n",
    "    active_electrodes = []\n",
    "    all_spike_times = []\n",
    "    \n",
    "    for electrode_id, spike_times in SpikeTimes.items():\n",
    "        if len(spike_times) == 0:\n",
    "            continue\n",
    "            \n",
    "        all_spike_times.extend(spike_times)\n",
    "        \n",
    "        # Check if electrode is active\n",
    "        recording_duration = max(spike_times) - min(spike_times) if len(spike_times) > 1 else 1.0\n",
    "        firing_rate = len(spike_times) / recording_duration\n",
    "        \n",
    "        if firing_rate < min_active_threshold:\n",
    "            continue\n",
    "        \n",
    "        active_electrodes.append(electrode_id)\n",
    "        \n",
    "        # Detect bursts on this electrode\n",
    "        bursts, isi_cutoff, burst_info = logISI_burst_detection(\n",
    "            spike_times, max_isi_cutoff, void_threshold, min_spikes_in_burst\n",
    "        )\n",
    "        \n",
    "        if len(bursts) > 0:\n",
    "            electrode_bursts[electrode_id] = {\n",
    "                'bursts': bursts,\n",
    "                'isi_cutoff': isi_cutoff,\n",
    "                'info': burst_info\n",
    "            }\n",
    "    \n",
    "    if len(active_electrodes) == 0:\n",
    "        print(\"Warning: No active electrodes detected!\")\n",
    "        return None\n",
    "    \n",
    "    if len(electrode_bursts) == 0:\n",
    "        print(\"Warning: No bursts detected on any electrode!\")\n",
    "        return None\n",
    "    \n",
    "    # Determine minimum number of electrodes needed for network burst\n",
    "    if isinstance(min_active_electrodes, float) and min_active_electrodes < 1:\n",
    "        min_electrodes_threshold = int(np.ceil(min_active_electrodes * len(active_electrodes)))\n",
    "    else:\n",
    "        min_electrodes_threshold = int(min_active_electrodes)\n",
    "    \n",
    "    # Step 2: Find network bursts by detecting temporal overlap\n",
    "    all_spike_times = np.array(sorted(all_spike_times))\n",
    "    if len(all_spike_times) == 0:\n",
    "        return None\n",
    "    \n",
    "    recording_start = all_spike_times[0]\n",
    "    recording_end = all_spike_times[-1]\n",
    "    recording_duration = recording_end - recording_start\n",
    "    \n",
    "    # Slide a window through time and count how many electrodes are bursting\n",
    "    time_step = time_window / 2  # Overlap windows by 50%\n",
    "    time_points = np.arange(recording_start, recording_end, time_step)\n",
    "    \n",
    "    electrodes_bursting_count = []\n",
    "    \n",
    "    for t in time_points:\n",
    "        count = 0\n",
    "        for electrode_id, burst_data in electrode_bursts.items():\n",
    "            for burst_start, burst_end, num_spikes in burst_data['bursts']:\n",
    "                # Check if this burst overlaps with current time window\n",
    "                if burst_start <= t + time_window and burst_end >= t:\n",
    "                    count += 1\n",
    "                    break  # Count each electrode only once per time point\n",
    "        electrodes_bursting_count.append(count)\n",
    "    \n",
    "    electrodes_bursting_count = np.array(electrodes_bursting_count)\n",
    "\n",
    "    smoothing_sigma =1.0\n",
    "    electrode_bursting_count_smooth = gaussian_filter1d(electrodes_bursting_count.astype(float), sigma=smoothing_sigma)\n",
    "\n",
    "    \n",
    "    # Step 3: Identify network bursts where enough electrodes burst simultaneously\n",
    "    network_burst_active = electrode_bursting_count_smooth >= min_electrodes_threshold\n",
    "    \n",
    "    # Find continuous regions of network bursting\n",
    "    network_bursts = []\n",
    "    in_network_burst = False\n",
    "    current_burst_start = None\n",
    "    current_burst_max_electrodes = 0\n",
    "    \n",
    "    for i, is_active in enumerate(network_burst_active):\n",
    "        if is_active and not in_network_burst:\n",
    "            # Start of network burst\n",
    "            in_network_burst = True\n",
    "            current_burst_start = time_points[i]\n",
    "            current_burst_max_electrodes = electrodes_bursting_count[i]\n",
    "        elif is_active and in_network_burst:\n",
    "            # Continue network burst\n",
    "            current_burst_max_electrodes = max(current_burst_max_electrodes, \n",
    "                                               electrodes_bursting_count[i])\n",
    "        elif not is_active and in_network_burst:\n",
    "            # End of network burst\n",
    "            in_network_burst = False\n",
    "            current_burst_end = time_points[i-1] + time_window\n",
    "            \n",
    "            # Count total spikes in this network burst\n",
    "            spikes_in_burst = np.sum((all_spike_times >= current_burst_start) & \n",
    "                                    (all_spike_times <= current_burst_end))\n",
    "            \n",
    "            network_bursts.append((current_burst_start, current_burst_end, \n",
    "                                  spikes_in_burst, current_burst_max_electrodes))\n",
    "    \n",
    "    # Check if recording ended during a network burst\n",
    "    if in_network_burst:\n",
    "        current_burst_end = time_points[-1] + time_window\n",
    "        spikes_in_burst = np.sum((all_spike_times >= current_burst_start) & \n",
    "                                (all_spike_times <= current_burst_end))\n",
    "        network_bursts.append((current_burst_start, current_burst_end, \n",
    "                              spikes_in_burst, current_burst_max_electrodes))\n",
    "    #Drop bursts shorter than min duration\n",
    "    min_network_burst_duration = 0.1  # 100 ms\n",
    "    network_bursts = [nb for nb in network_bursts if (nb[1] - nb[0]) >= min_network_burst_duration]\n",
    "    #Merge bursts separated by less than min duration\n",
    "    min_gap_to_merge = 0.05  # 50 ms\n",
    "    if len(network_bursts) > 1:\n",
    "        network_bursts = sorted(network_bursts, key=lambda x: x[0])\n",
    "        merged_bursts = []\n",
    "        for nb in network_bursts:\n",
    "            if not merged_bursts:\n",
    "                merged_bursts.append(nb)\n",
    "                continue\n",
    "            last = merged_bursts[-1]\n",
    "            last_start, last_end, last_spikes, last_electrodes = last\n",
    "            this_start, this_end, this_spikes, this_electrodes = nb\n",
    "\n",
    "            #compute gap\n",
    "            gap = this_start - last_end\n",
    "            if gap <= min_gap_to_merge:\n",
    "                last[1] = this_end\n",
    "                last[2] = last_spikes + this_spikes\n",
    "                last[3] = max(last_electrodes, this_electrodes)\n",
    "            else:\n",
    "                merged_bursts.append(list(nb))\n",
    "        network_bursts = [tuple(nb) for nb in merged_bursts]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 4: Calculate network burst statistics\n",
    "    if len(network_bursts) > 0:\n",
    "        burst_durations = [nb[1] - nb[0] for nb in network_bursts]\n",
    "        spikes_per_burst = [nb[2] for nb in network_bursts]\n",
    "        electrodes_per_burst = [nb[3] for nb in network_bursts]\n",
    "        \n",
    "        if len(network_bursts) > 1:\n",
    "            ibis = [network_bursts[i+1][0] - network_bursts[i][1] \n",
    "                   for i in range(len(network_bursts)-1)]\n",
    "            mean_ibi = np.mean(ibis)\n",
    "            cov_ibi = np.std(ibis) / mean_ibi if mean_ibi > 0 else np.nan\n",
    "        else:\n",
    "            mean_ibi = np.nan\n",
    "            cov_ibi = np.nan\n",
    "        \n",
    "        total_burst_spikes = sum(spikes_per_burst)\n",
    "        network_burst_percentage = (total_burst_spikes / len(all_spike_times)) * 100\n",
    "        \n",
    "        network_data = {\n",
    "            'num_network_bursts': len(network_bursts),\n",
    "            'network_burst_rate': len(network_bursts) / recording_duration,\n",
    "            'mean_network_burst_duration': np.mean(burst_durations),\n",
    "            'std_network_burst_duration': np.std(burst_durations),\n",
    "            'cov_network_burst_duration': np.std(burst_durations) / np.mean(burst_durations) if np.mean(burst_durations) > 0 else np.nan,\n",
    "            'mean_ibi': mean_ibi,\n",
    "            'cov_ibi': cov_ibi,\n",
    "            'mean_spikes_per_network_burst': np.mean(spikes_per_burst),\n",
    "            'mean_electrodes_per_network_burst': np.mean(electrodes_per_burst),\n",
    "            'network_burst_percentage': network_burst_percentage,\n",
    "            'active_electrodes': len(active_electrodes),\n",
    "            'bursting_electrodes': len(electrode_bursts),\n",
    "            'total_spikes': len(all_spike_times),\n",
    "            'recording_duration': recording_duration\n",
    "        }\n",
    "    else:\n",
    "        network_data = {\n",
    "            'num_network_bursts': 0,\n",
    "            'network_burst_rate': 0.0,\n",
    "            'mean_network_burst_duration': np.nan,\n",
    "            'std_network_burst_duration': np.nan,\n",
    "            'cov_network_burst_duration': np.nan,\n",
    "            'mean_ibi': np.nan,\n",
    "            'cov_ibi': np.nan,\n",
    "            'mean_spikes_per_network_burst': np.nan,\n",
    "            'mean_electrodes_per_network_burst': np.nan,\n",
    "            'network_burst_percentage': 0.0,\n",
    "            'active_electrodes': len(active_electrodes),\n",
    "            'bursting_electrodes': len(electrode_bursts),\n",
    "            'total_spikes': len(all_spike_times),\n",
    "            'recording_duration': recording_duration\n",
    "        }\n",
    "    \n",
    "    # Step 5: Plot network burst activity\n",
    "    # Plot electrode participation over time\n",
    "    ax.plot(time_points, electrodes_bursting_count, 'b-', linewidth=1.5, \n",
    "           label='Electrodes Bursting')\n",
    "    \n",
    "    # Mark threshold\n",
    "    ax.axhline(y=min_electrodes_threshold, color='gray', linestyle='--', \n",
    "              linewidth=1.5, alpha=0.7, \n",
    "              label=f'Threshold ({min_electrodes_threshold} electrodes)')\n",
    "    \n",
    "    # Highlight network bursts\n",
    "    for nb_start, nb_end, nb_spikes, nb_electrodes in network_bursts:\n",
    "        ax.axvspan(nb_start, nb_end, alpha=0.3, color='gray')\n",
    "    \n",
    "    # Mark network burst peaks\n",
    "    if len(network_bursts) > 0:\n",
    "        burst_centers = [(nb[0] + nb[1]) / 2 for nb in network_bursts]\n",
    "        burst_peaks = [nb[3] for nb in network_bursts]\n",
    "        ax.plot(burst_centers, burst_peaks, 'r*', markersize=3, \n",
    "               label=f'Network Bursts (n={len(network_bursts)})')\n",
    "    \n",
    "    # Set axis properties\n",
    "    ax.set_xlim([recording_start, recording_end])\n",
    "    ax.set_ylim([0, len(active_electrodes) * 1.1])\n",
    "    ax.set_ylabel('Number of Bursting Electrodes', fontsize=11)\n",
    "    ax.set_xlabel('Time (s)', fontsize=11)\n",
    "    ax.set_title(f'Network Burst Detection (LogISI) - {len(network_bursts)} Network Bursts', \n",
    "                fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return ax, network_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gaussianNetworkBursts import plot_network_activity as pna_gaussian\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "isi_threshold = 0.1\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "# Raster panel (unchanged)\n",
    "helper.plot_raster_with_bursts(\n",
    "    axs[0],\n",
    "    spike_times,\n",
    "    bursts,\n",
    "    sorted_units=sorted_units,\n",
    "    title_suffix=\"(Sorted Raster Order)\"\n",
    ")\n",
    "\n",
    "# NEW: use your improved Gaussian-based network activity function\n",
    "network_data = pna_gaussian(\n",
    "    axs[1],\n",
    "    spike_times,\n",
    "    binSize=0.01,          # finer bins\n",
    "    gaussianSigma=0.1,     # 100 ms\n",
    "    thresholdBurst=3.0,\n",
    "    min_peak_distance=1.0,\n",
    "    min_active_threshold=0.1\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "axs[1].set_xlim(0, 60)    # or derive from network_data if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logISINetworkBurst import plot_network_bursts_logISI  as pna_logISI\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "# still fine to use your old per-unit burst stats for raster,\n",
    "# OR you can eventually swap to per-unit logISI (below in section 3)\n",
    "isi_threshold = 0.1\n",
    "burst_statistics = helper.detect_bursts_statistics(spike_times, isi_threshold)\n",
    "bursts = [unit_stats['bursts'] for unit_stats in burst_statistics.values()]\n",
    "\n",
    "spike_counts = {unit: len(times) for unit, times in spike_times.items()}\n",
    "sorted_units = sorted(spike_counts, key=spike_counts.get)\n",
    "\n",
    "helper.plot_raster_with_bursts(\n",
    "    axs[0],\n",
    "    spike_times,\n",
    "    bursts,\n",
    "    sorted_units=sorted_units,\n",
    "    title_suffix=\"(Sorted Raster Order)\"\n",
    ")\n",
    "\n",
    "# NEW: LogISI-based network bursts\n",
    "network_data_logisi = pna_logISI(\n",
    "    axs[1],\n",
    "    spike_times,\n",
    "    max_isi_cutoff=0.1,\n",
    "    void_threshold=0.7,\n",
    "    min_spikes_in_burst=3,\n",
    "    min_active_electrodes=0.25,      # 25% of active electrodes\n",
    "    time_window=0.05,                # 50 ms window\n",
    "    min_active_threshold=0.1         # Hz\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "axs[1].set_xlim(8, 18)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "si_ks4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
