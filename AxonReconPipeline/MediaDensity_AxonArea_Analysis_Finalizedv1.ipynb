{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5054eb30",
   "metadata": {},
   "source": [
    "## AXON TRACKING ANALYSIS NOTEBOOK\n",
    "### Part 1: Loading the data \n",
    "Instructions: \n",
    "1. root_path  = Path (\"ENTER FILE PATH FOR SCOPE EXPORTED CSV FILES HERE\")\n",
    "2. run the first cell - this will read all the csv files in each sub folder and preprocess it for analysis  \n",
    "\n",
    "All Relevant CSV Files in Scope Data:   \n",
    "branch_metrics.csv  \n",
    "axon_summary_metrics.csv   \n",
    "neuron_metrics.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Root directory\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/FolicAcid_T4_02252025_SA/Folic_Acid_T4_02252025_SA_AxonTracking_CSVs/\")\n",
    "\n",
    "# This will store all the data\n",
    "data_dict = {}\n",
    "\n",
    "# Walk through each subfolder\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        # Extract run number from folder name (after last \"_\")\n",
    "        parts = run_folder.name.split(\"_\")\n",
    "        # Only use run number if there are at least 3 parts and the last part is numeric\n",
    "        if len(parts) >= 3 or not parts[-1].isdigit():\n",
    "            run_number = parts[-1]\n",
    "        else: \n",
    "            continue\n",
    "        # Drop the first row of every folder\n",
    "        run_data = {}\n",
    "        # Path to csv folder\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                # Remove \".csv\" from filename to use as key\n",
    "                key_name = file.stem  # e.g., branch_metrics, axon_summary_metrics\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file, header = 1)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53117316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and print all run numbers from data dictionary\n",
    "all_run_numbers = list(data_dict.keys())\n",
    "print(f\"Data loaded for {len(all_run_numbers)} runs.\")\n",
    "print(\"Extracted run numbers:\", all_run_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c1ced1",
   "metadata": {},
   "source": [
    "### various metrics you can define 'column name' as to generate plots such as: \n",
    "\n",
    "Branch Length  \t\n",
    "Branch Conduction Velocity\t  \n",
    "Conduction Velocity Fit\t  \n",
    "Latency From Initiation Site\t  \n",
    "Distance From Initiation Site\t  \n",
    "Total Number of Spikes\t  \n",
    "Neuron Firing Rate\t  \n",
    "ISI Violation Rate\t  \n",
    "Fisher Projection Distance\t  \n",
    "Silhouette Score\t  \n",
    "Mean Number Spikes per Config\t  \n",
    "Footprint Completeness  \n",
    "\n",
    "### Use the Dropdown menu to generate custom plots     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9cb20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from ipywidgets import widgets, interact\n",
    "\n",
    "def get_column_titles_from_second_row(data_dict):\n",
    "    column_titles = set()\n",
    "    for file_dict in data_dict.values():\n",
    "        for df in file_dict.values():\n",
    "            if len(df) > 1:  # Ensure there is a second row\n",
    "                second_row = df.iloc[1]\n",
    "                column_titles.update(second_row.index)\n",
    "    return sorted(column_titles)\n",
    "\n",
    "def search_column_across_runs(data_dict, column_name):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        # Extract numeric part of run_num\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        for file_name, df in file_dict.items():\n",
    "            if column_name in df.columns:\n",
    "                for val in df[column_name].dropna():\n",
    "                    results.append({\n",
    "                        \"run_number\": numeric_run_num,  # Ensure numeric sorting\n",
    "                        \"file\": file_name,\n",
    "                        \"value\": val\n",
    "                    })\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values(by='run_number').reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Extract column titles from the second row\n",
    "column_titles = get_column_titles_from_second_row(data_dict)\n",
    "\n",
    "# Create a dropdown widget for column selection\n",
    "dropdown = widgets.Dropdown(\n",
    "    options=column_titles,\n",
    "    description='Column:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Function to display results based on dropdown selection\n",
    "def display_results(column_name):\n",
    "    global results_df  # Make results_df accessible globally\n",
    "    if column_name:\n",
    "        results_df = search_column_across_runs(data_dict, column_name)\n",
    "        display(results_df)\n",
    "\n",
    "# Use interact to link the dropdown with the display function\n",
    "interact(display_results, column_name=dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2427184",
   "metadata": {},
   "source": [
    "### decrease jitter by editing this following line\n",
    "\n",
    "sns.stripplot(\n",
    "    x='run_number', y='value', data=results_df_cleaned,\n",
    "    **jitter=0.05**\n",
    ")\n",
    "\n",
    "this will decrease the amount of clutter that each data point will take up, or you can increase it for each point to take up more space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Ensure results_df is defined globally before running this cell\n",
    "if 'results_df' not in globals():\n",
    "    raise ValueError(\"Please select a column from the dropdown first to generate results_df.\")\n",
    "\n",
    "# Step 1: Clean up non-numeric rows\n",
    "results_df_cleaned = results_df[pd.to_numeric(results_df['value'], errors='coerce').notnull()].copy()\n",
    "results_df_cleaned['value'] = results_df_cleaned['value'].astype(float)\n",
    "\n",
    "# Step 2: Compute average and standard error per run\n",
    "summary_df = results_df_cleaned.groupby('run_number').agg(\n",
    "    mean_value=('value', 'mean'),\n",
    "    stderr_value=('value', lambda x: x.std(ddof=1) / np.sqrt(len(x)))\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Set up the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Bar plot with error bars\n",
    "sns.barplot(\n",
    "    x='run_number', y='mean_value', data=summary_df,\n",
    "    yerr=summary_df['stderr_value'], capsize=0.2,\n",
    "    color='skyblue', edgecolor='black'\n",
    ")\n",
    "\n",
    "# Dot plot of individual points (with jitter)\n",
    "sns.stripplot(\n",
    "    x='run_number', y='value', data=results_df_cleaned,\n",
    "    color='black', alpha=0.5, jitter=0.05\n",
    ")\n",
    "\n",
    "# Final touches\n",
    "plt.xlabel('Run Number')\n",
    "plt.ylabel(dropdown.value)  # Use the selected column name dynamically\n",
    "plt.title(f'Average {dropdown.value} per Run (with SEM and Individual Data Points)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886dda4",
   "metadata": {},
   "source": [
    "### Generates a bar plot, Box plot, Line plot, and density plot for the selected column from the dropdown menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Ensure results_df_cleaned and summary_df exist\n",
    "if 'results_df_cleaned' not in globals():\n",
    "    # Clean up non-numeric rows\n",
    "    results_df_cleaned = results_df[pd.to_numeric(results_df['value'], errors='coerce').notnull()].copy()\n",
    "    results_df_cleaned['value'] = results_df_cleaned['value'].astype(float)\n",
    "\n",
    "summary_df = results_df_cleaned.groupby('run_number').agg(\n",
    "    mean_value=('value', 'mean'),\n",
    "    stderr_value=('value', lambda x: x.std(ddof=1) / np.sqrt(len(x)))\n",
    ").reset_index()\n",
    "\n",
    "# Set a nice style\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"husl\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1ï¸âƒ£ Bar plot with error bars\n",
    "sns.barplot(\n",
    "    x='run_number', y='mean_value', data=summary_df,\n",
    "    errorbar=('ci', 0),  # disables seaborn's own CI calculation\n",
    "    ax=axes[0], color='skyblue', edgecolor='black'\n",
    ")\n",
    "\n",
    "# Add manual error bars using Matplotlib after plotting\n",
    "axes[0].errorbar(\n",
    "    x=np.arange(len(summary_df)),\n",
    "    y=summary_df['mean_value'],\n",
    "    yerr=summary_df['stderr_value'],\n",
    "    fmt='none',\n",
    "    ecolor='black',\n",
    "    capsize=4,\n",
    "    elinewidth=1.2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "axes[0].set_title(f'Bar Plot: Mean Â± SEM for {dropdown.value}')\n",
    "axes[0].set_xlabel('Run Number')\n",
    "axes[0].set_ylabel(dropdown.value)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Overlay stripplot for individual data\n",
    "sns.stripplot(\n",
    "    x='run_number', y='value', data=results_df_cleaned,\n",
    "    color='black', alpha=0.5, jitter=0.2, ax=axes[0]\n",
    ")\n",
    "\n",
    "# 2ï¸âƒ£ Line plot with markers (trend over runs)\n",
    "sns.lineplot(\n",
    "    x='run_number', y='value', data=results_df_cleaned,\n",
    "    estimator='mean', ci='sd', marker='o', linewidth=2, ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(f'Line Plot: Mean Â± SD for {dropdown.value}')\n",
    "axes[1].set_xlabel('Run Number')\n",
    "axes[1].set_ylabel(dropdown.value)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3ï¸âƒ£ Box plot for distribution per run\n",
    "sns.boxplot(\n",
    "    x='run_number', y='value', data=results_df_cleaned,\n",
    "    ax=axes[2], palette=\"Set2\"\n",
    ")\n",
    "axes[2].set_title(f'Box Plot: Distribution per Run')\n",
    "axes[2].set_xlabel('Run Number')\n",
    "axes[2].set_ylabel(dropdown.value)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4ï¸âƒ£ KDE (density) plot â€” all runs overlaid\n",
    "for run_num, subset in results_df_cleaned.groupby('run_number'):\n",
    "    sns.kdeplot(subset['value'], label=f'Run {run_num}', fill=True, alpha=0.3, ax=axes[3])\n",
    "axes[3].set_title(f'Density Plot: Distribution of {dropdown.value}')\n",
    "axes[3].set_xlabel(dropdown.value)\n",
    "axes[3].legend(title=\"Run Number\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748b08f",
   "metadata": {},
   "source": [
    "### Load MetaData (Reference Sheet) for Comparative Analysis on various conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532a392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "\n",
    "# Step 1: Load and filter the metadata from tracking sheet\n",
    "def load_and_filter_metadata(metadata_file_path):\n",
    "    \"\"\"Load metadata and filter for Axon Tracking assays only\"\"\"\n",
    "    metadata = pd.read_csv(metadata_file_path)  # Regular CSV format\n",
    "    # Filter columns 0 to 6 (titles \"Date\" to \"Neuron Source\")\n",
    "    metadata = metadata.iloc[:, 0:7]\n",
    "    # Filter for Axon Tracking assays only\n",
    "    axon_tracking_data = metadata[metadata['Assay'] == 'Axon Tracking 1hr'].copy()\n",
    "    axon_tracking_data.head()\n",
    "\n",
    "    # Create a mapping of run number to density/media info\n",
    "    run_condition_map = {} \n",
    "    for _, row in axon_tracking_data.iterrows():\n",
    "        wells = str(row['Wells_Recorded']).split(',')\n",
    "        sources = str(row['Neuron Source']).split(',')\n",
    "        \n",
    "        # Ensure both lists are of the same length\n",
    "        if len(wells) != len(sources):\n",
    "            print(f\"Warning: Mismatch in lengths for Run {row['Run #']}. Wells: {len(wells)}, Sources: {len(sources)}\")\n",
    "            continue\n",
    "        \n",
    "        # Map each well to its corresponding source\n",
    "        well_source_map = {well.strip(): source.strip() for well, source in zip(wells, sources)}\n",
    "        \n",
    "        # Add to the run_condition_map\n",
    "        run_condition_map[row['Run #']] = well_source_map\n",
    "    \n",
    "    # Convert the mapping to a DataFrame for better visualization\n",
    "    run_condition_df = pd.DataFrame([\n",
    "        {'Run #': run, 'Well': well, 'Neuron Source': source}\n",
    "        for run, well_sources in run_condition_map.items()\n",
    "        for well, source in well_sources.items()\n",
    "    ])\n",
    "    \n",
    "    print(run_condition_df)\n",
    "    return run_condition_df\n",
    "\n",
    "# Load metadata\n",
    "metadata_file_path = \"/mnt/disk20tb/shruti/MaxTwo MEA Primary Neurons Assays(FolicAcid_T4_02252025_SA).csv\"\n",
    "run_condition_df = load_and_filter_metadata(metadata_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0df282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# import re\n",
    "\n",
    "\n",
    "# SUPPOSED TO PLOT FROM REFERENCE SHEET \n",
    "# # Step 4: Plotting functions\n",
    "# def plot_density_comparison(df, metric_name, plot_type='both'):\n",
    "#     \"\"\"Create plots comparing densities for a given metric\"\"\"\n",
    "#     # Clean up non-numeric values\n",
    "#     df_clean = df[pd.to_numeric(df['value'], errors='coerce').notnull()].copy()\n",
    "#     df_clean['value'] = df_clean['value'].astype(float)\n",
    "    \n",
    "#     if df_clean.empty:\n",
    "#         print(f\"No valid data found for {metric_name}\")\n",
    "#         return\n",
    "    \n",
    "#     # Create density labels and order them from least to greatest\n",
    "#     df_clean['density_label'] = df_clean['density'].astype(str) + 'K ' + df_clean['media']\n",
    "    \n",
    "#     # Create ordered categories based on density (numerical) then media (alphabetical)\n",
    "#     density_order = df_clean.groupby('density_label')['density'].first().sort_values().index.tolist()\n",
    "#     df_clean['density_label'] = pd.Categorical(df_clean['density_label'], categories=density_order, ordered=True)\n",
    "    \n",
    "#     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "#     if plot_type in ['both', 'violin']:\n",
    "#         plt.subplot(2, 2, 1)\n",
    "#         sns.violinplot(data=df_clean, x='density_label', y='value', order=density_order)\n",
    "#         plt.title(f'{metric_name} Distribution by Density')\n",
    "#         plt.xticks(rotation=45)\n",
    "    \n",
    "#     if plot_type in ['both', 'box']:\n",
    "#         plt.subplot(2, 2, 2)\n",
    "#         sns.boxplot(data=df_clean, x='density_label', y='value', order=density_order)\n",
    "#         plt.title(f'{metric_name} Box Plot by Density')\n",
    "#         plt.xticks(rotation=45)\n",
    "    \n",
    "#     if plot_type in ['both', 'bar']:\n",
    "#         plt.subplot(2, 2, 3)\n",
    "#         summary_df = df_clean.groupby('density_label').agg(\n",
    "#             mean_value=('value', 'mean'),\n",
    "#             stderr_value=('value', lambda x: x.std(ddof=1) / np.sqrt(len(x)) if len(x) > 1 else 0)\n",
    "#         ).reset_index()\n",
    "        \n",
    "#         # Reorder summary_df according to density_order\n",
    "#         summary_df['density_label'] = pd.Categorical(summary_df['density_label'], categories=density_order, ordered=True)\n",
    "#         summary_df = summary_df.sort_values('density_label').reset_index(drop=True)\n",
    "        \n",
    "#         # Create bar plot\n",
    "#         bars = plt.bar(range(len(summary_df)), summary_df['mean_value'], \n",
    "#                       yerr=summary_df['stderr_value'], capsize=5,\n",
    "#                       color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "#         plt.xlabel('Density')\n",
    "#         plt.ylabel(f'Mean {metric_name}')\n",
    "#         plt.title(f'Mean {metric_name} by Density (Â±SEM)')\n",
    "#         plt.xticks(range(len(summary_df)), summary_df['density_label'], rotation=45)\n",
    "    \n",
    "#     if plot_type in ['both', 'strip']:\n",
    "#         plt.subplot(2, 2, 4)\n",
    "#         sns.stripplot(data=df_clean, x='density_label', y='value', \n",
    "#                      alpha=0.7, jitter=0.3, size=4, order=density_order)\n",
    "#         plt.title(f'Individual {metric_name} Values by Density')\n",
    "#         plt.xticks(rotation=45)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print summary statistics (ordered)\n",
    "#     print(f\"\\nSummary Statistics for {metric_name}:\")\n",
    "#     print(\"-\" * 50)\n",
    "#     summary_stats = df_clean.groupby('density_label')['value'].agg([\n",
    "#         'count', 'mean', 'std', 'min', 'max'\n",
    "#     ]).round(3)\n",
    "#     # Reorder by density\n",
    "#     summary_stats = summary_stats.reindex(density_order)\n",
    "#     print(summary_stats)\n",
    "\n",
    "# # Step 5: Statistical analysis\n",
    "# def statistical_analysis(df, metric_name):\n",
    "#     \"\"\"Perform statistical analysis comparing densities\"\"\"\n",
    "#     try:\n",
    "#         from scipy import stats\n",
    "#     except ImportError:\n",
    "#         print(\"scipy not available. Skipping statistical analysis.\")\n",
    "#         return\n",
    "    \n",
    "#     df_clean = df[pd.to_numeric(df['value'], errors='coerce').notnull()].copy()\n",
    "#     df_clean['value'] = df_clean['value'].astype(float)\n",
    "#     df_clean['density_label'] = df_clean['density'].astype(str) + 'K ' + df_clean['media']\n",
    "    \n",
    "#     # Order density labels from least to greatest\n",
    "#     density_order = df_clean.groupby('density_label')['density'].first().sort_values().index.tolist()\n",
    "#     df_clean['density_label'] = pd.Categorical(df_clean['density_label'], categories=density_order, ordered=True)\n",
    "    \n",
    "#     # Group data by density (in order)\n",
    "#     groups = []\n",
    "#     group_names = []\n",
    "#     for density in density_order:\n",
    "#         group_data = df_clean[df_clean['density_label'] == density]['value'].values\n",
    "#         if len(group_data) > 0:\n",
    "#             groups.append(group_data)\n",
    "#             group_names.append(density)\n",
    "    \n",
    "#     if len(groups) > 2:\n",
    "#         # ANOVA for multiple groups\n",
    "#         f_stat, p_value = stats.f_oneway(*groups)\n",
    "#         print(f\"\\nANOVA Results for {metric_name}:\")\n",
    "#         print(f\"F-statistic: {f_stat:.4f}\")\n",
    "#         print(f\"p-value: {p_value:.4f}\")\n",
    "        \n",
    "#         if p_value < 0.05:\n",
    "#             print(\"Significant difference found between groups!\")\n",
    "            \n",
    "#             # Post-hoc pairwise comparisons\n",
    "#             print(\"\\nPairwise t-tests (Bonferroni corrected):\")\n",
    "#             n_comparisons = len(groups) * (len(groups) - 1) // 2\n",
    "#             alpha_corrected = 0.05 / n_comparisons\n",
    "            \n",
    "#             for i in range(len(groups)):\n",
    "#                 for j in range(i+1, len(groups)):\n",
    "#                     t_stat, p_val = stats.ttest_ind(groups[i], groups[j])\n",
    "#                     significant = \"***\" if p_val < alpha_corrected else \"\"\n",
    "#                     print(f\"{group_names[i]} vs {group_names[j]}: p = {p_val:.4f} {significant}\")\n",
    "#         else:\n",
    "#             print(\"No significant differences found between groups.\")\n",
    "    \n",
    "#     elif len(groups) == 2:\n",
    "#         # t-test for two groups\n",
    "#         t_stat, p_value = stats.ttest_ind(groups[0], groups[1])\n",
    "#         print(f\"\\nt-test Results for {metric_name}:\")\n",
    "#         print(f\"t-statistic: {t_stat:.4f}\")\n",
    "#         print(f\"p-value: {p_value:.4f}\")\n",
    "#         print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "#     else:\n",
    "#         print(f\"Not enough groups for statistical comparison (found {len(groups)} groups)\")\n",
    "\n",
    "# # Function to explore available metrics\n",
    "# def explore_available_metrics(data_dict):\n",
    "#     \"\"\"Find all available column names across all files\"\"\"\n",
    "#     all_columns = set()\n",
    "#     for run_num, file_dict in data_dict.items():\n",
    "#         for file_name, df in file_dict.items():\n",
    "#             all_columns.update(df.columns)\n",
    "    \n",
    "#     print(\"Available metrics to analyze:\")\n",
    "#     for col in sorted(all_columns):\n",
    "#         print(f\"  - {col}\")\n",
    "    \n",
    "#     return sorted(all_columns)\n",
    "\n",
    "# # Main execution\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Replace these paths with your actual file path\n",
    "#     metadata_file_path = \"/mnt/disk20tb/shruti/MaxTwo MEA Primary Neurons Assays(FolicAcid_T4_02252025_SA).csv\"  # The tab-separated file you pasted\n",
    "#     detailed_data_path = root_path\n",
    "\n",
    "#     # Load data\n",
    "#     print(\"Loading metadata...\")\n",
    "#     try:\n",
    "#         run_density_map = load_and_filter_metadata(metadata_file_path)\n",
    "#         print(f\"Found {len(run_density_map)} axon tracking runs\")\n",
    "        \n",
    "#         # Print the runs found for debugging\n",
    "#         print(\"Axon tracking runs found:\")\n",
    "#         for run_num, info in run_density_map.items():\n",
    "#             print(f\"  Run {run_num}: {len(info['wells'])} wells\")\n",
    "#             for well, well_info in info['wells'].items():\n",
    "#                 print(f\"    Well {well}: {well_info['density']}K {well_info['media']}\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading metadata: {e}\")\n",
    "#         run_density_map = {}\n",
    "\n",
    "#     print(\"\\nLoading detailed data...\")\n",
    "#     try:\n",
    "#         data_dict = load_detailed_data(detailed_data_path)\n",
    "#         print(f\"Loaded data for {len(data_dict)} runs\")\n",
    "        \n",
    "#         # Print which runs have data\n",
    "#         print(\"Runs with detailed data:\", sorted(data_dict.keys()))\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error loading detailed data: {e}\")\n",
    "#         data_dict = {}\n",
    "\n",
    "#     # Check for overlap between metadata and detailed data\n",
    "#     if run_density_map and data_dict:\n",
    "#         metadata_runs = set(run_density_map.keys())\n",
    "#         detailed_runs = set(data_dict.keys())\n",
    "#         overlap = metadata_runs.intersection(detailed_runs)\n",
    "#         print(f\"\\nRuns with both metadata and detailed data: {sorted(overlap)}\")\n",
    "        \n",
    "#         if not overlap:\n",
    "#             print(\"WARNING: No overlap between metadata runs and detailed data runs!\")\n",
    "#             print(f\"Metadata runs: {sorted(metadata_runs)}\")\n",
    "#             print(f\"Detailed data runs: {sorted(detailed_runs)}\")\n",
    "\n",
    "#     # Uncomment to see all available metrics:\n",
    "#     if data_dict:\n",
    "#         print(\"\\n\" + \"=\"*60)\n",
    "#         available_metrics = explore_available_metrics(data_dict)\n",
    "#         print(\"=\"*60)\n",
    "\n",
    "#     # Analyze different metrics\n",
    "#     metrics_to_analyze = ['branchLen', 'neuronFiringRate', 'branchConductionVel', 'totNoSpikes']\n",
    "\n",
    "#     for metric in metrics_to_analyze:\n",
    "#         print(f\"\\n{'='*60}\")\n",
    "#         print(f\"Analyzing {metric}\")\n",
    "#         print('='*60)\n",
    "        \n",
    "#         # Get data with density information\n",
    "#         results_df = search_column_with_density(data_dict, run_density_map, metric)\n",
    "        \n",
    "#         if not results_df.empty:\n",
    "#             print(f\"Found {len(results_df)} data points for {metric}\")\n",
    "            \n",
    "#             # Print unique density combinations found\n",
    "#             unique_densities = results_df.groupby(['density', 'media']).size().reset_index(name='count')\n",
    "#             print(\"Density combinations found:\")\n",
    "#             for _, row in unique_densities.iterrows():\n",
    "#                 print(f\"  {row['density']}K {row['media']}: {row['count']} data points\")\n",
    "            \n",
    "#             # Create plots\n",
    "#             plot_density_comparison(results_df, metric)\n",
    "            \n",
    "#             # Statistical analysis\n",
    "#             statistical_analysis(results_df, metric)\n",
    "#         else:\n",
    "#             print(f\"No data found for {metric}\")\n",
    "            \n",
    "#             # Debug: check if column exists anywhere\n",
    "#             found_in_files = []\n",
    "#             for run_num, file_dict in data_dict.items():\n",
    "#                 for file_name, df in file_dict.items():\n",
    "#                     if metric in df.columns:\n",
    "#                         found_in_files.append(f\"Run {run_num}, file {file_name}\")\n",
    "            \n",
    "#             if found_in_files:\n",
    "#                 print(f\"  But {metric} was found in: {found_in_files}\")\n",
    "#             else:\n",
    "#                 print(f\"  {metric} not found in any files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388e93c",
   "metadata": {},
   "source": [
    "## AXONAL AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf77724",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''First Cell: Single Axonal Area Calculation from PNG Image\n",
    "    Input: PNG image file path\n",
    "    Output: Axon-covered area in ÂµmÂ²'''\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your PNG file\n",
    "image_path = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/250717/M07137/AxonTracking/000148/analysis/FootprintExtraction_v1/0001/Well4/FootprintNeuron#16.png\"  # Replace with the actual path\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Convert image to numpy array\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Convert to grayscale using luminance formula\n",
    "gray_img = np.dot(img_np[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# Apply a grayscale threshold to isolate the signal (tweak this threshold as needed)\n",
    "threshold = 30  # Adjust if needed\n",
    "axon_mask = gray_img > threshold\n",
    "\n",
    "# Estimate pixel size from scale bar: 100 Âµm â‰ˆ 18 pixels â†’ 1 pixel â‰ˆ 5.56 Âµm\n",
    "pixel_size_um = 100 / 18\n",
    "pixel_area_um2 = pixel_size_um ** 2\n",
    "\n",
    "# Calculate axonal area\n",
    "axon_area_um2 = np.sum(axon_mask) * pixel_area_um2\n",
    "\n",
    "# Output results\n",
    "print(f\"Axon-covered area: {axon_area_um2:.2f} ÂµmÂ²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9fd6e",
   "metadata": {},
   "source": [
    "BASE_DIR = FILE PATH TO RAW_DATA FOLDER   \n",
    "OUTPUT_DIR = ENTER FILE PATH TO FOLDER YOU WANT YOUR RESULTS TO BE IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# CONFIGURATION\n",
    "BASE_DIR = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/FolicAcid_T4_02252025_SA/FolicAcid_T4_02252025_SA/\"\n",
    "OUTPUT_DIR = \"/mnt/disk20tb/shruti/AxonTracking?_NeedsOrganizing/\"\n",
    "OUTPUT_CSV = \"axon_area_results.csv\"\n",
    "PIXELS_PER_100_UM = 18  # from scale bar\n",
    "PIXEL_AREA_UM2 = (100 / PIXELS_PER_100_UM) ** 2  # ÂµmÂ² per pixel\n",
    "THRESHOLD = 30  # grayscale or red channel threshold (adjust as needed)\n",
    "\n",
    "def is_target_image(filename):\n",
    "    return re.match(r\"FootprintNeuron#\\d+\\.png\", filename)\n",
    "\n",
    "def extract_metadata(filepath):\n",
    "    \"\"\"\n",
    "    Extract Plate ID, AxonTracking ID, Well number, Neuron number from the path\n",
    "    \"\"\"\n",
    "    parts = filepath.split(os.sep)\n",
    "    try:\n",
    "        plate_id = next(p for p in parts if p.startswith(\"M\"))\n",
    "        tracking_id = parts[parts.index(\"AxonTracking\") + 1]\n",
    "        well_match = re.search(r\"Well(\\d+)\", filepath)\n",
    "        neuron_match = re.search(r\"FootprintNeuron#(\\d+)\", filepath)\n",
    "        return {\n",
    "            \"plate_id\": plate_id,\n",
    "            \"tracking_id\": tracking_id,\n",
    "            \"well\": f\"Well{well_match.group(1)}\" if well_match else \"Unknown\",\n",
    "            \"neuron\": f\"Neuron#{neuron_match.group(1)}\" if neuron_match else \"Unknown\"\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"plate_id\": \"Unknown\", \"tracking_id\": \"Unknown\", \"well\": \"Unknown\", \"neuron\": \"Unknown\"}\n",
    "\n",
    "def analyze_image(image_path, threshold=THRESHOLD):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Use red channel as signal indicator (red = high ÂµV in colormap)\n",
    "    red_channel = img_np[:, :, 0]\n",
    "\n",
    "    # Apply threshold\n",
    "    signal_mask = red_channel > threshold\n",
    "    pixel_count = np.sum(signal_mask)\n",
    "    area_um2 = pixel_count * PIXEL_AREA_UM2\n",
    "\n",
    "    return area_um2, pixel_count\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(BASE_DIR):\n",
    "        for file in files:\n",
    "            if is_target_image(file):\n",
    "                full_path = os.path.join(root, file)\n",
    "                metadata = extract_metadata(full_path)\n",
    "                area_um2, pixel_count = analyze_image(full_path)\n",
    "\n",
    "                results.append({\n",
    "                    \"Plate ID\": metadata[\"plate_id\"],\n",
    "                    \"Tracking ID\": metadata[\"tracking_id\"],\n",
    "                    \"Well\": metadata[\"well\"],\n",
    "                    \"Neuron\": metadata[\"neuron\"],\n",
    "                    \"Pixel Count\": pixel_count,\n",
    "                    \"Axon Area (ÂµmÂ²)\": round(area_um2, 2),\n",
    "                    \"Threshold Used\": THRESHOLD\n",
    "                })\n",
    "                #print(results)\n",
    "\n",
    "    # Output nicely\n",
    "    print(\"\\nðŸ“Š Axon Area Analysis Results:\\n\")\n",
    "    #for r in results:\n",
    "        #print(f\"{r['Plate ID']} | {r['Tracking ID']} | {r['Well']} | {r['Neuron']}: {r['Axon Area (ÂµmÂ²)']} ÂµmÂ² (Pixels: {r['Pixel Count']})\")\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(OUTPUT_DIR + \"/\" + OUTPUT_CSV, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"\\nâœ… Results saved to {OUTPUT_DIR + OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497aa7d3",
   "metadata": {},
   "source": [
    "#### Plotting for Axonal Area Per Run, Per Neuron, Per Well - Color coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TEST ON ONE RUN'''\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"axon_area_results.csv\")  # Update path if needed\n",
    "\n",
    "# Optional: sort for better viultssuals\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r'#(\\d+)').astype(int)\n",
    "df = df.sort_values([\"Well\", \"Neuron ID\"])\n",
    "\n",
    "# Plot 1: Bar Plot per Neuron (grouped by Well)\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x=\"Neuron\", y=\"Axon Area (ÂµmÂ²)\", hue=\"Well\", data=df, ci = None)\n",
    "plt.title(\"Axon Area per Neuron Grouped by Well\")\n",
    "plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Box Plot per Well\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Well\", y=\"Axon Area (ÂµmÂ²)\", data=df)\n",
    "plt.title(\"Distribution of Axon Area per Well\")\n",
    "plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"axon_area_results.csv\")\n",
    "\n",
    "# Extract Neuron ID number for sorting\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r'#(\\d+)').astype(int)\n",
    "\n",
    "# Sort for consistency\n",
    "df = df.sort_values([\"Tracking ID\", \"Well\", \"Neuron ID\"])\n",
    "\n",
    "# Group by run (Tracking ID)\n",
    "for tracking_id, run_df in df.groupby(\"Tracking ID\"):\n",
    "    print(f\"\\nðŸ“Š Plotting Tracking ID: {tracking_id}\")\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x = \"Neuron\",\n",
    "        y=\"Axon Area (ÂµmÂ²)\",\n",
    "        hue=\"Well\",\n",
    "        data=run_df,\n",
    "        ci=None,\n",
    "        dodge=True, \n",
    "        alpha=0.7,\n",
    "        edgecolor=\"gray\",\n",
    "        linewidth=0.5\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Axon Area per Neuron â€” Run: {tracking_id}\")\n",
    "    plt.xlabel(\"Neuron ID\")\n",
    "    plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Well\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.stripplot(\n",
    "        x=\"Neuron\",\n",
    "        y=\"Axon Area (ÂµmÂ²)\",\n",
    "        hue=\"Well\",\n",
    "        data=run_df,\n",
    "        dodge=True,\n",
    "        jitter=True,\n",
    "        alpha=0.7,\n",
    "        size=8,\n",
    "        edgecolor=\"gray\",\n",
    "        linewidth=0.5 \n",
    "\n",
    "    )\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # # Violinplot\n",
    "    # sns.violinplot(data=run_df, x='Neuron ID', y='Axon Area (ÂµmÂ²)', palette='viridis', legend = True)\n",
    "    # plt.title('Violinplot')\n",
    "    # plt.show()\n",
    "\n",
    "   \n",
    "    plt.title(f\"Axon Area per Neuron â€” Run: {tracking_id}\")\n",
    "    plt.xlabel(\"Neuron ID\")\n",
    "    plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Well\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the PNG\n",
    "image_path = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/250714/M07137/AxonTracking/000103/analysis/FootprintExtraction_v1/0001/Well3/FootprintNeuron#14.png\"  # Replace with the actual path  # Change to your path\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray_img = np.dot(img_np[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# Threshold to detect signal (axon area)\n",
    "threshold = 30\n",
    "axon_mask = gray_img > threshold\n",
    "\n",
    "# Estimate pixel size and area\n",
    "pixels_per_100um = 18\n",
    "pixel_area_um2 = (100 / pixels_per_100um) ** 2\n",
    "axon_area_um2 = np.sum(axon_mask) * pixel_area_um2\n",
    "\n",
    "print(f\"Estimated axon-covered area: {axon_area_um2:.2f} ÂµmÂ²\")\n",
    "\n",
    "# ðŸ”½ Save visualization of axon detection\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(gray_img, cmap='gray')  # grayscale background\n",
    "plt.imshow(axon_mask, cmap='Reds', alpha=0.5)  # red overlay for detected axon\n",
    "plt.axis('off')\n",
    "plt.title(\"Detected Axon Region\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR + \"axon_detection_overlay.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… Saved axon detection overlay as 'axon_detection_overlay.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_THRESHOLD_UM2 = 800_000  # Any footprint area above this = \"Detected\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing analysis\n",
    "df = pd.read_csv(\"axon_area_results.csv\")\n",
    "\n",
    "# Set your detection threshold\n",
    "DETECTION_THRESHOLD_UM2 = 800_000\n",
    "\n",
    "# Add detection status\n",
    "df[\"Detected\"] = df[\"Axon Area (ÂµmÂ²)\"] >= DETECTION_THRESHOLD_UM2\n",
    "\n",
    "# Organize data\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r\"#(\\d+)\").astype(int)\n",
    "df = df.sort_values([\"Well\", \"Neuron ID\"])\n",
    "\n",
    "# âœ… Save full detection DataFrame\n",
    "df.to_csv(\"axon_detection_classified.csv\", index=False)\n",
    "\n",
    "# ðŸ“Š Detection summary per Well\n",
    "summary = df.groupby(\"Well\")[\"Detected\"].sum().reset_index()\n",
    "summary.columns = [\"Well\", \"Neurons Detected\"]\n",
    "summary[\"Total Neurons\"] = df.groupby(\"Well\")[\"Neuron\"].count().values\n",
    "summary[\"Detection Rate (%)\"] = (summary[\"Neurons Detected\"] / summary[\"Total Neurons\"] * 100).round(1)\n",
    "\n",
    "# âœ… Save summary table\n",
    "summary.to_csv(\"axon_detection_summary.csv\", index=False)\n",
    "\n",
    "# Print summary nicely\n",
    "print(\"\\nðŸ“Œ Detection Summary per Well:\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab369b",
   "metadata": {},
   "source": [
    "### If you only want to include some run numbers then you can input those run numbers under RUNS_TO_INCLUDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# CONFIGURATION\n",
    "#BASE_DIR = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "#OUTPUT_DIR = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/\"\n",
    "PIXELS_PER_100UM = 18\n",
    "PIXEL_AREA_UM2 = (100 / PIXELS_PER_100UM) ** 2\n",
    "THRESHOLD = 30\n",
    "DETECTION_THRESHOLD_UM2 = 800_000\n",
    "RUNS_TO_INCLUDE = {'000044', '000089', '000103', '000148', '000173', '000225'}\n",
    "\n",
    "def is_footprint_image(filename):\n",
    "    return re.match(r\"FootprintNeuron#\\d+\\.png\", filename)\n",
    "\n",
    "def extract_metadata(filepath):\n",
    "    parts = filepath.split(os.sep)\n",
    "    try:\n",
    "        plate_id = next(p for p in parts if p.startswith(\"M\"))\n",
    "        run_id = parts[parts.index(\"AxonTracking\") + 1]\n",
    "        well = next(p for p in parts if re.match(r\"Well\\d+\", p))\n",
    "        neuron = re.search(r\"FootprintNeuron#(\\d+)\", filepath).group(1)\n",
    "        return plate_id, run_id, well, neuron\n",
    "    except Exception:\n",
    "        return \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\"\n",
    "\n",
    "def analyze_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_np = np.array(img)\n",
    "    red_channel = img_np[:, :, 0]\n",
    "    mask = red_channel > THRESHOLD\n",
    "    area_um2 = np.sum(mask) * PIXEL_AREA_UM2\n",
    "    return area_um2\n",
    "\n",
    "def main():\n",
    "    detected_rows = []\n",
    "    summary_dict = {}\n",
    "\n",
    "    for root, _, files in os.walk(BASE_DIR):\n",
    "        for file in files:\n",
    "            if is_footprint_image(file):\n",
    "                full_path = os.path.join(root, file)\n",
    "                plate_id, run_id, well, neuron = extract_metadata(full_path)\n",
    "\n",
    "                if run_id not in RUNS_TO_INCLUDE:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    area = analyze_image(full_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error processing {full_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                detected = area >= DETECTION_THRESHOLD_UM2\n",
    "\n",
    "                if detected:\n",
    "                    detected_rows.append({\n",
    "                        \"Run #\": run_id,\n",
    "                        \"Well #\": well,\n",
    "                        \"FootprintNeuron #\": neuron,\n",
    "                        \"Detected\": \"Yes\",\n",
    "                        \"Detection Area (ÂµmÂ²)\": round(area, 2)\n",
    "                    })\n",
    "\n",
    "                # Track total & detected counts per run+well\n",
    "                key = (run_id, well)\n",
    "                if key not in summary_dict:\n",
    "                    summary_dict[key] = {\"total\": 0, \"detected\": 0}\n",
    "                summary_dict[key][\"total\"] += 1\n",
    "                if detected:\n",
    "                    summary_dict[key][\"detected\"] += 1\n",
    "\n",
    "    # Output DataFrame 1: All detected neurons\n",
    "    df_detected = pd.DataFrame(detected_rows)\n",
    "    detected_csv = os.path.join(OUTPUT_DIR, \"axon_detected_footprints.csv\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    df_detected.to_csv(detected_csv, index=False)\n",
    "\n",
    "    # Output DataFrame 2: Summary per well\n",
    "    summary_data = []\n",
    "    for (run, well), stats in summary_dict.items():\n",
    "        detection_rate = (stats[\"detected\"] / stats[\"total\"]) * 100\n",
    "        summary_data.append({\n",
    "            \"Run #\": run,\n",
    "            \"Well #\": well,\n",
    "            \"Neurons Detected\": stats[\"detected\"],\n",
    "            \"Total Neurons\": stats[\"total\"],\n",
    "            \"Detection Rate (%)\": round(detection_rate, 1)\n",
    "        })\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_csv = os.path.join(OUTPUT_DIR, \"axon_detection_summary_per_well.csv\")\n",
    "    df_summary.to_csv(summary_csv, index=False)\n",
    "\n",
    "    print(\"âœ… CSVs generated:\")\n",
    "    print(f\" - {detected_csv}\")\n",
    "    print(f\" - {summary_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819cbe9d",
   "metadata": {},
   "source": [
    "### PLOTTING OF DETECTED NEURONS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detection_summary_per_well.csv\"  # Update with the correct path\n",
    "df_detection_summary = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(df_detection_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'Well #' and iterate through each well\n",
    "wells = df_detection_summary[\"Well #\"].unique()\n",
    "\n",
    "for well in wells:\n",
    "    # Filter data for the current well\n",
    "    df_well = df_detection_summary[df_detection_summary[\"Well #\"] == well]\n",
    "    \n",
    "    # Plot detected neuron count across runs\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(df_well[\"Run #\"], df_well[\"Neurons Detected\"], marker='o', label=f\"Well {well}\")\n",
    "    plt.title(f\"Detected Neurons Across Runs for {well}\", fontsize=14)\n",
    "    plt.xlabel(\"Run #\", fontsize=12)\n",
    "    plt.ylabel(\"Detected Neuron #\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pivot the data to prepare for grouped bar plot\n",
    "df_pivot = df_detection_summary.pivot(index=\"Run #\", columns=\"Well #\", values=\"Neurons Detected\")\n",
    "\n",
    "# Plot grouped bar chart\n",
    "df_pivot.plot(kind=\"bar\", figsize=(12, 8), width=0.8)\n",
    "plt.title(\"Detected Neurons Across Runs for All Wells\", fontsize=14)\n",
    "plt.xlabel(\"Run #\", fontsize=12)\n",
    "plt.ylabel(\"Detected Neuron #\", fontsize=12)\n",
    "plt.legend(title=\"Well #\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pivot the data to prepare for grouped bar plot\n",
    "df_pivot = df_detection_summary.pivot(index=\"Run #\", columns=\"Well #\", values=\"Neurons Detected\")\n",
    "\n",
    "# Manual well label mapping\n",
    "well_labels = {\n",
    "    2: \"Well 2: 80K NBP\",\n",
    "    4: \"Well 4: 120K NBP\", \n",
    "    5: \"Well 5: 100K NBP\"\n",
    "}\n",
    "\n",
    "# Create enhanced column labels\n",
    "enhanced_columns = []\n",
    "for col in df_pivot.columns:\n",
    "    # Extract number from column name like \"Well1\", \"Well2\", etc.\n",
    "    if isinstance(col, str) and col.startswith(\"Well\"):\n",
    "        well_num = int(col.replace(\"Well\", \"\"))\n",
    "    else:\n",
    "        well_num = int(col) if not isinstance(col, str) else col\n",
    "    \n",
    "    if well_num in well_labels:\n",
    "        enhanced_columns.append(well_labels[well_num])\n",
    "    else:\n",
    "        enhanced_columns.append(col)  # Keep original column name\n",
    "\n",
    "# Update the column names\n",
    "df_pivot.columns = enhanced_columns\n",
    "\n",
    "# Plot grouped bar chart (same as your original)\n",
    "df_pivot.plot(kind=\"bar\", figsize=(12, 8), width=0.8)\n",
    "plt.title(\"Detected Neurons Across Runs for All Wells\", fontsize=14)\n",
    "plt.xlabel(\"Run #\", fontsize=12)\n",
    "plt.ylabel(\"Detected Neuron #\", fontsize=12)\n",
    "plt.legend(title=\"Well #\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff27a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# === Load tracking sheet (metadata) ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load axon footprint data ===\n",
    "df = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "\n",
    "# Clean and prepare data\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str)\n",
    "df[\"Well #\"] = df[\"Well #\"].astype(str)\n",
    "df[\"Well_Num\"] = df[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Filter only detected neurons (Detection Area > 800000)\n",
    "df_detected = df[df[\"Detection Area (ÂµmÂ²)\"] > 800000].copy()\n",
    "\n",
    "# === Add Density column using mapping ===\n",
    "def get_density(run_num, well_num):\n",
    "    mapping = run_well_density_map.get(run_num, {})\n",
    "    return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "df_detected[\"Density\"] = df_detected.apply(\n",
    "    lambda row: get_density(row[\"Run #\"], row[\"Well_Num\"]), axis=1\n",
    ")\n",
    "\n",
    "# Remove rows with Unknown density\n",
    "df_detected = df_detected[df_detected[\"Density\"] != \"Unknown\"]\n",
    "\n",
    "# Sort densities numerically for consistent ordering\n",
    "def extract_density_value(density_str):\n",
    "    match = re.search(r\"(\\d+)\", density_str)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "df_detected[\"Density_Value\"] = df_detected[\"Density\"].apply(extract_density_value)\n",
    "df_detected = df_detected.sort_values(\"Density_Value\")\n",
    "\n",
    "# === Create plots for each run ===\n",
    "for run_num, run_data in df_detected.groupby(\"Run #\"):\n",
    "    print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "    \n",
    "    # Calculate mean axonal area per density\n",
    "    mean_data = run_data.groupby(\"Density\")[\"Detection Area (ÂµmÂ²)\"].agg(['mean', 'count']).reset_index()\n",
    "    mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create bar plot for means\n",
    "    bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                   alpha=0.7, color='lightblue', edgecolor='navy', linewidth=2,\n",
    "                   label='Mean Axonal Area')\n",
    "    \n",
    "    # Add individual neuron markers\n",
    "    for density in mean_data[\"Density\"]:\n",
    "        density_data = run_data[run_data[\"Density\"] == density]\n",
    "        x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "        \n",
    "        # Add jitter to x-position for better visibility\n",
    "        jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "        plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                   density_data[\"Detection Area (ÂµmÂ²)\"],\n",
    "                   color='red', alpha=0.6, s=40, zorder=5,\n",
    "                   label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "    \n",
    "    # Add mean values as text on bars\n",
    "    for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "        plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.0f}\\n(n={count})', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f\"Axonal Area by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Density\", fontsize=14)\n",
    "    plt.ylabel(\"Axonal Area (ÂµmÂ²)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Format y-axis to show values in a readable format\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1000:.0f}K'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Summary for Run {run_num}:\")\n",
    "    for _, row in mean_data.iterrows():\n",
    "        print(f\"  {row['Density']}: Mean = {row['mean']:.0f} ÂµmÂ², Count = {row['count']} neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed95932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load metadata and create density mapping ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load detected neurons data ===\n",
    "detected_footprints = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "detected_footprints[\"Run #\"] = detected_footprints[\"Run #\"].astype(str)\n",
    "detected_footprints[\"Well_Num\"] = detected_footprints[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "detected_footprints = detected_footprints[detected_footprints[\"Detection Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Create set of detected neurons for filtering\n",
    "detected_neurons = set()\n",
    "for _, row in detected_footprints.iterrows():\n",
    "    detected_neurons.add((str(row[\"Run #\"]), row[\"Well_Num\"], row[\"FootprintNeuron #\"]))\n",
    "\n",
    "# === Load branch length data using your directory structure ===\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "def search_branch_data_across_runs(data_dict):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        \n",
    "        # Look for branch length data in branch_metrics or similar files\n",
    "        for file_name, df in file_dict.items():\n",
    "            if 'branch' in file_name.lower() and 'branchLen' in df.columns:\n",
    "                # Assuming there are columns for neuron ID and well\n",
    "                required_cols = ['branchLen']\n",
    "                \n",
    "                # Try to identify neuron and well columns (adjust based on your actual column names)\n",
    "                neuron_col = None\n",
    "                well_col = None\n",
    "                \n",
    "                for col in df.columns:\n",
    "                    if any(x in col.lower() for x in ['neuron', 'cell', 'id']):\n",
    "                        neuron_col = col\n",
    "                    if any(x in col.lower() for x in ['well']):\n",
    "                        well_col = col\n",
    "                \n",
    "                if neuron_col and well_col:\n",
    "                    for _, row in df.iterrows():\n",
    "                        if pd.notna(row['branchLen']):\n",
    "                            well_num = int(re.search(r'\\d+', str(row[well_col])).group()) if re.search(r'\\d+', str(row[well_col])) else None\n",
    "                            neuron_id = row[neuron_col]\n",
    "                            \n",
    "                            if well_num and (str(numeric_run_num), well_num, neuron_id) in detected_neurons:\n",
    "                                results.append({\n",
    "                                    \"run_number\": numeric_run_num,\n",
    "                                    \"well_num\": well_num,\n",
    "                                    \"neuron_id\": neuron_id,\n",
    "                                    \"branch_length\": row['branchLen']\n",
    "                                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the data using your existing structure\n",
    "data_dict = {}\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                key_name = file.stem\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n",
    "\n",
    "# Get branch length data for detected neurons\n",
    "branch_data = search_branch_data_across_runs(data_dict)\n",
    "\n",
    "if not branch_data.empty:\n",
    "    # Add density information\n",
    "    def get_density(run_num, well_num):\n",
    "        mapping = run_well_density_map.get(str(run_num), {})\n",
    "        return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "    branch_data[\"Density\"] = branch_data.apply(\n",
    "        lambda row: get_density(row[\"run_number\"], row[\"well_num\"]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove unknown densities\n",
    "    branch_data = branch_data[branch_data[\"Density\"] != \"Unknown\"]\n",
    "    \n",
    "    # Calculate sum of branch lengths per neuron\n",
    "    neuron_branch_sums = branch_data.groupby(['run_number', 'well_num', 'neuron_id', 'Density'])['branch_length'].sum().reset_index()\n",
    "    neuron_branch_sums.rename(columns={'branch_length': 'total_branch_length'}, inplace=True)\n",
    "    \n",
    "    # Sort densities numerically\n",
    "    def extract_density_value(density_str):\n",
    "        match = re.search(r\"(\\d+)\", density_str)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    neuron_branch_sums[\"Density_Value\"] = neuron_branch_sums[\"Density\"].apply(extract_density_value)\n",
    "    neuron_branch_sums = neuron_branch_sums.sort_values(\"Density_Value\")\n",
    "    \n",
    "    # === Create plots for each run ===\n",
    "    for run_num, run_data in neuron_branch_sums.groupby(\"run_number\"):\n",
    "        print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "        \n",
    "        # Calculate mean total branch length per density\n",
    "        mean_data = run_data.groupby(\"Density\")[\"total_branch_length\"].agg(['mean', 'count']).reset_index()\n",
    "        mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot for means\n",
    "        bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                       alpha=0.7, color='lightgreen', edgecolor='darkgreen', linewidth=2,\n",
    "                       label='Mean Total Branch Length')\n",
    "        \n",
    "        # Add individual neuron markers\n",
    "        for density in mean_data[\"Density\"]:\n",
    "            density_data = run_data[run_data[\"Density\"] == density]\n",
    "            x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "            \n",
    "            # Add jitter for better visibility\n",
    "            jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "            plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                       density_data[\"total_branch_length\"],\n",
    "                       color='orange', alpha=0.6, s=40, zorder=5,\n",
    "                       label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "        \n",
    "        # Add mean values as text on bars\n",
    "        for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "            plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.0f}\\n(n={count})', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f\"Total Branch Length per Detected Neuron by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Density\", fontsize=14)\n",
    "        plt.ylabel(\"Total Branch Length (Âµm)\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Summary for Run {run_num}:\")\n",
    "        for _, row in mean_data.iterrows():\n",
    "            print(f\"  {row['Density']}: Mean Total Branch Length = {row['mean']:.0f} Âµm, Count = {row['count']} neurons\")\n",
    "\n",
    "else:\n",
    "    print(\"No branch length data found. Please check column names and file structure.\")\n",
    "    print(\"Available columns in branch files:\")\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        for file_name, df in file_dict.items():\n",
    "            if 'branch' in file_name.lower():\n",
    "                print(f\"Run {run_num}, File {file_name}: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load metadata and create density mapping ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load detected neurons data ===\n",
    "detected_footprints = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "detected_footprints[\"Run #\"] = detected_footprints[\"Run #\"].astype(str)\n",
    "detected_footprints[\"Well_Num\"] = detected_footprints[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "detected_footprints = detected_footprints[detected_footprints[\"Detection Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Create set of detected neurons for filtering\n",
    "detected_neurons = set()\n",
    "for _, row in detected_footprints.iterrows():\n",
    "    detected_neurons.add((str(row[\"Run #\"]), row[\"Well_Num\"], row[\"FootprintNeuron #\"]))\n",
    "\n",
    "# === Load branch length data using your directory structure ===\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "def search_column_across_runs(data_dict, column_name):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        # Extract numeric part of run_num\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        for file_name, df in file_dict.items():\n",
    "            if column_name in df.columns:\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.notna(row[column_name]) and pd.notna(row['neuron']) and pd.notna(row['wellNo']):\n",
    "                        well_num = int(row['wellNo'])\n",
    "                        neuron_id = int(row['neuron'])\n",
    "                        \n",
    "                        # Check if this neuron is in our detected neurons set\n",
    "                        if (str(numeric_run_num), well_num, neuron_id) in detected_neurons:\n",
    "                            results.append({\n",
    "                                \"run_number\": numeric_run_num,\n",
    "                                \"well_num\": well_num, \n",
    "                                \"neuron_id\": neuron_id,\n",
    "                                \"branch_length\": row[column_name]\n",
    "                            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the data using your existing structure\n",
    "data_dict = {}\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                key_name = file.stem\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n",
    "\n",
    "# Get branch length data for detected neurons using your original function structure\n",
    "column_name = \"branchLen\"\n",
    "branch_results = search_column_across_runs(data_dict, column_name)\n",
    "\n",
    "if not branch_results.empty:\n",
    "    # Add density information\n",
    "    def get_density(run_num, well_num):\n",
    "        mapping = run_well_density_map.get(str(run_num), {})\n",
    "        return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "    branch_results[\"Density\"] = branch_results.apply(\n",
    "        lambda row: get_density(row[\"run_number\"], row[\"well_num\"]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove unknown densities\n",
    "    branch_results = branch_results[branch_results[\"Density\"] != \"Unknown\"]\n",
    "    \n",
    "    # Convert branch_length to numeric, handling any string values\n",
    "    branch_results['branch_length'] = pd.to_numeric(branch_results['branch_length'], errors='coerce')\n",
    "    \n",
    "    # Remove any rows where branch_length couldn't be converted to numeric\n",
    "    branch_results = branch_results.dropna(subset=['branch_length'])\n",
    "    \n",
    "    # Calculate sum of branch lengths per neuron\n",
    "    neuron_branch_sums = branch_results.groupby(['run_number', 'well_num', 'neuron_id', 'Density'])['branch_length'].sum().reset_index()\n",
    "    neuron_branch_sums.rename(columns={'branch_length': 'total_branch_length'}, inplace=True)\n",
    "    \n",
    "    # Sort densities numerically\n",
    "    def extract_density_value(density_str):\n",
    "        match = re.search(r\"(\\d+)\", density_str)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    neuron_branch_sums[\"Density_Value\"] = neuron_branch_sums[\"Density\"].apply(extract_density_value)\n",
    "    neuron_branch_sums = neuron_branch_sums.sort_values(\"Density_Value\")\n",
    "    \n",
    "    # === Create plots for each run ===\n",
    "    for run_num, run_data in neuron_branch_sums.groupby(\"run_number\"):\n",
    "        print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "        \n",
    "        # Calculate mean total branch length per density\n",
    "        mean_data = run_data.groupby(\"Density\")[\"total_branch_length\"].agg(['mean', 'count']).reset_index()\n",
    "        mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot for means\n",
    "        bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                       alpha=0.7, color='lightgreen', edgecolor='darkgreen', linewidth=2,\n",
    "                       label='Mean Total Branch Length')\n",
    "        \n",
    "        # Add individual neuron markers\n",
    "        for density in mean_data[\"Density\"]:\n",
    "            density_data = run_data[run_data[\"Density\"] == density]\n",
    "            x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "            \n",
    "            # Add jitter for better visibility\n",
    "            jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "            plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                       density_data[\"total_branch_length\"],\n",
    "                       color='orange', alpha=0.6, s=40, zorder=5,\n",
    "                       label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "        \n",
    "        # Add mean values as text on bars\n",
    "        for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "            plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.0f}\\n(n={count})', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f\"Total Branch Length per Detected Neuron by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Density\", fontsize=14)\n",
    "        plt.ylabel(\"Total Branch Length (Âµm)\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Summary for Run {run_num}:\")\n",
    "        for _, row in mean_data.iterrows():\n",
    "            print(f\"  {row['Density']}: Mean Total Branch Length = {row['mean']:.0f} Âµm, Count = {row['count']} neurons\")\n",
    "\n",
    "else:\n",
    "    print(\"No branch length data found. Please check column names and file structure.\")\n",
    "    print(\"Available columns in branch files:\")\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        for file_name, df in file_dict.items():\n",
    "            if 'branch' in file_name.lower():\n",
    "                print(f\"Run {run_num}, File {file_name}: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load metadata and create density mapping ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load detected neurons data ===\n",
    "detected_footprints = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "detected_footprints[\"Run #\"] = detected_footprints[\"Run #\"].astype(str)\n",
    "detected_footprints[\"Well_Num\"] = detected_footprints[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "detected_footprints = detected_footprints[detected_footprints[\"Detection Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Create set of detected neurons for filtering\n",
    "detected_neurons = set()\n",
    "for _, row in detected_footprints.iterrows():\n",
    "    detected_neurons.add((str(row[\"Run #\"]), row[\"Well_Num\"], row[\"FootprintNeuron #\"]))\n",
    "\n",
    "# === Load branch data using your directory structure ===\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "data_dict = {}\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                key_name = file.stem\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n",
    "\n",
    "# Function to count branches per detected neuron\n",
    "def count_branches_per_neuron(data_dict):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        # Extract numeric part of run_num\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        \n",
    "        # Look for branch_metrics file\n",
    "        if 'branch_metrics' in file_dict:\n",
    "            df = file_dict['branch_metrics']\n",
    "            \n",
    "            # Count occurrences of each neuron (each row = one branch)\n",
    "            for _, row in df.iterrows():\n",
    "                if pd.notna(row['neuron']) and pd.notna(row['wellNo']):\n",
    "                    well_num = int(row['wellNo'])\n",
    "                    neuron_id = int(row['neuron'])\n",
    "                    \n",
    "                    # Check if this neuron is in our detected neurons set\n",
    "                    if (str(numeric_run_num), well_num, neuron_id) in detected_neurons:\n",
    "                        results.append({\n",
    "                            \"run_number\": numeric_run_num,\n",
    "                            \"well_num\": well_num,\n",
    "                            \"neuron_id\": neuron_id\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Get branch count data\n",
    "branch_count_data = count_branches_per_neuron(data_dict)\n",
    "\n",
    "if not branch_count_data.empty:\n",
    "    # Count branches per neuron (each row represents one branch)\n",
    "    neuron_branch_counts = branch_count_data.groupby(['run_number', 'well_num', 'neuron_id']).size().reset_index(name='branch_count')\n",
    "    \n",
    "    # Add density information\n",
    "    def get_density(run_num, well_num):\n",
    "        mapping = run_well_density_map.get(str(run_num), {})\n",
    "        return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "    neuron_branch_counts[\"Density\"] = neuron_branch_counts.apply(\n",
    "        lambda row: get_density(row[\"run_number\"], row[\"well_num\"]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove unknown densities\n",
    "    neuron_branch_counts = neuron_branch_counts[neuron_branch_counts[\"Density\"] != \"Unknown\"]\n",
    "    \n",
    "    # Sort densities numerically\n",
    "    def extract_density_value(density_str):\n",
    "        match = re.search(r\"(\\d+)\", density_str)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    neuron_branch_counts[\"Density_Value\"] = neuron_branch_counts[\"Density\"].apply(extract_density_value)\n",
    "    neuron_branch_counts = neuron_branch_counts.sort_values(\"Density_Value\")\n",
    "    \n",
    "    # === Create plots for each run ===\n",
    "    for run_num, run_data in neuron_branch_counts.groupby(\"run_number\"):\n",
    "        print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "        \n",
    "        # Calculate mean branch count per density\n",
    "        mean_data = run_data.groupby(\"Density\")[\"branch_count\"].agg(['mean', 'count']).reset_index()\n",
    "        mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot for means\n",
    "        bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                       alpha=0.7, color='lightcoral', edgecolor='darkred', linewidth=2,\n",
    "                       label='Mean Branch Count')\n",
    "        \n",
    "        # Add individual neuron markers\n",
    "        for density in mean_data[\"Density\"]:\n",
    "            density_data = run_data[run_data[\"Density\"] == density]\n",
    "            x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "            \n",
    "            # Add jitter for better visibility\n",
    "            jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "            plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                       density_data[\"branch_count\"],\n",
    "                       color='blue', alpha=0.6, s=40, zorder=5,\n",
    "                       label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "        \n",
    "        # Add mean values as text on bars\n",
    "        for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "            plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.1f}\\n(n={count})', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f\"Number of Branches per Detected Neuron by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Density\", fontsize=14)\n",
    "        plt.ylabel(\"Number of Branches per Neuron\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        # Set y-axis to start from 0 for better comparison\n",
    "        plt.ylim(bottom=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Summary for Run {run_num}:\")\n",
    "        for _, row in mean_data.iterrows():\n",
    "            print(f\"  {row['Density']}: Mean Branches = {row['mean']:.1f}, Count = {row['count']} neurons\")\n",
    "\n",
    "else:\n",
    "    print(\"No branch data found for detected neurons.\")\n",
    "    print(\"Check that the run numbers match between your branch_metrics files and detected footprints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f98533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DETECTED_CSV = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"\n",
    "ORIGINAL_BASE = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/DetectedPNGs\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(DETECTED_CSV)\n",
    "\n",
    "# Debug: Print first few rows to understand the data structure\n",
    "print(\"First few rows of detected footprints:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Clean the data - handle the Run # formatting\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str)\n",
    "original_run_numbers = df[\"Run #\"].unique()\n",
    "print(f\"\\nOriginal run numbers: {original_run_numbers}\")\n",
    "\n",
    "# Try both with and without zero-padding for run numbers\n",
    "df[\"Run #\"] = df[\"Run #\"].str.zfill(6)\n",
    "padded_run_numbers = df[\"Run #\"].unique()\n",
    "print(f\"Padded run numbers: {padded_run_numbers}\")\n",
    "\n",
    "# Build multiple search patterns to handle different file naming conventions\n",
    "detected_patterns = []\n",
    "for _, row in df.iterrows():\n",
    "    run_num = row[\"Run #\"]\n",
    "    well = row[\"Well #\"]\n",
    "    neuron_num = row[\"FootprintNeuron #\"]\n",
    "    \n",
    "    # Try different filename patterns\n",
    "    patterns = [\n",
    "        f\"FootprintNeuron#{neuron_num}.png\",\n",
    "        f\"FootprintNeuron{neuron_num}.png\", \n",
    "        f\"Neuron#{neuron_num}.png\",\n",
    "        f\"Neuron{neuron_num}.png\",\n",
    "        f\"footprint_{neuron_num}.png\",\n",
    "        f\"footprint_neuron_{neuron_num}.png\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        detected_patterns.append((run_num, well, pattern, neuron_num))\n",
    "\n",
    "print(f\"\\nTotal search patterns created: {len(detected_patterns)}\")\n",
    "\n",
    "# Also try original run numbers (without padding)\n",
    "for _, row in df.iterrows():\n",
    "    run_num = str(row[\"Run #\"]).lstrip('0')  # Remove leading zeros\n",
    "    if run_num == '':  # Handle case where run number was all zeros\n",
    "        run_num = '0'\n",
    "    well = row[\"Well #\"]\n",
    "    neuron_num = row[\"FootprintNeuron #\"]\n",
    "    \n",
    "    patterns = [\n",
    "        f\"FootprintNeuron#{neuron_num}.png\",\n",
    "        f\"FootprintNeuron{neuron_num}.png\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        detected_patterns.append((run_num, well, pattern, neuron_num))\n",
    "\n",
    "missing = []\n",
    "found = 0\n",
    "found_files = []\n",
    "\n",
    "print(f\"\\nSearching in: {ORIGINAL_BASE}\")\n",
    "print(\"Sample directory structure check...\")\n",
    "\n",
    "# First, let's explore the directory structure\n",
    "sample_dirs = []\n",
    "for root, dirs, files in os.walk(ORIGINAL_BASE):\n",
    "    if len(sample_dirs) < 10:  # Just get first 10 directories for debugging\n",
    "        sample_dirs.append(root)\n",
    "    if len(sample_dirs) >= 10:\n",
    "        break\n",
    "\n",
    "print(\"Sample directory paths:\")\n",
    "for i, dir_path in enumerate(sample_dirs[:5]):\n",
    "    print(f\"  {i+1}: {dir_path}\")\n",
    "\n",
    "# Walk all files in the directory\n",
    "print(\"\\nSearching for PNG files...\")\n",
    "all_png_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(ORIGINAL_BASE):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\") and (\"footprint\" in file.lower() or \"neuron\" in file.lower()):\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_png_files.append((root, file))\n",
    "            \n",
    "            # Extract Run and Well from path\n",
    "            parts = root.split(os.sep)\n",
    "            \n",
    "            # Try to find run number in path (look for 6-digit numbers, then shorter ones)\n",
    "            run_candidates = [p for p in parts if p.isdigit()]\n",
    "            run_id = None\n",
    "            \n",
    "            # First try 6-digit numbers\n",
    "            six_digit = [p for p in run_candidates if len(p) == 6]\n",
    "            if six_digit:\n",
    "                run_id = six_digit[0]\n",
    "            elif run_candidates:\n",
    "                # Try other digit lengths\n",
    "                run_id = max(run_candidates, key=len)  # Take the longest digit string\n",
    "            \n",
    "            # Find well identifier\n",
    "            well = next((p for p in parts if p.lower().startswith(\"well\")), None)\n",
    "            \n",
    "            if run_id and well:\n",
    "                # Check against all our patterns\n",
    "                for pattern_run, pattern_well, pattern_file, neuron_num in detected_patterns:\n",
    "                    if (run_id == pattern_run and well == pattern_well and file == pattern_file):\n",
    "                        # Create destination directory\n",
    "                        dst_dir = os.path.join(OUTPUT_BASE, f\"Run{run_id}\", well)\n",
    "                        os.makedirs(dst_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Create new filename with more info\n",
    "                        new_filename = f\"Run{run_id}_{well}_Neuron{neuron_num}_{file}\"\n",
    "                        dst_path = os.path.join(dst_dir, new_filename)\n",
    "                        \n",
    "                        try:\n",
    "                            shutil.copy2(full_path, dst_path)\n",
    "                            found += 1\n",
    "                            found_files.append((run_id, well, file, neuron_num))\n",
    "                            print(f\"âœ… Copied: {file} -> {new_filename} in {dst_dir}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"âŒ Error copying {file}: {e}\")\n",
    "                        break\n",
    "\n",
    "print(f\"\\nTotal PNG files found in directory: {len(all_png_files)}\")\n",
    "print(f\"âœ… Total PNGs copied: {found}\")\n",
    "\n",
    "# Show some examples of PNG files found\n",
    "print(f\"\\nFirst 10 PNG files found:\")\n",
    "for i, (root, file) in enumerate(all_png_files[:10]):\n",
    "    print(f\"  {i+1}: {file} in {root}\")\n",
    "\n",
    "# Create summary of what was found vs what was expected\n",
    "expected_neurons = set((row[\"Run #\"], row[\"Well #\"], row[\"FootprintNeuron #\"]) for _, row in df.iterrows())\n",
    "found_neurons = set((run_id, well, neuron_num) for run_id, well, file, neuron_num in found_files)\n",
    "\n",
    "print(f\"\\nExpected neurons: {len(expected_neurons)}\")\n",
    "print(f\"Found neurons: {len(found_neurons)}\")\n",
    "\n",
    "# Find missing ones\n",
    "missing_neurons = expected_neurons - found_neurons\n",
    "if missing_neurons:\n",
    "    print(f\"\\nMissing {len(missing_neurons)} neurons:\")\n",
    "    missing_df = pd.DataFrame(list(missing_neurons), columns=[\"Run #\", \"Well #\", \"FootprintNeuron #\"])\n",
    "    missing_df.to_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/missing_detailed.csv\", index=False)\n",
    "    print(\"Detailed missing list saved to missing_detailed.csv\")\n",
    "    \n",
    "    # Show first few missing\n",
    "    for i, (run_id, well, neuron_num) in enumerate(list(missing_neurons)[:5]):\n",
    "        print(f\"  Missing: Run {run_id}, {well}, Neuron {neuron_num}\")\n",
    "\n",
    "# Save found files summary\n",
    "if found_files:\n",
    "    found_df = pd.DataFrame(found_files, columns=[\"Run #\", \"Well #\", \"Original_Filename\", \"FootprintNeuron #\"])\n",
    "    found_df.to_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/successfully_copied.csv\", index=False)\n",
    "    print(\"Successfully copied files list saved to successfully_copied.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e40ef",
   "metadata": {},
   "source": [
    "### REST IS STILL IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d88595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DETECTED_CSV = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"\n",
    "ORIGINAL_BASE = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/DetectedPNGs\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(DETECTED_CSV)\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str).str.zfill(6)\n",
    "\n",
    "# Build dictionary for fast lookup\n",
    "detected_set = set(\n",
    "    (row[\"Run #\"], row[\"Well #\"], f\"FootprintNeuron#{row['FootprintNeuron #']}.png\")\n",
    "    for _, row in df.iterrows()\n",
    ")\n",
    "\n",
    "missing = []\n",
    "found = 0\n",
    "\n",
    "# Walk all files in the directory\n",
    "for root, _, files in os.walk(ORIGINAL_BASE):\n",
    "    for file in files:\n",
    "        if file.startswith(\"FootprintNeuron#\") and file.endswith(\".png\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "\n",
    "            # Extract Run and Well from path\n",
    "            parts = root.split(os.sep)\n",
    "            try:\n",
    "                run_idx = next(i for i, p in enumerate(parts) if p.isdigit() and len(p) == 6)\n",
    "                run_id = parts[run_idx]\n",
    "                well = next((p for p in parts if p.startswith(\"Well\")), None)\n",
    "            except StopIteration:\n",
    "                continue  # skip malformed paths\n",
    "\n",
    "            # Is this file in our detection CSV?\n",
    "            key = (run_id, well, file)\n",
    "            if key in detected_set:\n",
    "                dst_dir = os.path.join(OUTPUT_BASE, f\"Run{run_id}\", well)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "                dst_path = os.path.join(dst_dir, file)\n",
    "                shutil.copy2(full_path, dst_path)\n",
    "                found += 1\n",
    "                print(f\"âœ… Copied: {file} to {dst_dir}\")\n",
    "                detected_set.remove(key)\n",
    "\n",
    "# Any that weren't found\n",
    "for run_id, well, file in detected_set:\n",
    "    missing.append((run_id, well, file))\n",
    "    print(f\"âš ï¸ Still missing: Run {run_id}, {well}, {file}\")\n",
    "\n",
    "# Save missing list\n",
    "pd.DataFrame(missing, columns=[\"Run #\", \"Well #\", \"Filename\"]).to_csv(\n",
    "    \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/missing_detected_pngs_updated.csv\", index=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Total PNGs copied: {found}\")\n",
    "print(f\"ðŸ“„ Missing list saved as: missing_detected_pngs_updated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf482e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DETECTED_CSV = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"\n",
    "ORIGINAL_BASE = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/DetectedPNGs\"\n",
    "\n",
    "# Load CSV and filter rows where Detected == \"Yes\"\n",
    "df = pd.read_csv(DETECTED_CSV)\n",
    "df = df[df[\"Detected\"] == \"Yes\"]  # Filter only rows with Detected == \"Yes\"\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str).str.zfill(6)  # Ensure Run # is zero-padded to 6 digits\n",
    "\n",
    "# Build a set for fast lookup\n",
    "detected_set = set(\n",
    "    (row[\"Run #\"], row[\"Well #\"].strip(), f\"FootprintNeuron#{int(row['FootprintNeuron #'])}.png\")\n",
    "    for _, row in df.iterrows()\n",
    ")\n",
    "\n",
    "missing = []\n",
    "found = 0\n",
    "\n",
    "# Walk through all files in the directory\n",
    "for root, _, files in os.walk(ORIGINAL_BASE):\n",
    "    for file in files:\n",
    "        if file.startswith(\"FootprintNeuron#\") and file.endswith(\".png\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "\n",
    "            # Extract Run and Well from the path\n",
    "            parts = root.split(os.sep)\n",
    "            try:\n",
    "                # Extract Run ID (6-digit folder)\n",
    "                run_idx = next(i for i, p in enumerate(parts) if p.isdigit() and len(p) == 6)\n",
    "                run_id = parts[run_idx].zfill(6)  # Ensure zero-padding\n",
    "\n",
    "                # Extract Well (folder starting with \"Well\")\n",
    "                well = next((p for p in parts if p.startswith(\"Well\")), None)\n",
    "                if well is None:\n",
    "                    continue  # Skip if Well is not found\n",
    "\n",
    "                # Normalize Well and File\n",
    "                well = well.strip()\n",
    "                file = file.strip()\n",
    "\n",
    "                # Check if this file is in our detection set\n",
    "                key = (run_id, well, file)\n",
    "                if key in detected_set:\n",
    "                    dst_dir = os.path.join(OUTPUT_BASE, f\"Run{run_id}\", well)\n",
    "                    os.makedirs(dst_dir, exist_ok=True)\n",
    "                    dst_path = os.path.join(dst_dir, file)\n",
    "                    shutil.copy2(full_path, dst_path)\n",
    "                    found += 1\n",
    "                    print(f\"âœ… Copied: {file} to {dst_dir}\")\n",
    "                    detected_set.remove(key)\n",
    "            except StopIteration:\n",
    "                continue  # Skip malformed paths\n",
    "\n",
    "# Handle any files that weren't found\n",
    "for run_id, well, file in detected_set:\n",
    "    missing.append((run_id, well, file))\n",
    "    print(f\"âš ï¸ Still missing: Run {run_id}, {well}, {file}\")\n",
    "\n",
    "# Save the missing list to a CSV file\n",
    "missing_csv_path = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/missing_detected_pngs_updated.csv\"\n",
    "pd.DataFrame(missing, columns=[\"Run #\", \"Well #\", \"Filename\"]).to_csv(missing_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Total PNGs copied: {found}\")\n",
    "print(f\"ðŸ“„ Missing list saved as: {missing_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Searching for: Run {run_id}, Well {well}, File {file}\")\n",
    "print(f\"Detected Set: {key in detected_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017865ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
