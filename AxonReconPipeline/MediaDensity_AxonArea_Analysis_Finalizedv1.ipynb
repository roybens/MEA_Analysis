{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Root directory\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "# This will store all the data\n",
    "data_dict = {}\n",
    "\n",
    "# Walk through each subfolder\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        # Extract run number from folder name (after last \"_\")\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        # Drop the first row of every folder\n",
    "        run_data = {}\n",
    "        # Path to csv folder\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                # Remove \".csv\" from filename to use as key\n",
    "                key_name = file.stem  # e.g., branch_metrics, axon_summary_metrics\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53117316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data loaded for {len(run_number)} runs.\")\n",
    "# Extract and print all run numbers from data_dict keys\n",
    "all_run_numbers = list(data_dict.keys())\n",
    "print(\"Extracted run numbers:\", all_run_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb97aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def search_column_across_runs(data_dict, column_name):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        # Extract numeric part of run_num\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        for file_name, df in file_dict.items():\n",
    "            if column_name in df.columns:\n",
    "                for val in df[column_name].dropna():\n",
    "                    results.append({\n",
    "                        \"run_number\": numeric_run_num,  # Ensure numeric sorting\n",
    "                        \"file\": file_name,\n",
    "                        \"value\": val\n",
    "                    })\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values(by='run_number').reset_index(drop=False)\n",
    "    return df\n",
    "column_name = \"noDetectedNeurons\" #can change this to whatever column you want to search for \n",
    "# Run the function\n",
    "results_df = search_column_across_runs(data_dict, column_name)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9574bc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c70fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Clean up non-numeric rows\n",
    "results_df_cleaned = results_df[pd.to_numeric(results_df['value'], errors='coerce').notnull()].copy()\n",
    "results_df_cleaned['value'] = results_df_cleaned['value'].astype(float)\n",
    "\n",
    "# Step 2: Compute average and standard error per run\n",
    "summary_df = results_df_cleaned.groupby('run_number').agg(\n",
    "    mean_value=('value', 'mean'),\n",
    "    stderr_value=('value', lambda x: x.std(ddof=1) / np.sqrt(len(x)))\n",
    ").reset_index()\n",
    "\n",
    "# Step 3: Set up the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Bar plot with error bars\n",
    "sns.barplot(\n",
    "    x='run_number', y='mean_value', data=summary_df,\n",
    "    yerr=summary_df['stderr_value'], capsize=0.2,\n",
    "    color='skyblue', edgecolor='black'\n",
    ")\n",
    "\n",
    "# Dot plot of individual points (with jitter)\n",
    "sns.stripplot(\n",
    "    x='run_number', y='value', data=results_df_cleaned,\n",
    "    color='black', alpha=0.5, jitter=0.25\n",
    ")\n",
    "\n",
    "# Final touches\n",
    "plt.xlabel('Run Number')\n",
    "plt.ylabel(column_name)\n",
    "plt.title(f'Average {column_name} per Run (with SEM and Individual Data Points)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a6b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "# from pathlib import Path\n",
    "# import re\n",
    "\n",
    "# # Step 1: Load and filter the metadata\n",
    "# def load_and_filter_metadata(file_path):\n",
    "#     \"\"\"Load metadata and filter for Axon Tracking assays only\"\"\"\n",
    "#     metadata = pd.read_csv(file_path)  # Assuming tab-separated\n",
    "    \n",
    "#     # Filter for Axon Tracking assays only\n",
    "#     axon_tracking_data = metadata[metadata['Assay'] == 'Axon Tracking'].copy()\n",
    "    \n",
    "#     # Create a mapping of run number to density/media info\n",
    "#     run_density_map = {}\n",
    "    \n",
    "#     for _, row in axon_tracking_data.iterrows():\n",
    "#         run_num = row['Run #']\n",
    "#         wells = str(row['Wells_Recorded']).split(',')\n",
    "#         sources = str(row['Neuron Source']).split(',')\n",
    "        \n",
    "#         # Clean up wells and sources\n",
    "#         wells = [w.strip() for w in wells]\n",
    "#         sources = [s.strip() for s in sources]\n",
    "        \n",
    "#         # Extract density and media info for each well\n",
    "#         well_info = {}\n",
    "#         for well, source in zip(wells, sources):\n",
    "#             # Parse density (e.g., \"120K\") and media (e.g., \"NBP\", \"DMEM\")\n",
    "#             density_match = re.search(r'(\\d+)K', source)\n",
    "#             media_match = re.search(r'(NBP|DMEM)', source)\n",
    "            \n",
    "#             if density_match and media_match:\n",
    "#                 density = int(density_match.group(1))\n",
    "#                 media = media_match.group(1)\n",
    "#                 well_info[int(well)] = {'density': density, 'media': media, 'full_source': source}\n",
    "        \n",
    "#         run_density_map[run_num] = {\n",
    "#             'date': row['Date'],\n",
    "#             'div': row['DIV'],\n",
    "#             'id': row['ID'],\n",
    "#             'wells': well_info\n",
    "#         }\n",
    "    \n",
    "#     return run_density_map\n",
    "\n",
    "# # Step 2: Load detailed CSV data (your existing code)\n",
    "# def load_detailed_data(root_path):\n",
    "#     \"\"\"Load all detailed CSV data from run folders\"\"\"\n",
    "#     data_dict = {}\n",
    "    \n",
    "#     for run_folder in Path(root_path).iterdir():\n",
    "#         if run_folder.is_dir():\n",
    "#             run_number = int(run_folder.name.split(\"_\")[-1])\n",
    "            \n",
    "#             csv_folder = run_folder / \"csv\"\n",
    "#             if csv_folder.exists():\n",
    "#                 run_data = {}\n",
    "#                 for file in csv_folder.glob(\"*.csv\"):\n",
    "#                     key_name = file.stem\n",
    "#                     try:\n",
    "#                         run_data[key_name] = pd.read_csv(file)\n",
    "#                     except Exception as e:\n",
    "#                         print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "#                 data_dict[run_number] = run_data\n",
    "    \n",
    "#     return data_dict\n",
    "\n",
    "# # Step 3: Enhanced search function that includes density information\n",
    "# def search_column_with_density(data_dict, run_density_map, column_name):\n",
    "#     \"\"\"Search for column across runs and add density information\"\"\"\n",
    "#     results = []\n",
    "    \n",
    "#     for run_num, file_dict in data_dict.items():\n",
    "#         if run_num not in run_density_map:\n",
    "#             print(f\"Warning: Run {run_num} not found in density mapping\")\n",
    "#             continue\n",
    "            \n",
    "#         for file_name, df in file_dict.items():\n",
    "#             if column_name in df.columns:\n",
    "#                 for idx, val in df[column_name].dropna().items():\n",
    "#                     # Try to get neuron number to map to well\n",
    "#                     neuron_num = None\n",
    "#                     if 'neuron' in df.columns:\n",
    "#                         neuron_num = df.loc[idx, 'neuron']\n",
    "                    \n",
    "#                     # Map neuron to density (this might need adjustment based on your data structure)\n",
    "#                     density_info = None\n",
    "#                     wells_info = run_density_map[run_num]['wells']\n",
    "                    \n",
    "#                     # If we have neuron number, try to map it to a well\n",
    "#                     if neuron_num is not None and neuron_num in wells_info:\n",
    "#                         density_info = wells_info[neuron_num]\n",
    "#                     elif len(wells_info) == 1:\n",
    "#                         # If only one well, use that\n",
    "#                         density_info = list(wells_info.values())[0]\n",
    "#                     else:\n",
    "#                         # Take the first well as default (you might want to adjust this)\n",
    "#                         density_info = list(wells_info.values())[0]\n",
    "                    \n",
    "#                     if density_info:\n",
    "#                         results.append({\n",
    "#                             \"run_number\": run_num,\n",
    "#                             \"file\": file_name,\n",
    "#                             \"value\": val,\n",
    "#                             \"density\": density_info['density'],\n",
    "#                             \"media\": density_info['media'],\n",
    "#                             \"neuron\": neuron_num,\n",
    "#                             \"date\": run_density_map[run_num]['date'],\n",
    "#                             \"div\": run_density_map[run_num]['div']\n",
    "#                         })\n",
    "    \n",
    "#     df = pd.DataFrame(results)\n",
    "#     if not df.empty:\n",
    "#         df = df.sort_values(by=['run_number', 'density']).reset_index(drop=True)\n",
    "#     return df\n",
    "\n",
    "# # Step 4: Plotting functions\n",
    "# def plot_density_comparison(df, metric_name, plot_type='both'):\n",
    "#     \"\"\"Create plots comparing densities for a given metric\"\"\"\n",
    "#     # Clean up non-numeric values\n",
    "#     df_clean = df[pd.to_numeric(df['value'], errors='coerce').notnull()].copy()\n",
    "#     df_clean['value'] = df_clean['value'].astype(float)\n",
    "    \n",
    "#     if df_clean.empty:\n",
    "#         print(f\"No valid data found for {metric_name}\")\n",
    "#         return\n",
    "    \n",
    "#     # Create density labels\n",
    "#     df_clean['density_label'] = df_clean['density'].astype(str) + 'K ' + df_clean['media']\n",
    "    \n",
    "#     plt.figure(figsize=(15, 10))\n",
    "    \n",
    "#     if plot_type in ['both', 'bar']:\n",
    "#         plt.subplot(2, 2, 3)\n",
    "#         summary_df = df_clean.groupby('density_label').agg(\n",
    "#             mean_value=('value', 'mean'),\n",
    "#             stderr_value=('value', lambda x: x.std(ddof=1) / np.sqrt(len(x)) if len(x) > 1 else 0)\n",
    "#         ).reset_index()\n",
    "        \n",
    "#         # Drop rows with NaN or invalid values\n",
    "#         summary_df = summary_df.dropna(subset=['mean_value', 'stderr_value'])\n",
    "        \n",
    "#         # Ensure yerr matches y in shape\n",
    "#         yerr = summary_df['stderr_value'].values\n",
    "#         if len(yerr) != len(summary_df['mean_value']):\n",
    "#             print(\"Warning: yerr and y values do not match in shape. Skipping error bars.\")\n",
    "#             yerr = None\n",
    "        \n",
    "#         sns.barplot(data=summary_df, x='density_label', y='mean_value', \n",
    "#                     yerr=yerr, capsize=0.2, color='skyblue', edgecolor='black')\n",
    "#         plt.title(f'Mean {metric_name} by Density (Â±SEM)')\n",
    "#         plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Print summary statistics\n",
    "#     print(f\"\\nSummary Statistics for {metric_name}:\")\n",
    "#     print(\"-\" * 50)\n",
    "#     summary_stats = df_clean.groupby('density_label')['value'].agg([\n",
    "#         'count', 'mean', 'std', 'min', 'max'\n",
    "#     ]).round(3)\n",
    "#     print(summary_stats)\n",
    "\n",
    "# # Step 5: Statistical analysis\n",
    "# def statistical_analysis(df, metric_name):\n",
    "#     \"\"\"Perform statistical analysis comparing densities\"\"\"\n",
    "#     from scipy import stats\n",
    "    \n",
    "#     df_clean = df[pd.to_numeric(df['value'], errors='coerce').notnull()].copy()\n",
    "#     df_clean['value'] = df_clean['value'].astype(float)\n",
    "#     df_clean['density_label'] = df_clean['density'].astype(str) + 'K ' + df_clean['media']\n",
    "    \n",
    "#     # Order density labels from least to greatest\n",
    "#     density_order = df_clean.groupby('density_label')['density'].first().sort_values().index.tolist()\n",
    "#     df_clean['density_label'] = pd.Categorical(df_clean['density_label'], categories=density_order, ordered=True)\n",
    "    \n",
    "#     # Group data by density (in order)\n",
    "#     groups = []\n",
    "#     group_names = []\n",
    "#     for density in density_order:\n",
    "#         group_data = df_clean[df_clean['density_label'] == density]['value'].values\n",
    "#         if len(group_data) > 0:\n",
    "#             groups.append(group_data)\n",
    "#             group_names.append(density)\n",
    "    \n",
    "#     if len(groups) > 2:\n",
    "#         # ANOVA for multiple groups\n",
    "#         f_stat, p_value = stats.f_oneway(*groups)\n",
    "#         print(f\"\\nANOVA Results for {metric_name}:\")\n",
    "#         print(f\"F-statistic: {f_stat:.4f}\")\n",
    "#         print(f\"p-value: {p_value:.4f}\")\n",
    "        \n",
    "#         if p_value < 0.05:\n",
    "#             print(\"Significant difference found between groups!\")\n",
    "            \n",
    "#             # Post-hoc pairwise comparisons\n",
    "#             print(\"\\nPairwise t-tests (Bonferroni corrected):\")\n",
    "#             n_comparisons = len(groups) * (len(groups) - 1) // 2\n",
    "#             alpha_corrected = 0.05 / n_comparisons\n",
    "            \n",
    "#             for i in range(len(groups)):\n",
    "#                 for j in range(i+1, len(groups)):\n",
    "#                     t_stat, p_val = stats.ttest_ind(groups[i], groups[j])\n",
    "#                     significant = \"***\" if p_val < alpha_corrected else \"\"\n",
    "#                     print(f\"{group_names[i]} vs {group_names[j]}: p = {p_val:.4f} {significant}\")\n",
    "    \n",
    "#     elif len(groups) == 2:\n",
    "#         # t-test for two groups\n",
    "#         t_stat, p_value = stats.ttest_ind(groups[0], groups[1])\n",
    "#         print(f\"\\nt-test Results for {metric_name}:\")\n",
    "#         print(f\"t-statistic: {t_stat:.4f}\")\n",
    "#         print(f\"p-value: {p_value:.4f}\")\n",
    "#         print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "\n",
    "# # Example usage:\n",
    "# # Replace these paths with your actual file paths\n",
    "# metadata_file_path = \"/mnt/disk20tb/shruti/MaxTwo_MEA Tracking Sheets_Sep2024 onwards_AR(AutoRecovered)(Media Density Exp_T3_070120).csv\"  # The tab-separated file you pasted\n",
    "# detailed_data_path = root_path\n",
    "\n",
    "# # Load data\n",
    "# print(\"Loading metadata...\")\n",
    "# run_density_map = load_and_filter_metadata(metadata_file_path)\n",
    "# print(f\"Found {len(run_density_map)} axon tracking runs\")\n",
    "\n",
    "# print(\"Loading detailed data...\")\n",
    "# data_dict = load_detailed_data(detailed_data_path)\n",
    "# print(f\"Loaded data for {len(data_dict)} runs\")\n",
    "\n",
    "# # Analyze different metrics\n",
    "# metrics_to_analyze = ['branchLen', 'neuronFiringRate', 'branchConductionVel', 'totNoSpikes']\n",
    "\n",
    "# for metric in metrics_to_analyze:\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Analyzing {metric}\")\n",
    "#     print('='*60)\n",
    "    \n",
    "#     # Get data with density information\n",
    "#     results_df = search_column_with_density(data_dict, run_density_map, metric)\n",
    "    \n",
    "#     if not results_df.empty:\n",
    "#         print(f\"Found {len(results_df)} data points for {metric}\")\n",
    "        \n",
    "#         # Create plots\n",
    "#         plot_density_comparison(results_df, metric)\n",
    "        \n",
    "#         # Statistical analysis\n",
    "#         statistical_analysis(results_df, metric)\n",
    "#     else:\n",
    "#         print(f\"No data found for {metric}\")\n",
    "\n",
    "# # Function to explore available metrics\n",
    "# def explore_available_metrics(data_dict):\n",
    "#     \"\"\"Find all available column names across all files\"\"\"\n",
    "#     all_columns = set()\n",
    "#     for run_num, file_dict in data_dict.items():\n",
    "#         for file_name, df in file_dict.items():\n",
    "#             all_columns.update(df.columns)\n",
    "    \n",
    "#     print(\"Available metrics to analyze:\")\n",
    "#     for col in sorted(all_columns):\n",
    "#         print(f\"  - {col}\")\n",
    "    \n",
    "#     return sorted(all_columns)\n",
    "\n",
    "# # Uncomment to see all available metrics:\n",
    "# # available_metrics = explore_available_metrics(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0df282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Step 1: Load and filter the metadata from tracking sheet\n",
    "def load_and_filter_metadata(file_path):\n",
    "    \"\"\"Load metadata and filter for Axon Tracking assays only\"\"\"\n",
    "    metadata = pd.read_csv(file_path)  # Regular CSV format\n",
    "    \n",
    "    # Filter for Axon Tracking assays only\n",
    "    axon_tracking_data = metadata[metadata['Assay'] == 'Axon Tracking'].copy()\n",
    "    \n",
    "    # Create a mapping of run number to density/media info\n",
    "    run_density_map = {}\n",
    "    \n",
    "    for _, row in axon_tracking_data.iterrows():\n",
    "        run_num = row['Run #']\n",
    "        wells = str(row['Wells_Recorded']).split(',')\n",
    "        sources = str(row['Neuron Source']).split(',')\n",
    "        \n",
    "        # Clean up wells and sources\n",
    "        wells = [w.strip() for w in wells]\n",
    "        sources = [s.strip() for s in sources]\n",
    "        \n",
    "        # Extract density and media info for each well\n",
    "        well_info = {}\n",
    "        for well, source in zip(wells, sources):\n",
    "            # Parse density (e.g., \"120K\") and media (e.g., \"NBP\", \"DMEM\")\n",
    "            density_match = re.search(r'(\\d+)K', source)\n",
    "            media_match = re.search(r'(NBP|DMEM)', source)\n",
    "            \n",
    "            if density_match and media_match:\n",
    "                density = int(density_match.group(1))\n",
    "                media = media_match.group(1)\n",
    "                well_info[int(well)] = {'density': density, 'media': media, 'full_source': source}\n",
    "        \n",
    "        run_density_map[run_num] = {\n",
    "            'date': row['Date'],\n",
    "            'div': row['DIV'],\n",
    "            'id': row['ID'],\n",
    "            'wells': well_info\n",
    "        }\n",
    "    \n",
    "    return run_density_map\n",
    "\n",
    "# Step 2: Load detailed CSV data (your existing code)\n",
    "def load_detailed_data(root_path):\n",
    "    \"\"\"Load all detailed CSV data from run folders\"\"\"\n",
    "    data_dict = {}\n",
    "    \n",
    "    for run_folder in Path(root_path).iterdir():\n",
    "        if run_folder.is_dir():\n",
    "            run_number = int(run_folder.name.split(\"_\")[-1])\n",
    "            \n",
    "            csv_folder = run_folder / \"csv\"\n",
    "            if csv_folder.exists():\n",
    "                run_data = {}\n",
    "                for file in csv_folder.glob(\"*.csv\"):\n",
    "                    key_name = file.stem\n",
    "                    try:\n",
    "                        run_data[key_name] = pd.read_csv(file)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "                data_dict[run_number] = run_data\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Step 3: Enhanced search function that includes density information\n",
    "def search_column_with_density(data_dict, run_density_map, column_name):\n",
    "    \"\"\"Search for column across runs and add density information\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        if run_num not in run_density_map:\n",
    "            print(f\"Warning: Run {run_num} not found in density mapping\")\n",
    "            continue\n",
    "            \n",
    "        for file_name, df in file_dict.items():\n",
    "            if column_name in df.columns:\n",
    "                for idx, val in df[column_name].dropna().items():\n",
    "                    # Try to get neuron number to map to well\n",
    "                    neuron_num = None\n",
    "                    if 'neuron' in df.columns:\n",
    "                        neuron_num = df.loc[idx, 'neuron']\n",
    "                    \n",
    "                    # Map neuron to density (this might need adjustment based on your data structure)\n",
    "                    density_info = None\n",
    "                    wells_info = run_density_map[run_num]['wells']\n",
    "                    \n",
    "                    # If we have neuron number, try to map it to a well\n",
    "                    if neuron_num is not None and neuron_num in wells_info:\n",
    "                        density_info = wells_info[neuron_num]\n",
    "                    elif len(wells_info) == 1:\n",
    "                        # If only one well, use that\n",
    "                        density_info = list(wells_info.values())[0]\n",
    "                    else:\n",
    "                        # Take the first well as default (you might want to adjust this)\n",
    "                        density_info = list(wells_info.values())[0]\n",
    "                    \n",
    "                    if density_info:\n",
    "                        results.append({\n",
    "                            \"run_number\": run_num,\n",
    "                            \"file\": file_name,\n",
    "                            \"value\": val,\n",
    "                            \"density\": density_info['density'],\n",
    "                            \"media\": density_info['media'],\n",
    "                            \"neuron\": neuron_num,\n",
    "                            \"date\": run_density_map[run_num]['date'],\n",
    "                            \"div\": run_density_map[run_num]['div']\n",
    "                        })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        df = df.sort_values(by=['run_number', 'density']).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Step 4: Plotting functions\n",
    "def plot_density_comparison(df, metric_name, plot_type='both'):\n",
    "    \"\"\"Create plots comparing densities for a given metric\"\"\"\n",
    "    # Clean up non-numeric values\n",
    "    df_clean = df[pd.to_numeric(df['value'], errors='coerce').notnull()].copy()\n",
    "    df_clean['value'] = df_clean['value'].astype(float)\n",
    "    \n",
    "    if df_clean.empty:\n",
    "        print(f\"No valid data found for {metric_name}\")\n",
    "        return\n",
    "    \n",
    "    # Create density labels and order them from least to greatest\n",
    "    df_clean['density_label'] = df_clean['density'].astype(str) + 'K ' + df_clean['media']\n",
    "    \n",
    "    # Create ordered categories based on density (numerical) then media (alphabetical)\n",
    "    density_order = df_clean.groupby('density_label')['density'].first().sort_values().index.tolist()\n",
    "    df_clean['density_label'] = pd.Categorical(df_clean['density_label'], categories=density_order, ordered=True)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    if plot_type in ['both', 'violin']:\n",
    "        plt.subplot(2, 2, 1)\n",
    "        sns.violinplot(data=df_clean, x='density_label', y='value', order=density_order)\n",
    "        plt.title(f'{metric_name} Distribution by Density')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    if plot_type in ['both', 'box']:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        sns.boxplot(data=df_clean, x='density_label', y='value', order=density_order)\n",
    "        plt.title(f'{metric_name} Box Plot by Density')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    if plot_type in ['both', 'bar']:\n",
    "        plt.subplot(2, 2, 3)\n",
    "        summary_df = df_clean.groupby('density_label').agg(\n",
    "            mean_value=('value', 'mean'),\n",
    "            stderr_value=('value', lambda x: x.std(ddof=1) / np.sqrt(len(x)) if len(x) > 1 else 0)\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Reorder summary_df according to density_order\n",
    "        summary_df['density_label'] = pd.Categorical(summary_df['density_label'], categories=density_order, ordered=True)\n",
    "        summary_df = summary_df.sort_values('density_label').reset_index(drop=True)\n",
    "        \n",
    "        # Create bar plot\n",
    "        bars = plt.bar(range(len(summary_df)), summary_df['mean_value'], \n",
    "                      yerr=summary_df['stderr_value'], capsize=5,\n",
    "                      color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Density')\n",
    "        plt.ylabel(f'Mean {metric_name}')\n",
    "        plt.title(f'Mean {metric_name} by Density (Â±SEM)')\n",
    "        plt.xticks(range(len(summary_df)), summary_df['density_label'], rotation=45)\n",
    "    \n",
    "    if plot_type in ['both', 'strip']:\n",
    "        plt.subplot(2, 2, 4)\n",
    "        sns.stripplot(data=df_clean, x='density_label', y='value', \n",
    "                     alpha=0.7, jitter=0.3, size=4, order=density_order)\n",
    "        plt.title(f'Individual {metric_name} Values by Density')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics (ordered)\n",
    "    print(f\"\\nSummary Statistics for {metric_name}:\")\n",
    "    print(\"-\" * 50)\n",
    "    summary_stats = df_clean.groupby('density_label')['value'].agg([\n",
    "        'count', 'mean', 'std', 'min', 'max'\n",
    "    ]).round(3)\n",
    "    # Reorder by density\n",
    "    summary_stats = summary_stats.reindex(density_order)\n",
    "    print(summary_stats)\n",
    "\n",
    "# Step 5: Statistical analysis\n",
    "def statistical_analysis(df, metric_name):\n",
    "    \"\"\"Perform statistical analysis comparing densities\"\"\"\n",
    "    try:\n",
    "        from scipy import stats\n",
    "    except ImportError:\n",
    "        print(\"scipy not available. Skipping statistical analysis.\")\n",
    "        return\n",
    "    \n",
    "    df_clean = df[pd.to_numeric(df['value'], errors='coerce').notnull()].copy()\n",
    "    df_clean['value'] = df_clean['value'].astype(float)\n",
    "    df_clean['density_label'] = df_clean['density'].astype(str) + 'K ' + df_clean['media']\n",
    "    \n",
    "    # Order density labels from least to greatest\n",
    "    density_order = df_clean.groupby('density_label')['density'].first().sort_values().index.tolist()\n",
    "    df_clean['density_label'] = pd.Categorical(df_clean['density_label'], categories=density_order, ordered=True)\n",
    "    \n",
    "    # Group data by density (in order)\n",
    "    groups = []\n",
    "    group_names = []\n",
    "    for density in density_order:\n",
    "        group_data = df_clean[df_clean['density_label'] == density]['value'].values\n",
    "        if len(group_data) > 0:\n",
    "            groups.append(group_data)\n",
    "            group_names.append(density)\n",
    "    \n",
    "    if len(groups) > 2:\n",
    "        # ANOVA for multiple groups\n",
    "        f_stat, p_value = stats.f_oneway(*groups)\n",
    "        print(f\"\\nANOVA Results for {metric_name}:\")\n",
    "        print(f\"F-statistic: {f_stat:.4f}\")\n",
    "        print(f\"p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"Significant difference found between groups!\")\n",
    "            \n",
    "            # Post-hoc pairwise comparisons\n",
    "            print(\"\\nPairwise t-tests (Bonferroni corrected):\")\n",
    "            n_comparisons = len(groups) * (len(groups) - 1) // 2\n",
    "            alpha_corrected = 0.05 / n_comparisons\n",
    "            \n",
    "            for i in range(len(groups)):\n",
    "                for j in range(i+1, len(groups)):\n",
    "                    t_stat, p_val = stats.ttest_ind(groups[i], groups[j])\n",
    "                    significant = \"***\" if p_val < alpha_corrected else \"\"\n",
    "                    print(f\"{group_names[i]} vs {group_names[j]}: p = {p_val:.4f} {significant}\")\n",
    "        else:\n",
    "            print(\"No significant differences found between groups.\")\n",
    "    \n",
    "    elif len(groups) == 2:\n",
    "        # t-test for two groups\n",
    "        t_stat, p_value = stats.ttest_ind(groups[0], groups[1])\n",
    "        print(f\"\\nt-test Results for {metric_name}:\")\n",
    "        print(f\"t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"p-value: {p_value:.4f}\")\n",
    "        print(f\"Significant: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "    else:\n",
    "        print(f\"Not enough groups for statistical comparison (found {len(groups)} groups)\")\n",
    "\n",
    "# Function to explore available metrics\n",
    "def explore_available_metrics(data_dict):\n",
    "    \"\"\"Find all available column names across all files\"\"\"\n",
    "    all_columns = set()\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        for file_name, df in file_dict.items():\n",
    "            all_columns.update(df.columns)\n",
    "    \n",
    "    print(\"Available metrics to analyze:\")\n",
    "    for col in sorted(all_columns):\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    return sorted(all_columns)\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these paths with your actual file path\n",
    "    metadata_file_path = \"/mnt/disk20tb/shruti/MaxTwo_MEA Tracking Sheets_Sep2024 onwards_AR(AutoRecovered)(Media Density Exp_T3_070120).csv\"  # The tab-separated file you pasted\n",
    "    detailed_data_path = root_path\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading metadata...\")\n",
    "    try:\n",
    "        run_density_map = load_and_filter_metadata(metadata_file_path)\n",
    "        print(f\"Found {len(run_density_map)} axon tracking runs\")\n",
    "        \n",
    "        # Print the runs found for debugging\n",
    "        print(\"Axon tracking runs found:\")\n",
    "        for run_num, info in run_density_map.items():\n",
    "            print(f\"  Run {run_num}: {len(info['wells'])} wells\")\n",
    "            for well, well_info in info['wells'].items():\n",
    "                print(f\"    Well {well}: {well_info['density']}K {well_info['media']}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading metadata: {e}\")\n",
    "        run_density_map = {}\n",
    "\n",
    "    print(\"\\nLoading detailed data...\")\n",
    "    try:\n",
    "        data_dict = load_detailed_data(detailed_data_path)\n",
    "        print(f\"Loaded data for {len(data_dict)} runs\")\n",
    "        \n",
    "        # Print which runs have data\n",
    "        print(\"Runs with detailed data:\", sorted(data_dict.keys()))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading detailed data: {e}\")\n",
    "        data_dict = {}\n",
    "\n",
    "    # Check for overlap between metadata and detailed data\n",
    "    if run_density_map and data_dict:\n",
    "        metadata_runs = set(run_density_map.keys())\n",
    "        detailed_runs = set(data_dict.keys())\n",
    "        overlap = metadata_runs.intersection(detailed_runs)\n",
    "        print(f\"\\nRuns with both metadata and detailed data: {sorted(overlap)}\")\n",
    "        \n",
    "        if not overlap:\n",
    "            print(\"WARNING: No overlap between metadata runs and detailed data runs!\")\n",
    "            print(f\"Metadata runs: {sorted(metadata_runs)}\")\n",
    "            print(f\"Detailed data runs: {sorted(detailed_runs)}\")\n",
    "\n",
    "    # Uncomment to see all available metrics:\n",
    "    if data_dict:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        available_metrics = explore_available_metrics(data_dict)\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    # Analyze different metrics\n",
    "    metrics_to_analyze = ['branchLen', 'neuronFiringRate', 'branchConductionVel', 'totNoSpikes']\n",
    "\n",
    "    for metric in metrics_to_analyze:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Analyzing {metric}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Get data with density information\n",
    "        results_df = search_column_with_density(data_dict, run_density_map, metric)\n",
    "        \n",
    "        if not results_df.empty:\n",
    "            print(f\"Found {len(results_df)} data points for {metric}\")\n",
    "            \n",
    "            # Print unique density combinations found\n",
    "            unique_densities = results_df.groupby(['density', 'media']).size().reset_index(name='count')\n",
    "            print(\"Density combinations found:\")\n",
    "            for _, row in unique_densities.iterrows():\n",
    "                print(f\"  {row['density']}K {row['media']}: {row['count']} data points\")\n",
    "            \n",
    "            # Create plots\n",
    "            plot_density_comparison(results_df, metric)\n",
    "            \n",
    "            # Statistical analysis\n",
    "            statistical_analysis(results_df, metric)\n",
    "        else:\n",
    "            print(f\"No data found for {metric}\")\n",
    "            \n",
    "            # Debug: check if column exists anywhere\n",
    "            found_in_files = []\n",
    "            for run_num, file_dict in data_dict.items():\n",
    "                for file_name, df in file_dict.items():\n",
    "                    if metric in df.columns:\n",
    "                        found_in_files.append(f\"Run {run_num}, file {file_name}\")\n",
    "            \n",
    "            if found_in_files:\n",
    "                print(f\"  But {metric} was found in: {found_in_files}\")\n",
    "            else:\n",
    "                print(f\"  {metric} not found in any files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf77724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your PNG file\n",
    "image_path = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/250717/M07137/AxonTracking/000148/analysis/FootprintExtraction_v1/0001/Well4/FootprintNeuron#16.png\"  # Replace with the actual path\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Convert image to numpy array\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Convert to grayscale using luminance formula\n",
    "gray_img = np.dot(img_np[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# Apply a grayscale threshold to isolate the signal (tweak this threshold as needed)\n",
    "threshold = 30  # Adjust if needed\n",
    "axon_mask = gray_img > threshold\n",
    "\n",
    "# Estimate pixel size from scale bar: 100 Âµm â‰ˆ 18 pixels â†’ 1 pixel â‰ˆ 5.56 Âµm\n",
    "pixel_size_um = 100 / 18\n",
    "pixel_area_um2 = pixel_size_um ** 2\n",
    "\n",
    "# Calculate axonal area\n",
    "axon_area_um2 = np.sum(axon_mask) * pixel_area_um2\n",
    "\n",
    "# Output results\n",
    "print(f\"Axon-covered area: {axon_area_um2:.2f} ÂµmÂ²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b355fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# CONFIGURATION\n",
    "BASE_DIR = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_CSV = \"axon_area_results.csv\"\n",
    "PIXELS_PER_100_UM = 18  # from scale bar\n",
    "PIXEL_AREA_UM2 = (100 / PIXELS_PER_100_UM) ** 2  # ÂµmÂ² per pixel\n",
    "THRESHOLD = 30  # grayscale or red channel threshold (adjust as needed)\n",
    "\n",
    "def is_target_image(filename):\n",
    "    return re.match(r\"FootprintNeuron#\\d+\\.png\", filename)\n",
    "\n",
    "def extract_metadata(filepath):\n",
    "    \"\"\"\n",
    "    Extract Plate ID, AxonTracking ID, Well number, Neuron number from the path\n",
    "    \"\"\"\n",
    "    parts = filepath.split(os.sep)\n",
    "    try:\n",
    "        plate_id = next(p for p in parts if p.startswith(\"M\"))\n",
    "        tracking_id = parts[parts.index(\"AxonTracking\") + 1]\n",
    "        well_match = re.search(r\"Well(\\d+)\", filepath)\n",
    "        neuron_match = re.search(r\"FootprintNeuron#(\\d+)\", filepath)\n",
    "        return {\n",
    "            \"plate_id\": plate_id,\n",
    "            \"tracking_id\": tracking_id,\n",
    "            \"well\": f\"Well{well_match.group(1)}\" if well_match else \"Unknown\",\n",
    "            \"neuron\": f\"Neuron#{neuron_match.group(1)}\" if neuron_match else \"Unknown\"\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"plate_id\": \"Unknown\", \"tracking_id\": \"Unknown\", \"well\": \"Unknown\", \"neuron\": \"Unknown\"}\n",
    "\n",
    "def analyze_image(image_path, threshold=THRESHOLD):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    # Use red channel as signal indicator (red = high ÂµV in colormap)\n",
    "    red_channel = img_np[:, :, 0]\n",
    "\n",
    "    # Apply threshold\n",
    "    signal_mask = red_channel > threshold\n",
    "    pixel_count = np.sum(signal_mask)\n",
    "    area_um2 = pixel_count * PIXEL_AREA_UM2\n",
    "\n",
    "    return area_um2, pixel_count\n",
    "\n",
    "def main():\n",
    "    results = []\n",
    "\n",
    "    for root, _, files in os.walk(BASE_DIR):\n",
    "        for file in files:\n",
    "            if is_target_image(file):\n",
    "                full_path = os.path.join(root, file)\n",
    "                metadata = extract_metadata(full_path)\n",
    "                area_um2, pixel_count = analyze_image(full_path)\n",
    "\n",
    "                results.append({\n",
    "                    \"Plate ID\": metadata[\"plate_id\"],\n",
    "                    \"Tracking ID\": metadata[\"tracking_id\"],\n",
    "                    \"Well\": metadata[\"well\"],\n",
    "                    \"Neuron\": metadata[\"neuron\"],\n",
    "                    \"Pixel Count\": pixel_count,\n",
    "                    \"Axon Area (ÂµmÂ²)\": round(area_um2, 2),\n",
    "                    \"Threshold Used\": THRESHOLD\n",
    "                })\n",
    "                print(results)\n",
    "\n",
    "    # Output nicely\n",
    "    print(\"\\nðŸ“Š Axon Area Analysis Results:\\n\")\n",
    "    for r in results:\n",
    "        print(f\"{r['Plate ID']} | {r['Tracking ID']} | {r['Well']} | {r['Neuron']}: {r['Axon Area (ÂµmÂ²)']} ÂµmÂ² (Pixels: {r['Pixel Count']})\")\n",
    "\n",
    "    # Save to CSV\n",
    "    with open(OUTPUT_CSV, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=results[0].keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(results)\n",
    "\n",
    "    print(f\"\\nâœ… Results saved to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the CSV\n",
    "df = pd.read_csv(\"axon_area_results.csv\")  # Update path if needed\n",
    "\n",
    "# Optional: sort for better viultssuals\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r'#(\\d+)').astype(int)\n",
    "df = df.sort_values([\"Well\", \"Neuron ID\"])\n",
    "\n",
    "# Plot 1: Bar Plot per Neuron (grouped by Well)\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x=\"Neuron\", y=\"Axon Area (ÂµmÂ²)\", hue=\"Well\", data=df, ci = None)\n",
    "plt.title(\"Axon Area per Neuron Grouped by Well\")\n",
    "plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Box Plot per Well\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x=\"Well\", y=\"Axon Area (ÂµmÂ²)\", data=df)\n",
    "plt.title(\"Distribution of Axon Area per Well\")\n",
    "plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f3dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"axon_area_results.csv\")\n",
    "\n",
    "# Extract neuron number to sort correctly\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r'#(\\d+)').astype(int)\n",
    "\n",
    "# Group by Tracking ID (run)\n",
    "for tracking_id, run_df in df.groupby(\"Tracking ID\"):\n",
    "    print(f\"\\nðŸ“ˆ Generating plots for Tracking ID: {tracking_id}\")\n",
    "\n",
    "    # Get all wells in this run\n",
    "    wells = run_df[\"Well\"].unique()\n",
    "\n",
    "    # Create a subplot for each well\n",
    "    fig, axes = plt.subplots(len(wells), 1, figsize=(12, 5 * len(wells)), sharey=False)\n",
    "\n",
    "    if len(wells) == 1:\n",
    "        axes = [axes]  # Ensure it's iterable\n",
    "\n",
    "    for ax, well in zip(axes, wells):\n",
    "        well_df = run_df[run_df[\"Well\"] == well].sort_values(\"Neuron ID\")\n",
    "        sns.barplot(x=\"Neuron\", y=\"Axon Area (ÂµmÂ²)\", data=well_df, ax=ax, ci=None)\n",
    "        ax.set_title(f\"{tracking_id} - {well}\")\n",
    "        ax.set_ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "        ax.set_xlabel(\"Neuron\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a0b0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"axon_area_results.csv\")\n",
    "\n",
    "# Extract Neuron ID number for sorting\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r'#(\\d+)').astype(int)\n",
    "\n",
    "# Sort for consistency\n",
    "df = df.sort_values([\"Tracking ID\", \"Well\", \"Neuron ID\"])\n",
    "\n",
    "# Group by run (Tracking ID)\n",
    "for tracking_id, run_df in df.groupby(\"Tracking ID\"):\n",
    "    print(f\"\\nðŸ“Š Plotting Tracking ID: {tracking_id}\")\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.barplot(\n",
    "        x=\"Neuron ID\",\n",
    "        y=\"Axon Area (ÂµmÂ²)\",\n",
    "        hue=\"Well\",\n",
    "        data=run_df,\n",
    "        ci=None,\n",
    "        dodge=True\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"Axon Area per Neuron â€” Run: {tracking_id}\")\n",
    "    plt.xlabel(\"Neuron ID\")\n",
    "    plt.ylabel(\"Axon Area (ÂµmÂ²)\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=\"Well\", bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c715c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# === Load tracking sheet (metadata) ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "\n",
    "# Ensure both Run # and Tracking ID are strings\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "\n",
    "# Filter for Axon Tracking only\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density media\"}\n",
    "run_well_label_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "\n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        media_match = re.search(r\"(NBP|DMEM)\", source)\n",
    "\n",
    "        if density_match and media_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K {media_match.group(1)}\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "\n",
    "    run_well_label_map[run_num] = mapping\n",
    "\n",
    "# === Load axon area results ===\n",
    "df = pd.read_csv(\"axon_area_results.csv\")\n",
    "\n",
    "# Ensure Tracking ID is string for matching\n",
    "df[\"Tracking ID\"] = df[\"Tracking ID\"].astype(str)\n",
    "\n",
    "# Ensure Well column is integer\n",
    "df[\"Well\"] = df[\"Well\"].astype(str).str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Extract Neuron ID number for sorting\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r\"#(\\d+)\").astype(int)\n",
    "\n",
    "# Filter only detected neurons with Axon Area > 800000\n",
    "df = df[df[\"Axon Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Sort for consistency\n",
    "df = df.sort_values([\"Tracking ID\", \"Well\", \"Neuron ID\"])\n",
    "\n",
    "# === Add Well Label column using mapping ===\n",
    "def get_well_label(tracking_id, well):\n",
    "    mapping = run_well_label_map.get(tracking_id, {})\n",
    "    label = mapping.get(well)\n",
    "    return f\"Well {well}: {label}\" if label else None\n",
    "\n",
    "df[\"Well Label\"] = df.apply(lambda row: get_well_label(row[\"Tracking ID\"], row[\"Well\"]), axis=1)\n",
    "\n",
    "# Exclude rows with missing Well Labels\n",
    "df = df[df[\"Well Label\"].notna()]\n",
    "\n",
    "# === Plot per run ===\n",
    "for tracking_id, run_df in df.groupby(\"Tracking ID\"):\n",
    "    print(f\"\\nðŸ“Š Plotting Tracking ID: {tracking_id}\")\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.barplot(\n",
    "        x=\"Neuron ID\",\n",
    "        y=\"Axon Area (ÂµmÂ²)\",\n",
    "        hue=\"Well Label\",\n",
    "        data=run_df,\n",
    "        ci=None,\n",
    "        dodge=True\n",
    "    )\n",
    "\n",
    "    # Enhance plot readability\n",
    "    plt.title(f\"Axon Area per Neuron â€” Run: {tracking_id}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Neuron ID\", fontsize=14)\n",
    "    plt.ylabel(\"Axon Area (ÂµmÂ²)\", fontsize=14)\n",
    "    plt.xticks(rotation=45, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.legend(title=\"Neuron Source\", bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=12, title_fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797c1465",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the PNG\n",
    "image_path = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/250714/M07137/AxonTracking/000103/analysis/FootprintExtraction_v1/0001/Well3/FootprintNeuron#14.png\"  # Replace with the actual path  # Change to your path\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "img_np = np.array(img)\n",
    "\n",
    "# Convert to grayscale\n",
    "gray_img = np.dot(img_np[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# Threshold to detect signal (axon area)\n",
    "threshold = 30\n",
    "axon_mask = gray_img > threshold\n",
    "\n",
    "# Estimate pixel size and area\n",
    "pixels_per_100um = 18\n",
    "pixel_area_um2 = (100 / pixels_per_100um) ** 2\n",
    "axon_area_um2 = np.sum(axon_mask) * pixel_area_um2\n",
    "\n",
    "print(f\"Estimated axon-covered area: {axon_area_um2:.2f} ÂµmÂ²\")\n",
    "\n",
    "# ðŸ”½ Save visualization of axon detection\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(gray_img, cmap='gray')  # grayscale background\n",
    "plt.imshow(axon_mask, cmap='Reds', alpha=0.5)  # red overlay for detected axon\n",
    "plt.axis('off')\n",
    "plt.title(\"Detected Axon Region\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"axon_detection_overlay.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"âœ… Saved axon detection overlay as 'axon_detection_overlay.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92d911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DETECTION_THRESHOLD_UM2 = 800_000  # Any footprint area above this = \"Detected\"\n",
    "import pandas as pd\n",
    "\n",
    "# Load existing analysis\n",
    "df = pd.read_csv(\"axon_area_results.csv\")\n",
    "\n",
    "# Set your detection threshold\n",
    "DETECTION_THRESHOLD_UM2 = 800_000\n",
    "\n",
    "# Add detection status\n",
    "df[\"Detected\"] = df[\"Axon Area (ÂµmÂ²)\"] >= DETECTION_THRESHOLD_UM2\n",
    "\n",
    "# Organize data\n",
    "df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r\"#(\\d+)\").astype(int)\n",
    "df = df.sort_values([\"Well\", \"Neuron ID\"])\n",
    "\n",
    "# âœ… Save full detection DataFrame\n",
    "df.to_csv(\"axon_detection_classified.csv\", index=False)\n",
    "\n",
    "# ðŸ“Š Detection summary per Well\n",
    "summary = df.groupby(\"Well\")[\"Detected\"].sum().reset_index()\n",
    "summary.columns = [\"Well\", \"Neurons Detected\"]\n",
    "summary[\"Total Neurons\"] = df.groupby(\"Well\")[\"Neuron\"].count().values\n",
    "summary[\"Detection Rate (%)\"] = (summary[\"Neurons Detected\"] / summary[\"Total Neurons\"] * 100).round(1)\n",
    "\n",
    "# âœ… Save summary table\n",
    "summary.to_csv(\"axon_detection_summary.csv\", index=False)\n",
    "\n",
    "# Print summary nicely\n",
    "print(\"\\nðŸ“Œ Detection Summary per Well:\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd5496",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# CONFIG\n",
    "BASE_DIR = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025\"\n",
    "PIXELS_PER_100UM = 18\n",
    "PIXEL_AREA_UM2 = (100 / PIXELS_PER_100UM) ** 2\n",
    "THRESHOLD = 30  # Threshold for red channel\n",
    "DETECTION_THRESHOLD_UM2 = 800_000  # Area threshold for calling it a detected axon\n",
    "\n",
    "def is_footprint_image(filename):\n",
    "    return re.match(r\"FootprintNeuron#\\d+\\.png\", filename)\n",
    "\n",
    "def extract_metadata(filepath):\n",
    "    parts = filepath.split(os.sep)\n",
    "    try:\n",
    "        plate_id = next(p for p in parts if p.startswith(\"M\"))\n",
    "        tracking_id = parts[parts.index(\"AxonTracking\") + 1]\n",
    "        well = next(p for p in parts if p.startswith(\"Well\"))\n",
    "        neuron_match = re.search(r\"FootprintNeuron#(\\d+)\", filepath)\n",
    "        neuron = f\"Neuron#{neuron_match.group(1)}\" if neuron_match else \"Unknown\"\n",
    "        return plate_id, tracking_id, well, neuron\n",
    "    except Exception:\n",
    "        return \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\"\n",
    "\n",
    "def analyze_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_np = np.array(img)\n",
    "    red_channel = img_np[:, :, 0]\n",
    "    mask = red_channel > THRESHOLD\n",
    "    area_um2 = np.sum(mask) * PIXEL_AREA_UM2\n",
    "    return area_um2, mask, img_np\n",
    "\n",
    "def save_overlay(img_np, mask, save_path):\n",
    "    gray_img = np.dot(img_np[..., :3], [0.2989, 0.5870, 0.1140])\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(gray_img, cmap=\"gray\")\n",
    "    plt.imshow(mask, cmap=\"Reds\", alpha=0.5)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    all_results = []\n",
    "\n",
    "    for root, _, files in os.walk(BASE_DIR):\n",
    "        for file in files:\n",
    "            if is_footprint_image(file):\n",
    "                full_path = os.path.join(root, file)\n",
    "                plate_id, tracking_id, well, neuron = extract_metadata(full_path)\n",
    "\n",
    "                try:\n",
    "                    area_um2, mask, img_np = analyze_image(full_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error analyzing {full_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                detected = area_um2 >= DETECTION_THRESHOLD_UM2\n",
    "                overlay_filename = f\"{plate_id}_{tracking_id}_{well}_{neuron}_overlay.png\"\n",
    "                overlay_path = os.path.join(OUTPUT_BASE, \"overlays\", plate_id, tracking_id, well, overlay_filename)\n",
    "\n",
    "                save_overlay(img_np, mask, overlay_path)\n",
    "\n",
    "                all_results.append({\n",
    "                    \"Plate ID\": plate_id,\n",
    "                    \"Tracking ID\": tracking_id,\n",
    "                    \"Well\": well,\n",
    "                    \"Neuron\": neuron,\n",
    "                    \"File Path\": full_path,\n",
    "                    \"Axon Area (ÂµmÂ²)\": round(area_um2, 2),\n",
    "                    \"Detected\": detected\n",
    "                })\n",
    "\n",
    "    # Create output DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df[\"Neuron ID\"] = df[\"Neuron\"].str.extract(r\"#(\\d+)\").astype(float)\n",
    "    df = df.sort_values([\"Plate ID\", \"Tracking ID\", \"Well\", \"Neuron ID\"])\n",
    "\n",
    "    # Save full results\n",
    "    os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "    classified_csv = os.path.join(OUTPUT_BASE, \"axon_detection_classified.csv\")\n",
    "    df.to_csv(classified_csv, index=False)\n",
    "\n",
    "    # Create summary\n",
    "    summary = df.groupby([\"Plate ID\", \"Tracking ID\", \"Well\"])[\"Detected\"].agg(\n",
    "        Neurons_Detected=\"sum\", Total_Neurons=\"count\"\n",
    "    ).reset_index()\n",
    "    summary[\"Detection Rate (%)\"] = (summary[\"Neurons_Detected\"] / summary[\"Total_Neurons\"] * 100).round(1)\n",
    "    summary_csv = os.path.join(OUTPUT_BASE, \"axon_detection_summary.csv\")\n",
    "    summary.to_csv(summary_csv, index=False)\n",
    "\n",
    "    print(\"\\nâœ… DONE!\")\n",
    "    print(f\"Results saved to:\\n - {classified_csv}\\n - {summary_csv}\")\n",
    "    print(f\"Overlay images saved in:\\n - {os.path.join(OUTPUT_BASE, 'overlays')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()'''\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bcb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# CONFIGURATION\n",
    "BASE_DIR = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_DIR = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/\"\n",
    "PIXELS_PER_100UM = 18\n",
    "PIXEL_AREA_UM2 = (100 / PIXELS_PER_100UM) ** 2\n",
    "THRESHOLD = 30\n",
    "DETECTION_THRESHOLD_UM2 = 800_000\n",
    "RUNS_TO_INCLUDE = {'000044', '000089', '000103', '000148', '000173', '000225'}\n",
    "\n",
    "def is_footprint_image(filename):\n",
    "    return re.match(r\"FootprintNeuron#\\d+\\.png\", filename)\n",
    "\n",
    "def extract_metadata(filepath):\n",
    "    parts = filepath.split(os.sep)\n",
    "    try:\n",
    "        plate_id = next(p for p in parts if p.startswith(\"M\"))\n",
    "        run_id = parts[parts.index(\"AxonTracking\") + 1]\n",
    "        well = next(p for p in parts if re.match(r\"Well\\d+\", p))\n",
    "        neuron = re.search(r\"FootprintNeuron#(\\d+)\", filepath).group(1)\n",
    "        return plate_id, run_id, well, neuron\n",
    "    except Exception:\n",
    "        return \"Unknown\", \"Unknown\", \"Unknown\", \"Unknown\"\n",
    "\n",
    "def analyze_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_np = np.array(img)\n",
    "    red_channel = img_np[:, :, 0]\n",
    "    mask = red_channel > THRESHOLD\n",
    "    area_um2 = np.sum(mask) * PIXEL_AREA_UM2\n",
    "    return area_um2\n",
    "\n",
    "def main():\n",
    "    detected_rows = []\n",
    "    summary_dict = {}\n",
    "\n",
    "    for root, _, files in os.walk(BASE_DIR):\n",
    "        for file in files:\n",
    "            if is_footprint_image(file):\n",
    "                full_path = os.path.join(root, file)\n",
    "                plate_id, run_id, well, neuron = extract_metadata(full_path)\n",
    "\n",
    "                if run_id not in RUNS_TO_INCLUDE:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    area = analyze_image(full_path)\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Error processing {full_path}: {e}\")\n",
    "                    continue\n",
    "\n",
    "                detected = area >= DETECTION_THRESHOLD_UM2\n",
    "\n",
    "                if detected:\n",
    "                    detected_rows.append({\n",
    "                        \"Run #\": run_id,\n",
    "                        \"Well #\": well,\n",
    "                        \"FootprintNeuron #\": neuron,\n",
    "                        \"Detected\": \"Yes\",\n",
    "                        \"Detection Area (ÂµmÂ²)\": round(area, 2)\n",
    "                    })\n",
    "\n",
    "                # Track total & detected counts per run+well\n",
    "                key = (run_id, well)\n",
    "                if key not in summary_dict:\n",
    "                    summary_dict[key] = {\"total\": 0, \"detected\": 0}\n",
    "                summary_dict[key][\"total\"] += 1\n",
    "                if detected:\n",
    "                    summary_dict[key][\"detected\"] += 1\n",
    "\n",
    "    # Output DataFrame 1: All detected neurons\n",
    "    df_detected = pd.DataFrame(detected_rows)\n",
    "    detected_csv = os.path.join(OUTPUT_DIR, \"axon_detected_footprints.csv\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    df_detected.to_csv(detected_csv, index=False)\n",
    "\n",
    "    # Output DataFrame 2: Summary per well\n",
    "    summary_data = []\n",
    "    for (run, well), stats in summary_dict.items():\n",
    "        detection_rate = (stats[\"detected\"] / stats[\"total\"]) * 100\n",
    "        summary_data.append({\n",
    "            \"Run #\": run,\n",
    "            \"Well #\": well,\n",
    "            \"Neurons Detected\": stats[\"detected\"],\n",
    "            \"Total Neurons\": stats[\"total\"],\n",
    "            \"Detection Rate (%)\": round(detection_rate, 1)\n",
    "        })\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_csv = os.path.join(OUTPUT_DIR, \"axon_detection_summary_per_well.csv\")\n",
    "    df_summary.to_csv(summary_csv, index=False)\n",
    "\n",
    "    print(\"âœ… CSVs generated:\")\n",
    "    print(f\" - {detected_csv}\")\n",
    "    print(f\" - {summary_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9f39e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detection_summary_per_well.csv\"  # Update with the correct path\n",
    "df_detection_summary = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(df_detection_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904784a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the CSV file\n",
    "file_path = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"  # Update with the correct path\n",
    "df_detected_footprints = pd.read_csv(file_path)\n",
    "\n",
    "# Preview the data\n",
    "print(df_detected_footprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb3f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by 'Well #' and iterate through each well\n",
    "wells = df_detection_summary[\"Well #\"].unique()\n",
    "\n",
    "for well in wells:\n",
    "    # Filter data for the current well\n",
    "    df_well = df_detection_summary[df_detection_summary[\"Well #\"] == well]\n",
    "    \n",
    "    # Plot detected neuron count across runs\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(df_well[\"Run #\"], df_well[\"Neurons Detected\"], marker='o', label=f\"Well {well}\")\n",
    "    plt.title(f\"Detected Neurons Across Runs for Well {well}\", fontsize=14)\n",
    "    plt.xlabel(\"Run #\", fontsize=12)\n",
    "    plt.ylabel(\"Detected Neuron #\", fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf37da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pivot the data to prepare for grouped bar plot\n",
    "df_pivot = df_detection_summary.pivot(index=\"Run #\", columns=\"Well #\", values=\"Neurons Detected\")\n",
    "\n",
    "# Plot grouped bar chart\n",
    "df_pivot.plot(kind=\"bar\", figsize=(12, 8), width=0.8)\n",
    "plt.title(\"Detected Neurons Across Runs for All Wells\", fontsize=14)\n",
    "plt.xlabel(\"Run #\", fontsize=12)\n",
    "plt.ylabel(\"Detected Neuron #\", fontsize=12)\n",
    "plt.legend(title=\"Well #\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725db4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pivot the data to prepare for grouped bar plot\n",
    "df_pivot = df_detection_summary.pivot(index=\"Run #\", columns=\"Well #\", values=\"Neurons Detected\")\n",
    "\n",
    "# Manual well label mapping\n",
    "well_labels = {\n",
    "    2: \"Well 2: 80K NBP\",\n",
    "    4: \"Well 4: 120K NBP\", \n",
    "    5: \"Well 5: 100K NBP\"\n",
    "}\n",
    "\n",
    "# Create enhanced column labels\n",
    "enhanced_columns = []\n",
    "for col in df_pivot.columns:\n",
    "    # Extract number from column name like \"Well1\", \"Well2\", etc.\n",
    "    if isinstance(col, str) and col.startswith(\"Well\"):\n",
    "        well_num = int(col.replace(\"Well\", \"\"))\n",
    "    else:\n",
    "        well_num = int(col) if not isinstance(col, str) else col\n",
    "    \n",
    "    if well_num in well_labels:\n",
    "        enhanced_columns.append(well_labels[well_num])\n",
    "    else:\n",
    "        enhanced_columns.append(col)  # Keep original column name\n",
    "\n",
    "# Update the column names\n",
    "df_pivot.columns = enhanced_columns\n",
    "\n",
    "# Plot grouped bar chart (same as your original)\n",
    "df_pivot.plot(kind=\"bar\", figsize=(12, 8), width=0.8)\n",
    "plt.title(\"Detected Neurons Across Runs for All Wells\", fontsize=14)\n",
    "plt.xlabel(\"Run #\", fontsize=12)\n",
    "plt.ylabel(\"Detected Neuron #\", fontsize=12)\n",
    "plt.legend(title=\"Well #\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.grid(axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff27a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# === Load tracking sheet (metadata) ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load axon footprint data ===\n",
    "df = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "\n",
    "# Clean and prepare data\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str)\n",
    "df[\"Well #\"] = df[\"Well #\"].astype(str)\n",
    "df[\"Well_Num\"] = df[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "\n",
    "# Filter only detected neurons (Detection Area > 800000)\n",
    "df_detected = df[df[\"Detection Area (ÂµmÂ²)\"] > 800000].copy()\n",
    "\n",
    "# === Add Density column using mapping ===\n",
    "def get_density(run_num, well_num):\n",
    "    mapping = run_well_density_map.get(run_num, {})\n",
    "    return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "df_detected[\"Density\"] = df_detected.apply(\n",
    "    lambda row: get_density(row[\"Run #\"], row[\"Well_Num\"]), axis=1\n",
    ")\n",
    "\n",
    "# Remove rows with Unknown density\n",
    "df_detected = df_detected[df_detected[\"Density\"] != \"Unknown\"]\n",
    "\n",
    "# Sort densities numerically for consistent ordering\n",
    "def extract_density_value(density_str):\n",
    "    match = re.search(r\"(\\d+)\", density_str)\n",
    "    return int(match.group(1)) if match else 0\n",
    "\n",
    "df_detected[\"Density_Value\"] = df_detected[\"Density\"].apply(extract_density_value)\n",
    "df_detected = df_detected.sort_values(\"Density_Value\")\n",
    "\n",
    "# === Create plots for each run ===\n",
    "for run_num, run_data in df_detected.groupby(\"Run #\"):\n",
    "    print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "    \n",
    "    # Calculate mean axonal area per density\n",
    "    mean_data = run_data.groupby(\"Density\")[\"Detection Area (ÂµmÂ²)\"].agg(['mean', 'count']).reset_index()\n",
    "    mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Create bar plot for means\n",
    "    bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                   alpha=0.7, color='lightblue', edgecolor='navy', linewidth=2,\n",
    "                   label='Mean Axonal Area')\n",
    "    \n",
    "    # Add individual neuron markers\n",
    "    for density in mean_data[\"Density\"]:\n",
    "        density_data = run_data[run_data[\"Density\"] == density]\n",
    "        x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "        \n",
    "        # Add jitter to x-position for better visibility\n",
    "        jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "        plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                   density_data[\"Detection Area (ÂµmÂ²)\"],\n",
    "                   color='red', alpha=0.6, s=40, zorder=5,\n",
    "                   label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "    \n",
    "    # Add mean values as text on bars\n",
    "    for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "        plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.0f}\\n(n={count})', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f\"Axonal Area by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Density\", fontsize=14)\n",
    "    plt.ylabel(\"Axonal Area (ÂµmÂ²)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    \n",
    "    # Format y-axis to show values in a readable format\n",
    "    ax = plt.gca()\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1000:.0f}K'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"Summary for Run {run_num}:\")\n",
    "    for _, row in mean_data.iterrows():\n",
    "        print(f\"  {row['Density']}: Mean = {row['mean']:.0f} ÂµmÂ², Count = {row['count']} neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed95932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load metadata and create density mapping ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load detected neurons data ===\n",
    "detected_footprints = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "detected_footprints[\"Run #\"] = detected_footprints[\"Run #\"].astype(str)\n",
    "detected_footprints[\"Well_Num\"] = detected_footprints[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "detected_footprints = detected_footprints[detected_footprints[\"Detection Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Create set of detected neurons for filtering\n",
    "detected_neurons = set()\n",
    "for _, row in detected_footprints.iterrows():\n",
    "    detected_neurons.add((str(row[\"Run #\"]), row[\"Well_Num\"], row[\"FootprintNeuron #\"]))\n",
    "\n",
    "# === Load branch length data using your directory structure ===\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "def search_branch_data_across_runs(data_dict):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        \n",
    "        # Look for branch length data in branch_metrics or similar files\n",
    "        for file_name, df in file_dict.items():\n",
    "            if 'branch' in file_name.lower() and 'branchLen' in df.columns:\n",
    "                # Assuming there are columns for neuron ID and well\n",
    "                required_cols = ['branchLen']\n",
    "                \n",
    "                # Try to identify neuron and well columns (adjust based on your actual column names)\n",
    "                neuron_col = None\n",
    "                well_col = None\n",
    "                \n",
    "                for col in df.columns:\n",
    "                    if any(x in col.lower() for x in ['neuron', 'cell', 'id']):\n",
    "                        neuron_col = col\n",
    "                    if any(x in col.lower() for x in ['well']):\n",
    "                        well_col = col\n",
    "                \n",
    "                if neuron_col and well_col:\n",
    "                    for _, row in df.iterrows():\n",
    "                        if pd.notna(row['branchLen']):\n",
    "                            well_num = int(re.search(r'\\d+', str(row[well_col])).group()) if re.search(r'\\d+', str(row[well_col])) else None\n",
    "                            neuron_id = row[neuron_col]\n",
    "                            \n",
    "                            if well_num and (str(numeric_run_num), well_num, neuron_id) in detected_neurons:\n",
    "                                results.append({\n",
    "                                    \"run_number\": numeric_run_num,\n",
    "                                    \"well_num\": well_num,\n",
    "                                    \"neuron_id\": neuron_id,\n",
    "                                    \"branch_length\": row['branchLen']\n",
    "                                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the data using your existing structure\n",
    "data_dict = {}\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                key_name = file.stem\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n",
    "\n",
    "# Get branch length data for detected neurons\n",
    "branch_data = search_branch_data_across_runs(data_dict)\n",
    "\n",
    "if not branch_data.empty:\n",
    "    # Add density information\n",
    "    def get_density(run_num, well_num):\n",
    "        mapping = run_well_density_map.get(str(run_num), {})\n",
    "        return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "    branch_data[\"Density\"] = branch_data.apply(\n",
    "        lambda row: get_density(row[\"run_number\"], row[\"well_num\"]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove unknown densities\n",
    "    branch_data = branch_data[branch_data[\"Density\"] != \"Unknown\"]\n",
    "    \n",
    "    # Calculate sum of branch lengths per neuron\n",
    "    neuron_branch_sums = branch_data.groupby(['run_number', 'well_num', 'neuron_id', 'Density'])['branch_length'].sum().reset_index()\n",
    "    neuron_branch_sums.rename(columns={'branch_length': 'total_branch_length'}, inplace=True)\n",
    "    \n",
    "    # Sort densities numerically\n",
    "    def extract_density_value(density_str):\n",
    "        match = re.search(r\"(\\d+)\", density_str)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    neuron_branch_sums[\"Density_Value\"] = neuron_branch_sums[\"Density\"].apply(extract_density_value)\n",
    "    neuron_branch_sums = neuron_branch_sums.sort_values(\"Density_Value\")\n",
    "    \n",
    "    # === Create plots for each run ===\n",
    "    for run_num, run_data in neuron_branch_sums.groupby(\"run_number\"):\n",
    "        print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "        \n",
    "        # Calculate mean total branch length per density\n",
    "        mean_data = run_data.groupby(\"Density\")[\"total_branch_length\"].agg(['mean', 'count']).reset_index()\n",
    "        mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot for means\n",
    "        bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                       alpha=0.7, color='lightgreen', edgecolor='darkgreen', linewidth=2,\n",
    "                       label='Mean Total Branch Length')\n",
    "        \n",
    "        # Add individual neuron markers\n",
    "        for density in mean_data[\"Density\"]:\n",
    "            density_data = run_data[run_data[\"Density\"] == density]\n",
    "            x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "            \n",
    "            # Add jitter for better visibility\n",
    "            jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "            plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                       density_data[\"total_branch_length\"],\n",
    "                       color='orange', alpha=0.6, s=40, zorder=5,\n",
    "                       label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "        \n",
    "        # Add mean values as text on bars\n",
    "        for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "            plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.0f}\\n(n={count})', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f\"Total Branch Length per Detected Neuron by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Density\", fontsize=14)\n",
    "        plt.ylabel(\"Total Branch Length (Âµm)\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Summary for Run {run_num}:\")\n",
    "        for _, row in mean_data.iterrows():\n",
    "            print(f\"  {row['Density']}: Mean Total Branch Length = {row['mean']:.0f} Âµm, Count = {row['count']} neurons\")\n",
    "\n",
    "else:\n",
    "    print(\"No branch length data found. Please check column names and file structure.\")\n",
    "    print(\"Available columns in branch files:\")\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        for file_name, df in file_dict.items():\n",
    "            if 'branch' in file_name.lower():\n",
    "                print(f\"Run {run_num}, File {file_name}: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e2f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load metadata and create density mapping ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load detected neurons data ===\n",
    "detected_footprints = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "detected_footprints[\"Run #\"] = detected_footprints[\"Run #\"].astype(str)\n",
    "detected_footprints[\"Well_Num\"] = detected_footprints[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "detected_footprints = detected_footprints[detected_footprints[\"Detection Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Create set of detected neurons for filtering\n",
    "detected_neurons = set()\n",
    "for _, row in detected_footprints.iterrows():\n",
    "    detected_neurons.add((str(row[\"Run #\"]), row[\"Well_Num\"], row[\"FootprintNeuron #\"]))\n",
    "\n",
    "# === Load branch length data using your directory structure ===\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "def search_column_across_runs(data_dict, column_name):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        # Extract numeric part of run_num\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        for file_name, df in file_dict.items():\n",
    "            if column_name in df.columns:\n",
    "                for _, row in df.iterrows():\n",
    "                    if pd.notna(row[column_name]) and pd.notna(row['neuron']) and pd.notna(row['wellNo']):\n",
    "                        well_num = int(row['wellNo'])\n",
    "                        neuron_id = int(row['neuron'])\n",
    "                        \n",
    "                        # Check if this neuron is in our detected neurons set\n",
    "                        if (str(numeric_run_num), well_num, neuron_id) in detected_neurons:\n",
    "                            results.append({\n",
    "                                \"run_number\": numeric_run_num,\n",
    "                                \"well_num\": well_num, \n",
    "                                \"neuron_id\": neuron_id,\n",
    "                                \"branch_length\": row[column_name]\n",
    "                            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load the data using your existing structure\n",
    "data_dict = {}\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                key_name = file.stem\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n",
    "\n",
    "# Get branch length data for detected neurons using your original function structure\n",
    "column_name = \"branchLen\"\n",
    "branch_results = search_column_across_runs(data_dict, column_name)\n",
    "\n",
    "if not branch_results.empty:\n",
    "    # Add density information\n",
    "    def get_density(run_num, well_num):\n",
    "        mapping = run_well_density_map.get(str(run_num), {})\n",
    "        return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "    branch_results[\"Density\"] = branch_results.apply(\n",
    "        lambda row: get_density(row[\"run_number\"], row[\"well_num\"]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove unknown densities\n",
    "    branch_results = branch_results[branch_results[\"Density\"] != \"Unknown\"]\n",
    "    \n",
    "    # Convert branch_length to numeric, handling any string values\n",
    "    branch_results['branch_length'] = pd.to_numeric(branch_results['branch_length'], errors='coerce')\n",
    "    \n",
    "    # Remove any rows where branch_length couldn't be converted to numeric\n",
    "    branch_results = branch_results.dropna(subset=['branch_length'])\n",
    "    \n",
    "    # Calculate sum of branch lengths per neuron\n",
    "    neuron_branch_sums = branch_results.groupby(['run_number', 'well_num', 'neuron_id', 'Density'])['branch_length'].sum().reset_index()\n",
    "    neuron_branch_sums.rename(columns={'branch_length': 'total_branch_length'}, inplace=True)\n",
    "    \n",
    "    # Sort densities numerically\n",
    "    def extract_density_value(density_str):\n",
    "        match = re.search(r\"(\\d+)\", density_str)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    neuron_branch_sums[\"Density_Value\"] = neuron_branch_sums[\"Density\"].apply(extract_density_value)\n",
    "    neuron_branch_sums = neuron_branch_sums.sort_values(\"Density_Value\")\n",
    "    \n",
    "    # === Create plots for each run ===\n",
    "    for run_num, run_data in neuron_branch_sums.groupby(\"run_number\"):\n",
    "        print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "        \n",
    "        # Calculate mean total branch length per density\n",
    "        mean_data = run_data.groupby(\"Density\")[\"total_branch_length\"].agg(['mean', 'count']).reset_index()\n",
    "        mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot for means\n",
    "        bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                       alpha=0.7, color='lightgreen', edgecolor='darkgreen', linewidth=2,\n",
    "                       label='Mean Total Branch Length')\n",
    "        \n",
    "        # Add individual neuron markers\n",
    "        for density in mean_data[\"Density\"]:\n",
    "            density_data = run_data[run_data[\"Density\"] == density]\n",
    "            x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "            \n",
    "            # Add jitter for better visibility\n",
    "            jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "            plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                       density_data[\"total_branch_length\"],\n",
    "                       color='orange', alpha=0.6, s=40, zorder=5,\n",
    "                       label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "        \n",
    "        # Add mean values as text on bars\n",
    "        for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "            plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.0f}\\n(n={count})', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f\"Total Branch Length per Detected Neuron by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Density\", fontsize=14)\n",
    "        plt.ylabel(\"Total Branch Length (Âµm)\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Summary for Run {run_num}:\")\n",
    "        for _, row in mean_data.iterrows():\n",
    "            print(f\"  {row['Density']}: Mean Total Branch Length = {row['mean']:.0f} Âµm, Count = {row['count']} neurons\")\n",
    "\n",
    "else:\n",
    "    print(\"No branch length data found. Please check column names and file structure.\")\n",
    "    print(\"Available columns in branch files:\")\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        for file_name, df in file_dict.items():\n",
    "            if 'branch' in file_name.lower():\n",
    "                print(f\"Run {run_num}, File {file_name}: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1a9897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# === Load metadata and create density mapping ===\n",
    "metadata = pd.read_csv(metadata_file_path)\n",
    "metadata[\"Run #\"] = metadata[\"Run #\"].astype(str)\n",
    "axon_tracking_meta = metadata[metadata[\"Assay\"] == \"Axon Tracking\"].copy()\n",
    "\n",
    "# Build mapping: run_number â†’ {well_number: \"density\"}\n",
    "run_well_density_map = {}\n",
    "for _, row in axon_tracking_meta.iterrows():\n",
    "    run_num = str(row[\"Run #\"])\n",
    "    wells = str(row[\"Wells_Recorded\"]).split(\",\")\n",
    "    sources = str(row[\"Neuron Source\"]).split(\",\")\n",
    "    wells = [int(re.search(r\"\\d+\", w).group()) for w in wells if re.search(r\"\\d+\", w)]\n",
    "    sources = [s.strip() for s in sources]\n",
    "    \n",
    "    mapping = {}\n",
    "    for well, source in zip(wells, sources):\n",
    "        density_match = re.search(r\"(\\d+)K\", source)\n",
    "        if density_match:\n",
    "            mapping[well] = f\"{density_match.group(1)}K\"\n",
    "        else:\n",
    "            mapping[well] = source  # fallback\n",
    "    run_well_density_map[run_num] = mapping\n",
    "\n",
    "# === Load detected neurons data ===\n",
    "detected_footprints = pd.read_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\")\n",
    "detected_footprints[\"Run #\"] = detected_footprints[\"Run #\"].astype(str)\n",
    "detected_footprints[\"Well_Num\"] = detected_footprints[\"Well #\"].str.extract(r\"(\\d+)\").astype(int)\n",
    "detected_footprints = detected_footprints[detected_footprints[\"Detection Area (ÂµmÂ²)\"] > 800000]\n",
    "\n",
    "# Create set of detected neurons for filtering\n",
    "detected_neurons = set()\n",
    "for _, row in detected_footprints.iterrows():\n",
    "    detected_neurons.add((str(row[\"Run #\"]), row[\"Well_Num\"], row[\"FootprintNeuron #\"]))\n",
    "\n",
    "# === Load branch data using your directory structure ===\n",
    "root_path = Path(\"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR/shruti_axontracking_analysis_detailedcsvs/\")\n",
    "\n",
    "data_dict = {}\n",
    "for run_folder in root_path.iterdir():\n",
    "    if run_folder.is_dir():\n",
    "        run_number = run_folder.name.split(\"_\")[-1]\n",
    "        csv_folder = run_folder / \"csv\"\n",
    "        if csv_folder.exists():\n",
    "            run_data = {}\n",
    "            for file in csv_folder.glob(\"*.csv\"):\n",
    "                key_name = file.stem\n",
    "                try:\n",
    "                    run_data[key_name] = pd.read_csv(file)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file.name} in run {run_number}: {e}\")\n",
    "            data_dict[run_number] = run_data\n",
    "\n",
    "# Function to count branches per detected neuron\n",
    "def count_branches_per_neuron(data_dict):\n",
    "    results = []\n",
    "    for run_num, file_dict in data_dict.items():\n",
    "        # Extract numeric part of run_num\n",
    "        numeric_run_num = int(re.search(r'\\d+', run_num).group())\n",
    "        \n",
    "        # Look for branch_metrics file\n",
    "        if 'branch_metrics' in file_dict:\n",
    "            df = file_dict['branch_metrics']\n",
    "            \n",
    "            # Count occurrences of each neuron (each row = one branch)\n",
    "            for _, row in df.iterrows():\n",
    "                if pd.notna(row['neuron']) and pd.notna(row['wellNo']):\n",
    "                    well_num = int(row['wellNo'])\n",
    "                    neuron_id = int(row['neuron'])\n",
    "                    \n",
    "                    # Check if this neuron is in our detected neurons set\n",
    "                    if (str(numeric_run_num), well_num, neuron_id) in detected_neurons:\n",
    "                        results.append({\n",
    "                            \"run_number\": numeric_run_num,\n",
    "                            \"well_num\": well_num,\n",
    "                            \"neuron_id\": neuron_id\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Get branch count data\n",
    "branch_count_data = count_branches_per_neuron(data_dict)\n",
    "\n",
    "if not branch_count_data.empty:\n",
    "    # Count branches per neuron (each row represents one branch)\n",
    "    neuron_branch_counts = branch_count_data.groupby(['run_number', 'well_num', 'neuron_id']).size().reset_index(name='branch_count')\n",
    "    \n",
    "    # Add density information\n",
    "    def get_density(run_num, well_num):\n",
    "        mapping = run_well_density_map.get(str(run_num), {})\n",
    "        return mapping.get(well_num, \"Unknown\")\n",
    "\n",
    "    neuron_branch_counts[\"Density\"] = neuron_branch_counts.apply(\n",
    "        lambda row: get_density(row[\"run_number\"], row[\"well_num\"]), axis=1\n",
    "    )\n",
    "    \n",
    "    # Remove unknown densities\n",
    "    neuron_branch_counts = neuron_branch_counts[neuron_branch_counts[\"Density\"] != \"Unknown\"]\n",
    "    \n",
    "    # Sort densities numerically\n",
    "    def extract_density_value(density_str):\n",
    "        match = re.search(r\"(\\d+)\", density_str)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    neuron_branch_counts[\"Density_Value\"] = neuron_branch_counts[\"Density\"].apply(extract_density_value)\n",
    "    neuron_branch_counts = neuron_branch_counts.sort_values(\"Density_Value\")\n",
    "    \n",
    "    # === Create plots for each run ===\n",
    "    for run_num, run_data in neuron_branch_counts.groupby(\"run_number\"):\n",
    "        print(f\"\\nðŸ“Š Plotting Run: {run_num}\")\n",
    "        \n",
    "        # Calculate mean branch count per density\n",
    "        mean_data = run_data.groupby(\"Density\")[\"branch_count\"].agg(['mean', 'count']).reset_index()\n",
    "        mean_data = mean_data.sort_values(\"Density\", key=lambda x: x.apply(extract_density_value))\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Create bar plot for means\n",
    "        bars = plt.bar(mean_data[\"Density\"], mean_data[\"mean\"], \n",
    "                       alpha=0.7, color='lightcoral', edgecolor='darkred', linewidth=2,\n",
    "                       label='Mean Branch Count')\n",
    "        \n",
    "        # Add individual neuron markers\n",
    "        for density in mean_data[\"Density\"]:\n",
    "            density_data = run_data[run_data[\"Density\"] == density]\n",
    "            x_pos = list(mean_data[\"Density\"]).index(density)\n",
    "            \n",
    "            # Add jitter for better visibility\n",
    "            jitter = np.random.normal(0, 0.1, len(density_data))\n",
    "            plt.scatter([x_pos] * len(density_data) + jitter, \n",
    "                       density_data[\"branch_count\"],\n",
    "                       color='blue', alpha=0.6, s=40, zorder=5,\n",
    "                       label='Individual Neurons' if density == mean_data[\"Density\"].iloc[0] else \"\")\n",
    "        \n",
    "        # Add mean values as text on bars\n",
    "        for i, (density, mean_val, count) in enumerate(zip(mean_data[\"Density\"], mean_data[\"mean\"], mean_data[\"count\"])):\n",
    "            plt.text(i, mean_val + mean_val*0.02, f'{mean_val:.1f}\\n(n={count})', \n",
    "                    ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "        \n",
    "        # Formatting\n",
    "        plt.title(f\"Number of Branches per Detected Neuron by Density â€” Run: {run_num}\", fontsize=16, fontweight='bold')\n",
    "        plt.xlabel(\"Density\", fontsize=14)\n",
    "        plt.ylabel(\"Number of Branches per Neuron\", fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
    "        plt.legend(fontsize=12)\n",
    "        \n",
    "        # Set y-axis to start from 0 for better comparison\n",
    "        plt.ylim(bottom=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(f\"Summary for Run {run_num}:\")\n",
    "        for _, row in mean_data.iterrows():\n",
    "            print(f\"  {row['Density']}: Mean Branches = {row['mean']:.1f}, Count = {row['count']} neurons\")\n",
    "\n",
    "else:\n",
    "    print(\"No branch data found for detected neurons.\")\n",
    "    print(\"Check that the run numbers match between your branch_metrics files and detected footprints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f98533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DETECTED_CSV = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"\n",
    "ORIGINAL_BASE = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/DetectedPNGs\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(DETECTED_CSV)\n",
    "\n",
    "# Debug: Print first few rows to understand the data structure\n",
    "print(\"First few rows of detected footprints:\")\n",
    "print(df.head())\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Clean the data - handle the Run # formatting\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str)\n",
    "original_run_numbers = df[\"Run #\"].unique()\n",
    "print(f\"\\nOriginal run numbers: {original_run_numbers}\")\n",
    "\n",
    "# Try both with and without zero-padding for run numbers\n",
    "df[\"Run #\"] = df[\"Run #\"].str.zfill(6)\n",
    "padded_run_numbers = df[\"Run #\"].unique()\n",
    "print(f\"Padded run numbers: {padded_run_numbers}\")\n",
    "\n",
    "# Build multiple search patterns to handle different file naming conventions\n",
    "detected_patterns = []\n",
    "for _, row in df.iterrows():\n",
    "    run_num = row[\"Run #\"]\n",
    "    well = row[\"Well #\"]\n",
    "    neuron_num = row[\"FootprintNeuron #\"]\n",
    "    \n",
    "    # Try different filename patterns\n",
    "    patterns = [\n",
    "        f\"FootprintNeuron#{neuron_num}.png\",\n",
    "        f\"FootprintNeuron{neuron_num}.png\", \n",
    "        f\"Neuron#{neuron_num}.png\",\n",
    "        f\"Neuron{neuron_num}.png\",\n",
    "        f\"footprint_{neuron_num}.png\",\n",
    "        f\"footprint_neuron_{neuron_num}.png\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        detected_patterns.append((run_num, well, pattern, neuron_num))\n",
    "\n",
    "print(f\"\\nTotal search patterns created: {len(detected_patterns)}\")\n",
    "\n",
    "# Also try original run numbers (without padding)\n",
    "for _, row in df.iterrows():\n",
    "    run_num = str(row[\"Run #\"]).lstrip('0')  # Remove leading zeros\n",
    "    if run_num == '':  # Handle case where run number was all zeros\n",
    "        run_num = '0'\n",
    "    well = row[\"Well #\"]\n",
    "    neuron_num = row[\"FootprintNeuron #\"]\n",
    "    \n",
    "    patterns = [\n",
    "        f\"FootprintNeuron#{neuron_num}.png\",\n",
    "        f\"FootprintNeuron{neuron_num}.png\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        detected_patterns.append((run_num, well, pattern, neuron_num))\n",
    "\n",
    "missing = []\n",
    "found = 0\n",
    "found_files = []\n",
    "\n",
    "print(f\"\\nSearching in: {ORIGINAL_BASE}\")\n",
    "print(\"Sample directory structure check...\")\n",
    "\n",
    "# First, let's explore the directory structure\n",
    "sample_dirs = []\n",
    "for root, dirs, files in os.walk(ORIGINAL_BASE):\n",
    "    if len(sample_dirs) < 10:  # Just get first 10 directories for debugging\n",
    "        sample_dirs.append(root)\n",
    "    if len(sample_dirs) >= 10:\n",
    "        break\n",
    "\n",
    "print(\"Sample directory paths:\")\n",
    "for i, dir_path in enumerate(sample_dirs[:5]):\n",
    "    print(f\"  {i+1}: {dir_path}\")\n",
    "\n",
    "# Walk all files in the directory\n",
    "print(\"\\nSearching for PNG files...\")\n",
    "all_png_files = []\n",
    "\n",
    "for root, dirs, files in os.walk(ORIGINAL_BASE):\n",
    "    for file in files:\n",
    "        if file.endswith(\".png\") and (\"footprint\" in file.lower() or \"neuron\" in file.lower()):\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_png_files.append((root, file))\n",
    "            \n",
    "            # Extract Run and Well from path\n",
    "            parts = root.split(os.sep)\n",
    "            \n",
    "            # Try to find run number in path (look for 6-digit numbers, then shorter ones)\n",
    "            run_candidates = [p for p in parts if p.isdigit()]\n",
    "            run_id = None\n",
    "            \n",
    "            # First try 6-digit numbers\n",
    "            six_digit = [p for p in run_candidates if len(p) == 6]\n",
    "            if six_digit:\n",
    "                run_id = six_digit[0]\n",
    "            elif run_candidates:\n",
    "                # Try other digit lengths\n",
    "                run_id = max(run_candidates, key=len)  # Take the longest digit string\n",
    "            \n",
    "            # Find well identifier\n",
    "            well = next((p for p in parts if p.lower().startswith(\"well\")), None)\n",
    "            \n",
    "            if run_id and well:\n",
    "                # Check against all our patterns\n",
    "                for pattern_run, pattern_well, pattern_file, neuron_num in detected_patterns:\n",
    "                    if (run_id == pattern_run and well == pattern_well and file == pattern_file):\n",
    "                        # Create destination directory\n",
    "                        dst_dir = os.path.join(OUTPUT_BASE, f\"Run{run_id}\", well)\n",
    "                        os.makedirs(dst_dir, exist_ok=True)\n",
    "                        \n",
    "                        # Create new filename with more info\n",
    "                        new_filename = f\"Run{run_id}_{well}_Neuron{neuron_num}_{file}\"\n",
    "                        dst_path = os.path.join(dst_dir, new_filename)\n",
    "                        \n",
    "                        try:\n",
    "                            shutil.copy2(full_path, dst_path)\n",
    "                            found += 1\n",
    "                            found_files.append((run_id, well, file, neuron_num))\n",
    "                            print(f\"âœ… Copied: {file} -> {new_filename} in {dst_dir}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"âŒ Error copying {file}: {e}\")\n",
    "                        break\n",
    "\n",
    "print(f\"\\nTotal PNG files found in directory: {len(all_png_files)}\")\n",
    "print(f\"âœ… Total PNGs copied: {found}\")\n",
    "\n",
    "# Show some examples of PNG files found\n",
    "print(f\"\\nFirst 10 PNG files found:\")\n",
    "for i, (root, file) in enumerate(all_png_files[:10]):\n",
    "    print(f\"  {i+1}: {file} in {root}\")\n",
    "\n",
    "# Create summary of what was found vs what was expected\n",
    "expected_neurons = set((row[\"Run #\"], row[\"Well #\"], row[\"FootprintNeuron #\"]) for _, row in df.iterrows())\n",
    "found_neurons = set((run_id, well, neuron_num) for run_id, well, file, neuron_num in found_files)\n",
    "\n",
    "print(f\"\\nExpected neurons: {len(expected_neurons)}\")\n",
    "print(f\"Found neurons: {len(found_neurons)}\")\n",
    "\n",
    "# Find missing ones\n",
    "missing_neurons = expected_neurons - found_neurons\n",
    "if missing_neurons:\n",
    "    print(f\"\\nMissing {len(missing_neurons)} neurons:\")\n",
    "    missing_df = pd.DataFrame(list(missing_neurons), columns=[\"Run #\", \"Well #\", \"FootprintNeuron #\"])\n",
    "    missing_df.to_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/missing_detailed.csv\", index=False)\n",
    "    print(\"Detailed missing list saved to missing_detailed.csv\")\n",
    "    \n",
    "    # Show first few missing\n",
    "    for i, (run_id, well, neuron_num) in enumerate(list(missing_neurons)[:5]):\n",
    "        print(f\"  Missing: Run {run_id}, {well}, Neuron {neuron_num}\")\n",
    "\n",
    "# Save found files summary\n",
    "if found_files:\n",
    "    found_df = pd.DataFrame(found_files, columns=[\"Run #\", \"Well #\", \"Original_Filename\", \"FootprintNeuron #\"])\n",
    "    found_df.to_csv(\"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/successfully_copied.csv\", index=False)\n",
    "    print(\"Successfully copied files list saved to successfully_copied.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d88595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DETECTED_CSV = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"\n",
    "ORIGINAL_BASE = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/DetectedPNGs\"\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(DETECTED_CSV)\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str).str.zfill(6)\n",
    "\n",
    "# Build dictionary for fast lookup\n",
    "detected_set = set(\n",
    "    (row[\"Run #\"], row[\"Well #\"], f\"FootprintNeuron#{row['FootprintNeuron #']}.png\")\n",
    "    for _, row in df.iterrows()\n",
    ")\n",
    "\n",
    "missing = []\n",
    "found = 0\n",
    "\n",
    "# Walk all files in the directory\n",
    "for root, _, files in os.walk(ORIGINAL_BASE):\n",
    "    for file in files:\n",
    "        if file.startswith(\"FootprintNeuron#\") and file.endswith(\".png\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "\n",
    "            # Extract Run and Well from path\n",
    "            parts = root.split(os.sep)\n",
    "            try:\n",
    "                run_idx = next(i for i, p in enumerate(parts) if p.isdigit() and len(p) == 6)\n",
    "                run_id = parts[run_idx]\n",
    "                well = next((p for p in parts if p.startswith(\"Well\")), None)\n",
    "            except StopIteration:\n",
    "                continue  # skip malformed paths\n",
    "\n",
    "            # Is this file in our detection CSV?\n",
    "            key = (run_id, well, file)\n",
    "            if key in detected_set:\n",
    "                dst_dir = os.path.join(OUTPUT_BASE, f\"Run{run_id}\", well)\n",
    "                os.makedirs(dst_dir, exist_ok=True)\n",
    "                dst_path = os.path.join(dst_dir, file)\n",
    "                shutil.copy2(full_path, dst_path)\n",
    "                found += 1\n",
    "                print(f\"âœ… Copied: {file} to {dst_dir}\")\n",
    "                detected_set.remove(key)\n",
    "\n",
    "# Any that weren't found\n",
    "for run_id, well, file in detected_set:\n",
    "    missing.append((run_id, well, file))\n",
    "    print(f\"âš ï¸ Still missing: Run {run_id}, {well}, {file}\")\n",
    "\n",
    "# Save missing list\n",
    "pd.DataFrame(missing, columns=[\"Run #\", \"Well #\", \"Filename\"]).to_csv(\n",
    "    \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/missing_detected_pngs_updated.csv\", index=False\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Total PNGs copied: {found}\")\n",
    "print(f\"ðŸ“„ Missing list saved as: missing_detected_pngs_updated.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf482e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DETECTED_CSV = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/axon_detected_footprints.csv\"\n",
    "ORIGINAL_BASE = \"/mnt/ben-shalom_nas/raw_data/rbs_maxtwo_desktop/harddisk24tbvol1/Media_Density_T3_07012025_AR/Media_Density_T3_07012025_AR\"\n",
    "OUTPUT_BASE = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/DetectedPNGs\"\n",
    "\n",
    "# Load CSV and filter rows where Detected == \"Yes\"\n",
    "df = pd.read_csv(DETECTED_CSV)\n",
    "df = df[df[\"Detected\"] == \"Yes\"]  # Filter only rows with Detected == \"Yes\"\n",
    "df[\"Run #\"] = df[\"Run #\"].astype(str).str.zfill(6)  # Ensure Run # is zero-padded to 6 digits\n",
    "\n",
    "# Build a set for fast lookup\n",
    "detected_set = set(\n",
    "    (row[\"Run #\"], row[\"Well #\"].strip(), f\"FootprintNeuron#{int(row['FootprintNeuron #'])}.png\")\n",
    "    for _, row in df.iterrows()\n",
    ")\n",
    "\n",
    "missing = []\n",
    "found = 0\n",
    "\n",
    "# Walk through all files in the directory\n",
    "for root, _, files in os.walk(ORIGINAL_BASE):\n",
    "    for file in files:\n",
    "        if file.startswith(\"FootprintNeuron#\") and file.endswith(\".png\"):\n",
    "            full_path = os.path.join(root, file)\n",
    "\n",
    "            # Extract Run and Well from the path\n",
    "            parts = root.split(os.sep)\n",
    "            try:\n",
    "                # Extract Run ID (6-digit folder)\n",
    "                run_idx = next(i for i, p in enumerate(parts) if p.isdigit() and len(p) == 6)\n",
    "                run_id = parts[run_idx].zfill(6)  # Ensure zero-padding\n",
    "\n",
    "                # Extract Well (folder starting with \"Well\")\n",
    "                well = next((p for p in parts if p.startswith(\"Well\")), None)\n",
    "                if well is None:\n",
    "                    continue  # Skip if Well is not found\n",
    "\n",
    "                # Normalize Well and File\n",
    "                well = well.strip()\n",
    "                file = file.strip()\n",
    "\n",
    "                # Check if this file is in our detection set\n",
    "                key = (run_id, well, file)\n",
    "                if key in detected_set:\n",
    "                    dst_dir = os.path.join(OUTPUT_BASE, f\"Run{run_id}\", well)\n",
    "                    os.makedirs(dst_dir, exist_ok=True)\n",
    "                    dst_path = os.path.join(dst_dir, file)\n",
    "                    shutil.copy2(full_path, dst_path)\n",
    "                    found += 1\n",
    "                    print(f\"âœ… Copied: {file} to {dst_dir}\")\n",
    "                    detected_set.remove(key)\n",
    "            except StopIteration:\n",
    "                continue  # Skip malformed paths\n",
    "\n",
    "# Handle any files that weren't found\n",
    "for run_id, well, file in detected_set:\n",
    "    missing.append((run_id, well, file))\n",
    "    print(f\"âš ï¸ Still missing: Run {run_id}, {well}, {file}\")\n",
    "\n",
    "# Save the missing list to a CSV file\n",
    "missing_csv_path = \"/mnt/disk15tb/shruti/output_AxonTracking_T1_08052025/missing_detected_pngs_updated.csv\"\n",
    "pd.DataFrame(missing, columns=[\"Run #\", \"Well #\", \"Filename\"]).to_csv(missing_csv_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Total PNGs copied: {found}\")\n",
    "print(f\"ðŸ“„ Missing list saved as: {missing_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Searching for: Run {run_id}, Well {well}, File {file}\")\n",
    "print(f\"Detected Set: {key in detected_set}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017865ab",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
